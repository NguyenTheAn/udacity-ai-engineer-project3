1, Query string: https://azurecognitivesearchligirk.search.windows.net/indexes/course-index/docs?api-version=2021-04-30-Preview&search=azure
Results:
{ "@odata.context": "https://azurecognitivesearchligirk.search.windows.net/indexes('course-index')/$metadata#docs(*)", "value": [ { "@search.score": 7.03354, "RowKey": "21933bf9-68d3-4649-b58a-79fbb476be02", "instructor": null, "product": "azure", "title": "Provision databases in Azure Pipelines", "keyphrases": [ "Provision databases", "Azure Pipelines" ] }, { "@search.score": 7.03354, "RowKey": "360412d8-4f79-4219-acc3-6d5e2134d880", "instructor": null, "product": "azure", "title": "Provision databases in Azure Pipelines", "keyphrases": [ "Provision databases", "Azure Pipelines" ] }, { "@search.score": 7.03354, "RowKey": "4c970879-187f-473c-83d5-986109db3183", "instructor": null, "product": "azure", "title": "Provision databases in Azure Pipelines", "keyphrases": [ "Provision databases", "Azure Pipelines" ] }, { "@search.score": 7.03354, "RowKey": "c9f624d0-f2ec-49c8-bcb2-c89b79b051d2", "instructor": null, "product": "azure", "title": "Provision databases in Azure Pipelines", "keyphrases": [ "Provision databases", "Azure Pipelines" ] }, { "@search.score": 6.9618053, "RowKey": "1769801f-ea93-456d-afe4-06dc019e8f71", "instructor": null, "product": "azure", "title": "Automate Azure Functions deployments with Azure Pipelines", "keyphrases": [ "CI/CD pipeline", "Azure Functions" ] }, { "@search.score": 6.9618053, "RowKey": "56bc8c78-fa2b-4b59-9daa-ba89721ce27c", "instructor": null, "product": "azure", "title": "Automate Azure Functions deployments with Azure Pipelines", "keyphrases": [ "CI/CD pipeline", "Azure Functions" ] }, { "@search.score": 6.9618053, "RowKey": "76dcb4e7-6c94-4ed5-93e8-8ec05ce9b86b", "instructor": null, "product": "azure", "title": "Automate Azure Functions deployments with Azure Pipelines", "keyphrases": [ "CI/CD pipeline", "Azure Functions" ] }, { "@search.score": 6.9618053, "RowKey": "89c14ed4-e008-400d-baac-0549e8299b57", "instructor": null, "product": "azure", "title": "Automate Azure Functions deployments with Azure Pipelines", "keyphrases": [ "CI/CD pipeline", "Azure Functions" ] }, { "@search.score": 6.8687654, "RowKey": "48efecbf-9b8c-47a5-aa46-deea3294ab25", "instructor": null, "product": "azure-devops", "title": "Provision databases in Azure Pipelines", "keyphrases": [ "Provision databases", "Azure Pipelines" ] }, { "@search.score": 6.8687654, "RowKey": "f394c98b-a99f-4225-aa77-152294025ebd", "instructor": null, "product": "azure-devops", "title": "Provision databases in Azure Pipelines", "keyphrases": [ "Provision databases", "Azure Pipelines" ] }, { "@search.score": 6.8687654, "RowKey": "f41794e0-2585-4036-9237-05f74b61d6b0", "instructor": null, "product": "azure-devops", "title": "Provision databases in Azure Pipelines", "keyphrases": [ "Provision databases", "Azure Pipelines" ] }, { "@search.score": 6.8687654, "RowKey": "f41e6014-6251-40f2-85c5-03baed68b755", "instructor": null, "product": "azure-devops", "title": "Provision databases in Azure Pipelines", "keyphrases": [ "Provision databases", "Azure Pipelines" ] }, { "@search.score": 6.863673, "RowKey": "db69aa91-d4da-48f9-a77c-5603bcae3596", "instructor": null, "product": "azure", "title": "Load test Azure web apps by using Azure DevOps", "keyphrases": [ "Azure DevOps tools", "Azure portal", "load tests", "apps" ] }, { "@search.score": 6.797031, "RowKey": "3b2eceda-f984-4440-804d-c18cce4d3653", "instructor": null, "product": "azure-devops", "title": "Automate Azure Functions deployments with Azure Pipelines", "keyphrases": [ "CI/CD pipeline", "Azure Functions" ] }, { "@search.score": 6.797031, "RowKey": "7682ab16-a067-41b8-9188-b8c4f433cec6", "instructor": null, "product": "azure-devops", "title": "Automate Azure Functions deployments with Azure Pipelines", "keyphrases": [ "CI/CD pipeline", "Azure Functions" ] }, { "@search.score": 6.797031, "RowKey": "ae8e2557-daa0-4f90-bafd-d80ba94675c0", "instructor": null, "product": "azure-devops", "title": "Automate Azure Functions deployments with Azure Pipelines", "keyphrases": [ "CI/CD pipeline", "Azure Functions" ] }, { "@search.score": 6.797031, "RowKey": "f05da97a-a2dd-4dcc-8018-c9826fb2b0dd", "instructor": null, "product": "azure-devops", "title": "Automate Azure Functions deployments with Azure Pipelines", "keyphrases": [ "CI/CD pipeline", "Azure Functions" ] }, { "@search.score": 6.7769766, "RowKey": "3f5306a1-2460-4628-9ada-8f0628b20bbd", "instructor": null, "product": "azure", "title": "Monitor models with Azure Machine Learning", "keyphrases": [ "Azure Machine Learning", "models" ] }, { "@search.score": 6.661062, "RowKey": "59c58efa-d830-4c65-8ac4-5432bb0ec73e", "instructor": null, "product": "azure", "title": "Introduction to the Azure Machine Learning SDK", "keyphrases": [ "Azure Machine Learning SDK", "Introduction" ] }, { "@search.score": 6.661062, "RowKey": "6456b6d8-0af1-4dfd-b895-37ecfb3df42b", "instructor": null, "product": "azure", "title": "Introduction to the Azure Machine Learning SDK", "keyphrases": [ "Azure Machine Learning SDK", "Introduction" ] }, { "@search.score": 6.661062, "RowKey": "94a8abf2-6e37-40ed-8fed-6c23d50a7424", "instructor": null, "product": "azure", "title": "Monitor data drift with Azure Machine Learning", "keyphrases": [ "Azure Machine Learning", "data drift" ] }, { "@search.score": 6.661062, "RowKey": "9e2bd51e-0f26-487b-b828-4bd6fd850743", "instructor": null, "product": "azure", "title": "Tune hyperparameters with Azure Machine Learning", "keyphrases": [ "Azure Machine Learning", "Tune hyperparameters" ] }, { "@search.score": 6.612202, "RowKey": "181bbcf0-6713-4ee7-b858-3cf54674ed18", "instructor": null, "product": "azure-portal", "title": "Monitor models with Azure Machine Learning", "keyphrases": [ "Azure Machine Learning", "models" ] }, { "@search.score": 6.612202, "RowKey": "d6417f73-e2e0-4eba-8996-bcda8666a60b", "instructor": null, "product": "azure-databricks", "title": "Implement CI/CD with Azure DevOps", "keyphrases": [ "Azure DevOps", "CI/CD" ] }, { "@search.score": 6.531625, "RowKey": "13f442b0-f1d1-4402-8c3e-13f0c8b6ce8d", "instructor": null, "product": "azure-cognitive-search", "title": "Create an Azure Cognitive Search solution", "keyphrases": [ "Azure Cognitive Search solution" ] }, { "@search.score": 6.531625, "RowKey": "4ffb7ba0-0771-4a01-9da7-915635fb6ba4", "instructor": null, "product": "azure-machine-learning", "title": "Monitor models with Azure Machine Learning", "keyphrases": [ "Azure Machine Learning", "models" ] }, { "@search.score": 6.531625, "RowKey": "683814dd-934d-4e72-a5fd-81a3c8037999", "instructor": null, "product": "azure-cognitive-search", "title": "Create an Azure Cognitive Search solution", "keyphrases": [ "Azure Cognitive Search solution" ] }, { "@search.score": 6.531625, "RowKey": "e00a9b7f-cd37-41b1-87d3-d87018dcf606", "instructor": null, "product": "azure-cognitive-search", "title": "Create an Azure Cognitive Search solution", "keyphrases": [ "Azure Cognitive Search solution" ] }, { "@search.score": 6.496288, "RowKey": "16681c04-f885-49d8-ab7e-e477936a6bdc", "instructor": null, "product": "azure-portal", "title": "Tune hyperparameters with Azure Machine Learning", "keyphrases": [ "Azure Machine Learning", "Tune hyperparameters" ] }, { "@search.score": 6.496288, "RowKey": "a5ffe016-69df-4be9-be9a-76bdfc4092c2", "instructor": null, "product": "azure-portal", "title": "Monitor data drift with Azure Machine Learning", "keyphrases": [ "Azure Machine Learning", "data drift" ] }, { "@search.score": 6.41571, "RowKey": "2b0ef195-b399-4e30-a61d-07b2bcf90857", "instructor": null, "product": "azure-machine-learning", "title": "Monitor data drift with Azure Machine Learning", "keyphrases": [ "Azure Machine Learning", "data drift" ] }, { "@search.score": 6.41571, "RowKey": "2eb64953-5f7a-4985-8632-314014c8a8c2", "instructor": null, "product": "azure-machine-learning", "title": "Introduction to the Azure Machine Learning SDK", "keyphrases": [ "Azure Machine Learning SDK", "Introduction" ] }, { "@search.score": 6.41571, "RowKey": "a7182168-4625-4f3f-8348-bd0ddc542f63", "instructor": null, "product": "azure-machine-learning", "title": "Introduction to the Azure Machine Learning SDK", "keyphrases": [ "Azure Machine Learning SDK", "Introduction" ] }, { "@search.score": 6.41571, "RowKey": "c3e839d4-2913-4d44-843d-9ed5250afe83", "instructor": null, "product": "azure-machine-learning", "title": "Tune hyperparameters with Azure Machine Learning", "keyphrases": [ "Azure Machine Learning", "Tune hyperparameters" ] }, { "@search.score": 6.3419023, "RowKey": "7a91556d-73a0-40d8-ad02-1c1be1323330", "instructor": null, "product": "dotnet", "title": "Load test Azure web apps by using Azure DevOps", "keyphrases": [ "Azure DevOps tools", "Azure portal", "load tests", "apps" ] }, { "@search.score": 6.3419023, "RowKey": "94039250-79f1-4494-b86b-749b88834ee8", "instructor": null, "product": "dotnet-core", "title": "Load test Azure web apps by using Azure DevOps", "keyphrases": [ "Azure DevOps tools", "Azure portal", "load tests", "apps" ] }, { "@search.score": 6.3419023, "RowKey": "c461332f-335a-4d44-b169-2e5eef88d45f", "instructor": null, "product": "aspnet-core", "title": "Load test Azure web apps by using Azure DevOps", "keyphrases": [ "Azure DevOps tools", "Azure portal", "load tests", "apps" ] }, { "@search.score": 6.2760816, "RowKey": "1b484256-265d-4584-96f1-28819888c92f", "instructor": null, "product": "azure", "title": "Configure infrastructure in Azure Pipelines", "keyphrases": [ "configuration management tools", "Azure Pipelines", "infrastructure" ] }, { "@search.score": 6.2760816, "RowKey": "3507b031-6c3c-4a37-87f1-791e775b70f6", "instructor": null, "product": "azure", "title": "Configure infrastructure in Azure Pipelines", "keyphrases": [ "configuration management tools", "Azure Pipelines", "infrastructure" ] }, { "@search.score": 6.2760816, "RowKey": "8498078e-8004-4397-9753-9b1c8242ee2b", "instructor": null, "product": "azure", "title": "Configure infrastructure in Azure Pipelines", "keyphrases": [ "configuration management tools", "Azure Pipelines", "infrastructure" ] }, { "@search.score": 6.2760816, "RowKey": "9fa7eae2-9879-4f9d-bfad-1a166d7b2bd1", "instructor": null, "product": "azure", "title": "Configure infrastructure in Azure Pipelines", "keyphrases": [ "configuration management tools", "Azure Pipelines", "infrastructure" ] }, { "@search.score": 6.170228, "RowKey": "15f02c60-301b-4418-9368-044c0bbcd36f", "instructor": null, "product": "azure", "title": "Train a machine learning model with Azure Machine Learning", "keyphrases": [ "machine learning model", "Azure Machine Learning" ] }, { "@search.score": 6.170228, "RowKey": "3f048030-d800-4cbe-9261-024a642ecbf7", "instructor": null, "product": "azure", "title": "Automate machine learning model selection with Azure Machine Learning", "keyphrases": [ "machine learning model selection", "Azure Machine Learning" ] }, { "@search.score": 6.170228, "RowKey": "79e7e990-c902-459e-8639-56d9646520fc", "instructor": null, "product": "azure", "title": "Explain machine learning models with Azure Machine Learning", "keyphrases": [ "machine learning models", "Azure Machine Learning" ] }, { "@search.score": 6.170228, "RowKey": "868b0ff7-95e5-49f4-b490-0eb51b136d87", "instructor": null, "product": "azure", "title": "Automate machine learning model selection with Azure Machine Learning", "keyphrases": [ "machine learning model selection", "Azure Machine Learning" ] }, { "@search.score": 6.170228, "RowKey": "c1f7d0eb-8786-4f99-8eb9-db55ef8f789b", "instructor": null, "product": "azure", "title": "Train a machine learning model with Azure Machine Learning", "keyphrases": [ "machine learning model", "Azure Machine Learning" ] }, { "@search.score": 6.170228, "RowKey": "d6a8fd2e-d3a5-449e-8d07-804b61d20a60", "instructor": null, "product": "azure", "title": "Deploy batch inference pipelines with Azure Machine Learning", "keyphrases": [ "batch inference pipelines", "Azure Machine Learning" ] }, { "@search.score": 6.153724, "RowKey": "1918bcce-cb5a-4db1-871e-052f4af46b79", "instructor": null, "product": "azure-functions", "title": "Create a custom skill for Azure Cognitive Search", "keyphrases": [ "Azure Cognitive Search", "custom skill" ] }, { "@search.score": 6.153724, "RowKey": "2f19c7cb-45b5-45f1-a6ec-00409eadab7b", "instructor": null, "product": "azure-functions", "title": "Create a custom skill for Azure Cognitive Search", "keyphrases": [ "Azure Cognitive Search", "custom skill" ] }, { "@search.score": 6.153724, "RowKey": "3348b913-81d8-42c8-a511-36b9d7004fa9", "instructor": null, "product": "azure-databricks", "title": "Work with Azure Machine Learning to deploy serving models", "keyphrases": [ "Azure Machine Learning", "serving models" ] } ], "@odata.nextLink": "https://azurecognitivesearchligirk.search.windows.net/indexes('course-index')/docs?api-version=2021-04-30-Preview&search=azure&$skip=50" }

2, Query string: https://azurecognitivesearchligirk.search.windows.net/indexes/library-index/docs?api-version=2021-04-30-Preview&search=mobile
Results:
{ "@odata.context": "https://azurecognitivesearchligirk.search.windows.net/indexes('library-index')/$metadata#docs(*)", "value": [ { "@search.score": 9.378774, "content": "\nContext‑aware rule learning \nfrom smartphone data: survey, challenges \nand future directions\nIqbal H. Sarker1,2*\n\nIntroduction\nIn recent days, smartphones have become an essential part of our daily life and con-\nsidered as highly personal devices of individuals. These devices are also known as one \nof the most important IoT (Internet of Things) devices, because of their capabilities \nto interconnect their users with the Internet, and corresponding data processing [1]. \nSmartphones are also considered as “next generation, multifunctional cell phones that \nfacilitates data processing as well as enhanced wireless connectivity” [2]. The cellular net-\nwork coverage has reached 96.8% of the world population, and this number even reaches \n100% of the population in the developed countries [3]. In recent statistics, according to \nGoogle Trends [4] we have shown in Fig.  1, that users’ interest on “Mobile Phones” is \nmore and more than other platforms like “Desktop Computer”, “Laptop Computer” or \n\nAbstract \n\nSmartphones are considered as one of the most essential and highly personal devices \nof individuals in our current world. Due to the popularity of context-aware technol-\nogy and recent developments in smartphones, these devices can collect and process \nraw contextual data about users’ surrounding environment and their corresponding \nbehavioral activities with their phones. Thus, smartphone data analytics and building \ndata-driven context-aware systems have gained wide attention from both academia \nand industry in recent days. In order to build intelligent context-aware applications on \nsmartphones, effectively learning a set of context-aware rules from smartphone data \nis the key. This requires advanced data analytical techniques with high precision and \nintelligent decision making strategies based on contexts. In comparison to traditional \napproaches, machine learning based techniques provide more effective and efficient \nresults for smartphone data analytics and corresponding context-aware rule learning. \nThus, this article first makes a survey on previous work in the area of contextual smart-\nphone data analytics and then presents a discussion of challenges and future directions \nfor effectively learning context-aware rules from smartphone data, in order to build \nrule-based automated and intelligent systems.\n\nKeywords: Smartphone data, Machine learning, Data science, Clustering, \nClassification, Association, Rule learning, Personalization, Time-series, User behavior \nmodeling, Predictive analytics, Context-aware computing, Mobile and IoT services, \nIntelligent systems\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nSURVEY PAPER\n\nSarker J Big Data (2019) 6:95 \nhttps://doi.org/10.1186/s40537‑019‑0258‑4\n\n*Correspondence: \nmsarker@swin.edu.au \n1 Swinburne University \nof Technology, \nMelbourne VIC-3122, \nAustralia\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-019-0258-4&domain=pdf\n\n\nPage 2 of 25Sarker J Big Data (2019) 6:95 \n\n“Tablet Computer” for the last 5 years from 2014 to 2019. Figure 1 represents timestamp \ninformation in terms of particular date in x-axis and corresponding search interests in \nthe range of 0 to 100 in terms of popularity relative to the highest point on the chart in \ny-axis. For instance, a value of 100 (maximum) in y-axis represents the peak popularity \nfor a particular term, while 0 (minimum) means the term was lowest in terms of popu-\nlarity [4].\n\nDue to the advanced features and recent developments in smartphones, these devices \ncan collect raw contextual data about users’ surrounding environment and their corre-\nsponding behavioral activities with their phones in a daily basis [5]. As a result, smart-\nphone data becomes a great source to understand users’ behavioral activity patterns in \ndifferent contexts, and to derive useful information, i.e., context-aware rules, for the pur-\npose of building rule-based intelligent context-aware systems. A context-aware rule has \ntwo parts, which follows “IF-THEN” logical structure to formulate [6]. The antecedent \npart represents users’ surrounding contextual information, e.g., temporal context, spa-\ntial context, social contexts, or others relevant contextual information and the conse-\nquent part represents their corresponding behavioral activities or usage. Let’s consider \nan example of a context-aware mobile notification management system for a smart-\nphone user Alice. A context-aware rule for such system could be “The user typically \ndismisses mobile notifications while at work; however, accepts the notifications in the \nevening from her family members, even though she is in work”. A set of such context-\naware behavioral rules including general and specific exceptions, may vary from user-to-\nuser according to their preferences. In addition to the personalized services mentioned \nabove, the relevant context-aware rules in different surrounding contexts could be appli-\ncable to other broad application areas, like context-aware  software and IoT services, \nintelligent eHealth services, and context-aware smart city services, intelligent cybersecu-\nrity services etc. utilizing the relevant contextual data of that particular domain. Overall, \nthis study is typically for those data science and machine learning researchers, and prac-\ntitioners who particularly want to work on data-driven intelligent context-aware systems \nand services based on machine learning rules.\n\nEffectively learning context-aware rules from smartphone data is challenging because \nof many reasons, ranging from understanding raw data to applications. A number of \nresearch [7–9] has been done on mining context-aware rules from smartphone data for \nvarious purposes. However, to effectively learn such rules for the purpose of building \n\nFig. 1 Users’ interest trends over time, where x-axis and y-axis represent a particular timestamp and \ncorresponding search interests in numeric values in terms of world-wide popularity respectively\n\n\n\n\n\nPage 3 of 25Sarker J Big Data (2019) 6:95 \n\nintelligent context-aware systems, a deeper analysis in contextual data patterns and \nlearning according to individuals’ usage is needed. Thus, advanced data analysis based \non machine learning techniques, can be used to make effective and efficient decision-\nmaking capabilities in different context-aware test cases for smartphones. Several \nmachine learning and data mining techniques, such as contextual data clustering, fea-\nture optimization and selection, rule-based classification and association analysis, incre-\nmental learning for dynamic updating and management, and corresponding rule-based \nprediction model can be designed to provide smartphone data analytic solutions. The \nreason is that such machine learning techniques can be more accurate, and more precise \nfor analyzing huge amount of contextual data. The aim of these advanced analytic tech-\nniques is to discover information, hidden patterns, and unknown correlations among the \ncontexts and eventually generate context-aware rules. For instance, a detailed analysis \nof time-series data and corresponding data clustering based on similar behavioral pat-\nterns, could lead to capture the diverse behaviors of an individual’s activities, thereby \nenabling more optimal time-based context-aware rules than the traditional approaches \n[10]. Thus, intelligent data-driven decisions using machine learning techniques can \nprofit better decision making capability over the traditional approaches while consider-\ning the multi-dimensional contexts.\n\nBased on our survey and analysis on existing research, little work has been done in \nterms of how machine learning techniques significantly impact on contextual smart-\nphone data and to learn corresponding context-aware rules. To address this short-\ncoming, this article first makes a survey on previous work in the area of contextual \nsmartphone data analytics in several perspectives involved in context-aware rules, such \nas time-series modeling that is also known as a discretization of temporal context, rule \ndiscovery techniques, and incremental learning and rule updation techniques, which has \nbeen highlighted in our earlier work [6]. After that this article presents a brief discussion \non challenges and future directions to overcome these issues. Based on our discussion, \nfinally we suggest a machine learning based context-aware rule learning framework for \nthe purpose of effectively learning context-aware rules from smartphone data, in order \nto build rule-based automated and intelligent systems.\n\nThe contributions of this paper are summarized as follows.\n\n• We first make a brief survey on previous work in the area of smartphone data analyt-\nics in several perspectives related to context-aware rule learning and summarize the \nshortcomings of these research.\n\n• We then present a brief discussion on the challenges and future directions to over-\ncome the issues to learn context-aware rules from smartphone data.\n\n• Finally, we suggest a machine learning based context-aware rule learning framework \nand briefly discuss the role of various layers associated with the framework, for the \npurpose of building rule-based intelligent context-aware systems.\n\nTo the best of our knowledge, this is the first article surveying context-aware rule learn-\ning strategies from smrtphone data. The remainder of the paper is organized as follows. \n“Background: contexts and smartphone data” section presents background information \non contexts and contextual smartphone data. “Context-aware rule learning strategies” \n\n\n\nPage 4 of 25Sarker J Big Data (2019) 6:95 \n\nsection  surveys previous work in various perspectives related to context-aware rule \nlearning. “Challenges and future directions” section briefly discusses the challenges and \nfuture directions of research regarding context-aware rule learning from smartphone \ndata. In “Suggested machine learning based framework” section we suggest a machine \nlearning based context-aware rule learning framework and discuss various layers with \ntheir roles while learning rules. Context-aware rule based applications section summa-\nrizes a number of real world applications based on context-aware rules. Finally, “Conclu-\nsion” section concludes this paper.\n\nBackground: contexts and smartphone data\nThis section reviews background information on the main characteristics of contexts \nand contextual smartphone data that address learning context-aware rules for the pur-\npose of building rule-based intelligent systems.\n\nCharacteristics of contexts\n\nThe term context can be used with a variety of different meanings in different purposes. \nThe notion of context has been used in numerous areas, including Pervasive and Ubiq-\nuitous Computing, Human Computer Interaction, Computer-Supported Collaborative \nWork, and Ambient Intelligence [11]. In this section, first we briefly review what is con-\ntext in the area of mobile and context-aware computing. In Ubiquitous and Pervasive \nComputing area, early works on context-awareness referred to context as primarily \nthe location of people and objects [12]. In recent works, context has been extended to \ninclude a broader collection of factors, such as physical and social aspects of an entity, \nas well as the activities of users [11]. Having examined the definitions and categories of \ncontext given by the pervasive and ubiquitous computing community, this section seeks \nto define our view of context within the scope of smartphone data analytics. As the defi-\nnitions of context to pervasive and ubiquitous computing area are also broad, this dis-\ncussion is intended to be illustrative rather than exhaustive.\n\nSeveral studies have attempted to define and represent the context from different \nperspectives. For instance, the user’s location information, the surrounding people and \nobjects around the user, and the changes to those objects are considered as contexts by \nSchilit et al. [12]. Brown et al. [13] also define contexts as user’s locational information, \ntemporal information, the surrounding people around the user, temperature, etc. Simi-\nlarly, the user’s locational information, environmental information, temporal informa-\ntion, user’s identity, are also taken into account as contexts by Ryan et  al. [14]. Other \ndefinitions of context have simply provided synonyms for context such as context as the \nenvironment or social situation. A number of researchers are taken into account the \ncontext as the environmental information of the user. For instance, in [15], the environ-\nmental information that the user’s computer knows about are taken into account as con-\ntext by Brown et al., whereas the social situation of the user is considered as a context \nin Franklin et al. [16]. On the other hand, a number of other researchers consider it to \nbe the environment related to the applications. For instance, Ward et al. [17] consider \nthe state of the surrounding information of the applications as contexts. Hull et al. [18] \ndefine context as the aspects of the current situation of the user and include the entire \n\n\n\nPage 5 of 25Sarker J Big Data (2019) 6:95 \n\nenvironment. The settings of applications are also treated as context in Rodden et  al. \n[19].\n\nAccording to Schilit et  al. [20] the important aspects of context are: (i) where you \nare, (ii) whom you are with, and (iii) what resources are nearby. The information of the \nchanging environment is taken into account as context in their definition. In addition to \nthe user environment (e.g., user location, nearby people around the user, and the cur-\nrent social situation of the user), they also include the computing environment and the \nphysical environment. For instance, connectivity, available processors, user input and \ndisplay, network capacity, and costs of computing can be the examples of the computing \nenvironment, while the noise level, temperature, the lighting level, can be the examples \nof the physical environment. Dey et al. [21] present a survey of alternative view of con-\ntext, which are largely imprecise and indirect, typically defining context by synonym or \nexample. Finally, they offer the following definition of context, which is perhaps now the \nmost widely accepted. According to Dey et al. [21] “Context is any information that can \nbe used to characterize the situation of an entity. An entity is person, place or object \nthat is considered relevant to the interaction between a user and an application, includ-\ning the user and the application themselves”. Thus, based on the definition of Dey et al. \n[21], we can define context in the scope of this work as “Context is any information that \ncan be used to characterize users’ day-to-day situations that have an influence on their \nsmartphone usage”. An example of relevant contexts could be temporal context, spatial \ncontext, or social context etc. that might have an influence to make individuals’ diverse \ndecisions on smartphone usage in their daily life activities.\n\nContextual smartphone data\n\nWe live in the age of data [22], where everything that surrounds us is linked to a data \nsource and everything in our lives is captured digitally. Mobile or cellular phones have \nbecome increasingly ubiquitous and powerful to log user diverse activities for under-\nstanding their preferences and phone usage behavior. For instance, smart mobile phones \nhave the ability to log various types of context data related to a user’s phone call activities \nabout when the user makes outgoing calls, or accepts, rejects, and misses the incoming \ncalls [23–26]. In addition to such call related meta data, other dimensions of contex-\ntual information such as user location [27], user’s day-to-day situation [28], the social \nrelationship between the caller an callee identified by the individual’s unique phone \ncontact number [29] are also recorded by the smart mobile phones. Thus, call log data \ncollected by the smart mobile phone can be used as a context source to modeling indi-\nvidual mobile phone user behavior in smart context-aware mobile communication sys-\ntems [30]. In addition to voice communication, short message service (SMS) is known \nas text communication service allows the exchange of short text messages of individual \nmobile phone users, using standardized communications rules or protocols. According \nto the International Telecommunication Union [31], short messages have become a mas-\nsive commercial industry, worth over 81 billion dollars globally. The numerous growth \nin the number of mobile phone users in the world has lead to a dramatic increasing of \nspam messages [32]. The SMS log contains all the message including the spam and non-\nspam text messages [32, 33], which can be used in the task of automatic spam filtering \n[25, 32], or predicting good time or bad time to deliver such messages [33].\n\n\n\nPage 6 of 25Sarker J Big Data (2019) 6:95 \n\nWith the rapid development of smartphones, people use these devices for using vari-\nous categories of apps such as Multimedia, Facebook, Gmail, Youtube, Skype, Game [9, \n34]. Thus, smartphone apps log contains these usage with relevant contextual informa-\ntion [8, 9, 35–37]. Such logs can be used for mining the contextual behavioral patterns of \nindividual mobile phone users that is, which app is preferred by a particular user under \na certain context to provide personalized context-aware recommendation. In the real \nworld, a variety of smart mobile applications use notifications in order to inform the \nusers about various kinds of events, news or just to send them reminders or alerts. For \ninstance, the notifications of inviting games on social networks, social or promotional \nemails, or a number of predictive suggestions by various smart phone applications, \ne.g., Twitter, Facebook, LinkedIN, WhatsApp, Viver, Skype, Youtube [7]. The extracted \ncontextual patterns from smartphone notification logs can be used to build intelligent \nmobile notification management systems according to their preferences.\n\nUser navigation in the web in another major activities of individual users. Thus, web \nlog contains the information about user mobile web navigation, web searching, e-mail, \nentertainment, chat, misc, news, TV, netting, travel, sport, banking, and related contex-\ntual information [38–40]. Mining contextual usage patterns from such log data, can be \nused to make accurate context-aware predictions about user navigation and to adapt the \nportal structure according to the needs of users. Similarly, game log contains the infor-\nmation about playing various types such games such as action, adventure, casual, puzzle, \nRPG, strategy, sports etc. of individual mobile phone users, and related contextual infor-\nmation [41]. The extracted contextual patterns from such logs data, can be used to build \npersonalized mobile game recommendation system for individual mobile phone users \naccording to their own preferences.\n\nThe ubiquity of smart mobile phones and their computing capabilities for various real \nlife purposes provide an opportunity of using these devices as a life-logging device, i.e., \npersonal e-memories [42]. In a more technical sense, life-logs sense and store individ-\nual’s contextual information from their surrounding environment through a variety of \nsensors available in their smart mobile phones, which are the core components of life-\nlogs such as user phone calls, SMS headers (no content), App use (e.g., Skype, What-\nsapp, Youtube etc.), physical activities form Google play API, and related contextual \ninformation such as WiFi and Bluetooth devices in user’s proximity, geographical loca-\ntion, temporal information [42]. The extracted contextual patterns or behavioral rules of \nindividual mobile phone users utilizing such life log data, can be used to improve user \nexperience in their daily life. In addition to these personalized log data, smartphones are \nalso capable for collecting and processing IoT data [1]. Based on such smartphone data \nhaving contextual information, in this paper, we briefly review the existing rule learn-\ning strategies and discuss the open challenges and opportunities by highlighting future \ndirections for context-aware rule learning.\n\nContext‑aware rule learning strategies\nIn this section, we review existing strategies related to learning rules based on contex-\ntual information in various perspectives. This includes time-series modeling that cre-\nates behavioral data clusters for generating temporal context based rules, contextual rule \n\n\n\nPage 7 of 25Sarker J Big Data (2019) 6:95 \n\ndiscovery by taking into account multi-dimensional contexts, such as temporal, spatial \nor social contexts, and incremental learning to dynamic updating of rules.\n\nModeling time‑series smartphone data\n\nTime is the most important context that impacts on mobile user behavior for making \ndecisions [38]. Individual’s behaviors vary over time in the real world and the mobile \nphones record the exact time of all diverse activities of the users with their mobile \nphones. A time series is a sequence of data points ordered in time [43]. However, to use \nsuch time-series data into behavioral rules, an effective modeling of temporal context \nis needed. Thus, time-series segmentation becomes one of the research focuses in this \nstudy as exact time in mobile phone data is not very informative to mine behavioral rules \nof individual mobile phone users. According to [44], time-based behavior modeling is an \nopen problem. Hence, we summarize the existing time-series segmentation approaches \n\nTable 1 Various types of static time segments used in different applications\n\nTime interval type Number \nof segments\n\nUsed time interval and segment details References\n\nEqual 3 Morning [7:00–12:00], afternoon [13:00–18:00] and \nevening [19:00–24:00]\n\nSong et al. [46]\n\nEqual 3 [0:00–7:59], [8:00–15:59] and [16:00–23:59] Rawassizadeh et al. [47]\n\nEqual 4 Morning [6:00–12:00], afternoon [12:00–18:00], \nevening [18:00–24:00] and night [0:00–6:00]\n\nMukherji et al. [48]\n\nEqual 4 Morning [6:00–12:00], afternoon [12:00–18:00], \nevening [18:00–24:00] and night [0:00–6:00]\n\nBayir et al. [49]\n\nEqual 4 Morning, afternoon, evening and night Paireekreng et al. [41]\n\nEqual 4 Morning [6:00–11:59], day [12:00–17:59], evening \n[18:00–23:59], overnight [0:00–5:59]\n\nJayarajah et al. [50]\n\nEqual 4 Night [0:00–6:00 a.m.], morning [6:00 a.m.–12:00 \np.m.], afternoon [12:00–6:00 p.m.], and evening \n[6:00 p.m.–0:00 a.m.]\n\nDo et al. [51]\n\nUnequal 3 Morning (beginning at 6:00 a.m. and ending at \nnoon), afternoon (ending at 6:00 p.m.), night (all \nremaining hours)\n\nXu et al. [52]\n\nUnequal 4 Morning [6:00–12:00], afternoon [12:00–16:00], \nevening [16:00–20:00] and night [20:00–24:00 \nand 0:00–6:00]\n\nMehrotra et al. [7]\n\nUnequal 5 Morning [7:00–11:00], noon [11:00–14:00], after-\nnoon [14:00–18:00] and so on\n\nZhu et al. [9]\n\nUnequal 5 Morning, forenoon, afternoon, evening, and night Oulasvirta et al. [53]\n\nUnequal 5 Morning [7:00–11:00], noon [11:00–14:00], after-\nnoon [14:00–18:00], evening [18:00–21:00], and \nnight [21:00–Next day 7:00]\n\nYu et al. [54]\n\nUnequal > 5 Early morning, morning, late morning, midnight \nand so on\n\nNaboulsi et al. [55]\n\nUnequal > 5 Early morning, morning, late morning, midnight \nand so on\n\nDashdorj et al. [56]\n\nUnequal > 5 Early morning, morning, late morning, midnight \nand so on\n\nShin et al. [57]\n\nUnequal 8 S1[0:00–7:00 a.m.], S2[7:00–9:00 a.m.], S3[9:00–\n11:00 a.m.], S4[11:00 a.m.–2:00 p.m.], S5[2:00–\n5:00 p.m.], S6[5:00–7:00 p.m.], S7[7:00–9:00 p.m.] \nand S8[9:00 p.m.–12:00 a.m.]\n\nFarrahi et al. [58]\n\n\n\nPage 8 of 25Sarker J Big Data (2019) 6:95 \n\ninto two broad categories; (i) static segmentation, and (ii) dynamic segmentation, that \nare used in various mobile applications.\n\nStatic segmentation\n\nA static segmentation is easy to understand and can be useful to analyze population \nbehavior comparing across the mobile phone users. In order to generate segments, \nrecently, most of the researchers (shown in Table 1) take into account only the temporal \ncoverage (24-h-a-day) and statically segment time into arbitrary categories (e.g., morn-\ning) or periods (e.g., 1 h). Such static segmentation of time mainly focuses on time inter-\nvals. According to [45], there are mainly two types of time intervals: one is equal and \nanother one is unequal. For instance, four different time segments, i.e., morning [6:00–\n12:00], afternoon [12:00–18:00], evening [18:00–24:00] and night [0:00–6:00] can be an \nexample of equal interval based segmentation because of their same interval length. On \nthe other hand, another four time slots such as morning [6:00–12:00], afternoon [12:00–\n16:00], evening [16:00–20:00] and night [20:00–24:00 and 0:00–6:00] can be an example \nof unequal interval based segmentation. For this example, different lengths of time inter-\nval are used to do the segmentation. In Table 1, we have summarized a number of works \nthat use static segmentation considering either equal or unequal time interval in various \npurposes.\n\nAlthough, various time intervals and corresponding segmentation summarized in \nTable 1 are used in different purposes, these approaches take into account a fixed num-\nber of segments for all users. However, while performing such segmentation users’ behav-\nioral evidence that differs from user-to-user over time in the real world, is not taken into \naccount. Thus, these static generation of segments may not suitable for producing high \nconfidence temporal rules for individual smartphone users. For instance, N1 number \nof segments might give meaningful results for one case, while N2 number of segments \ncould give better results for another case, where N1 = N2 . Therefore, a dynamic segmen-\ntation of time rather than statically generation could be able to reflect individuals’ behav-\nioral evidence over time and can play a role to produce high confidence rules according \nto their usage records.\n\nDynamic segmentation\n\nAs discussed above, a segmentation technique that generates variable number of seg-\nments would be more meaningful to model users’ behavior. Thus, dynamic segmenta-\ntion technique rather than static segmentation can be used in order to achieve the goal. \nIn a dynamic segmentation, the number of segments are not fixed and predefined; may \nchange depending on their behavioral characteristics, patterns or preferences. Several \ndynamic segmentation techniques in terms of generating variable number of segments \nexist for modeling users’ behavioral activities in temporal contexts. A number of authors \nsimply take into account a single parameter, e.g., interval length or base period, to gener-\nate the segments. The number of time segments varies according to this period. If Tmax \nrepresents the whole time period of 24-h-a-day and BP is a base period, then the num-\nber of segments will be Tmax/BP [10]. If the base period increases, the number of time \nsegments decreases and vice-versa. For instance, if the base period is 5 min, then the \nnumber of segments will be the division result of 24-h-a-day and 5. In this example, a \n\n\n\nPage 9 of 25Sarker J Big Data (2019) 6:95 \n\nbase period, e.g., 5 min, is assumed as the finest granularity to distinguish day-to-day \nactivities of an individual. If the base period incremented to 15 min, then the number \nof segments decreases, where 15 min can be assumed as the finest granularity. Thus the \nnumber of segments varies based on the base time period. Similarly, individuals’ calen-\ndar schedules and corresponding time boundaries can also be used to determine var-\niable length of time segments, in order to model users’ behavior in temporal context, \nwhich may vary according to users’ preferences [59]. For instance, one user may have a \nparticular event between 1 and 2 p.m., while another may have in another time bound-\nary between 1:30 and 2:30 p.m.. Thus, the time segmentation varies according to their \ndaily life activities scheduled in their personal calendars. Similarly, multiple thresholds, \nsliding window, data shape based approaches are used in several applications, shown \nin Table 2. In addition to these approaches, a number of authors use machine learning \ntechniques such as clustering, genetic algorithm etc. In Table  2, we have summarized \na number of works that use such type of dynamic segmentation techniques in various \npurposes.\n\nClustering highlighted in Table  2 is one of the important machine learning tech-\nniques in forming large time segments where certain user behavior patterns are taken \ninto account. Usually, clustering algorithms are designed with certain assumptions and \nfavor certain type of problems. In this sense, it is not accurate to say ‘best’ in the con-\ntext of clustering algorithms; it depends on specific application [75]. Among the cluster-\ning algorithms the K-means algorithm is the best-known squared error-based clustering \nalgorithm [76]. However, this algorithm needs to specify the initial partitions and fixed \nnumber of clusters K. The convergence centroids also vary with different initial points. \nSometimes this algorithm is influenced by outliers because of mean value calculation. \n\nTable 2 Various types of dynamic time segments used in different applications\n\nBase technique Description References\n\nSingle parameter A predefined value of time interval, e.g., 15 min \nis used to generate segments\n\nOzer et al. [60]\n\nA different value of time interval, e.g., 30 min is \nused for segmentation\n\nDo et al. [61], Farrahi et al. [62]\n\nA relatively large value of the parameter, e.g., \n2-h is used to generate time segments\n\nKaratzoglou et al. [63]\n\nAnother large value of time interval, e.g., 3-h is \nused for segmentation to make the number \nof segments small\n\nPhithakkitnukoon et al. [64]\n\nCalendar Various calendar schedules and corresponding \ntime boundaries are used to model users’ \nbehavior in temporal context\n\nKhail et al. [65], Dekel et al. [66], Zulkernain \net al. [67], Seo et al. [68], Sarker et al. [28, \n59]\n\nMulti-thresholds To identify the lower and upper boundary \nof a particular segment for the purpose of \nsegmenting time-series log data\n\nHalvey et al. [38]\n\nData shape A data shape based time-series data analysis Zhang et al. [45], Shokoohi et al. [69]\n\nSliding window A sliding window is used to analyze time-series \ndata\n\nHartono et al. [70], Keogh et al. [71]\n\nClustering A predefined number of clusters is used to \ndiscover rules from time-series data\n\nDas et al. [72]\n\nGenetic algorithm A genetic algorithm is used to analyze time-\nseries data\n\nLu et al. [73], Kandasamy et al. [74]\n\n\n\nPage 10 of 25Sarker J Big Data (2019) 6:95 \n\nMore importantly, the characteristic of this algorithm might not be directly applicable \nfor the purpose of learning  context-aware rules. The reason is that users’ behave dif-\nferently in different contexts, which also may vary from user-to-user in the real world. \nThus, it’s difficult to assume a number of clusters K to capture their diverse behaviors \neffectively. Another similar K-medoids method [77] is more robust than K-means algo-\nrithm in the presence of outliers because a medoid is less influenced by outliers than a \nmean. Though it minimizes the outlier problem but the other characteristic mismatches \nexist between K-means and the problem of time-series modeling.\n\nAs the size and number of time segments depend on the user’s behavior and it differs \nfrom user-to-user, a bottom-up hierarchical data processing can help to make behavioral \nclusters. Existing hierarchical algorithms are mainly classified as agglomerative methods \nand device methods. However, the device clustering method is not commonly used in \npractice [75]. The simplest and most popular agglomerative clustering is single linkage \n[78] and complete linkage [79]. Another method, nearest neighbor [75], is also similar to \nthe single linkage agglomerative clustering algorithm. All these hierarchical algorithms \nuse a proximity matrix which is generated by computing the distance between a new \ncluster and other clusters. Then according to the matrix value these algorithms succes-\nsiv", "metadata_storage_path": "aHR0cHM6Ly9zdG9yYWdlYWNjb3VudGxpZ2lyay5ibG9iLmNvcmUud2luZG93cy5uZXQvcGFwZXIvczQwNTM3LTAxOS0wMjU4LTQucGRm0", "metadata_author": "Iqbal H. Sarker ", "metadata_title": "Context-aware rule learning from smartphone data: survey, challenges and future directions", "keyphrases": [ "Creative Commons Attribution 4.0 International License", "intelligent decision making strategies", "advanced data analytical techniques", "25Sarker J Big Data", "Context‑aware rule learning", "Intelligent systems Open Access", "machine learning based techniques", "Creative Commons license", "corresponding context-aware rule learning", "Iqbal H. Sarker", "User behavior modeling", "creat iveco mmons", "intelligent context-aware applications", "raw contextual data", "corresponding search interests", "data-driven context-aware systems", "phone data analytics", "original author(s", "corresponding data processing", "multifunctional cell phones", "users’ surrounding environment", "smartphone data", "Data science", "Predictive analytics", "context-aware rules", "Context-aware computing", "author information", "future directions", "recent days", "daily life", "important IoT", "next generation", "wireless connectivity", "work coverage", "developed countries", "recent statistics", "Google Trends", "users’ interest", "other platforms", "Desktop Computer", "Laptop Computer", "current world", "recent developments", "behavioral activities", "wide attention", "high precision", "traditional approaches", "efficient results", "previous work", "rule-based automated", "IoT services", "unrestricted use", "appropriate credit", "doi.org", "1 Swinburne University", "Full list", "Tablet Computer", "last 5 years", "timestamp information", "particular date", "highest point", "personal devices", "Things) devices", "essential part", "world population", "Mobile Phones", "particular term", "SURVEY PAPER", "peak popularity", "challenges", "Introduction", "smartphones", "individuals", "Internet", "capabilities", "number", "Fig.", "Abstract", "ogy", "academia", "industry", "order", "set", "key", "contexts", "comparison", "effective", "article", "area", "discussion", "Clustering", "Classification", "Association", "Personalization", "Time-series", "terms", "distribution", "reproduction", "medium", "source", "link", "changes", "Correspondence", "msarker", "Melbourne", "Australia", "creativecommons", "licenses", "crossmark", "Page", "Figure", "x-axis", "range", "chart", "instance", "value", "maximum", "y-axis", "minimum", "context-aware mobile notification management system", "other broad application areas", "corresponding rule-based prediction model", "data-driven intelligent context-aware systems", "different context-aware test cases", "rule-based intelligent context-aware systems", "smartphone data analytic solutions", "context-aware smart city services", "users’ behavioral activity patterns", "corresponding behavioral activities", "THEN” logical structure", "intelligent eHealth services", "Users’ interest trends", "aware behavioral rules", "relevant contextual data", "contextual data patterns", "contextual data clustering", "different surrounding contexts", "relevant context-aware rules", "machine learning researchers", "machine learning techniques", "relevant contextual information", "data mining techniques", "advanced data analysis", "machine learning rules", "different contexts", "rule-based classification", "raw data", "context-aware  software", "Mobile Phone", "hidden patterns", "data science", "advanced features", "mobile notifications", "mental learning", "social contexts", "personalized services", "rity services", "deeper analysis", "association analysis", "daily basis", "great source", "useful information", "two parts", "temporal context", "tial context", "family members", "specific exceptions", "particular domain", "prac- titioners", "many reasons", "various purposes", "particular timestamp", "numeric values", "world-wide popularity", "ture optimization", "dynamic updating", "The reason", "huge amount", "unknown correlations", "quent part", "individuals’ usage", "phone user", "devices", "result", "antecedent", "others", "example", "Alice", "work", "evening", "general", "preferences", "addition", "cable", "study", "applications", "building", "Jun", "Oct", "Dec", "Feb", "Apr", "Aug", "Desktop", "Tablet", "Laptop", "Several", "selection", "aim", "2016", "Suggested machine learning based framework", "Context-aware rule based applications", "optimal time-based context-aware rules", "context-aware rule learning framework", "smartphone data analyt- ics", "Context-aware rule learning strategies", "real world applications", "rule discovery techniques", "intelligent data-driven decisions", "decision making capability", "rule-based intelligent systems", "Human Computer Interaction", "learning context-aware rules", "corresponding data clustering", "corresponding context-aware rules", "smartphone data analytics", "contextual smartphone data", "incremental learning", "context-aware computing", "updation techniques", "time-series data", "smrtphone data", "diverse behaviors", "several perspectives", "time-series modeling", "various layers", "various perspectives", "different meanings", "different purposes", "numerous areas", "uitous Computing", "Computer-Supported Collaborative", "Ambient Intelligence", "little work", "earlier work", "brief discussion", "detailed analysis", "background information", "main characteristics", "existing research", "brief survey", "first article", "multi-dimensional contexts", "sion” section", "terns", "individual", "activities", "profit", "discretization", "issues", "contributions", "paper", "shortcomings", "role", "knowledge", "remainder", "variety", "notion", "Pervasive", "mobile", "ubiquitous computing community", "ubiquitous computing area", "rent social situation", "Pervasive Computing area", "smartphone usage", "recent works", "broader collection", "defi- nitions", "dis- cussion", "Several studies", "temporal informa", "other hand", "current situation", "computing environment", "available processors", "network capacity", "noise level", "lighting level", "day situations", "social aspects", "surrounding people", "nearby people", "locational information", "environmental information", "surrounding information", "important aspects", "changing environment", "Other definitions", "other researchers", "alternative view", "con- text", "location information", "physical environment", "following definition", "social context", "relevant contexts", "user input", "spatial context", "user environment", "user location", "context-awareness", "objects", "factors", "entity", "users", "categories", "section", "scope", "different", "perspectives", "Schilit", "Brown", "temperature", "Simi", "account", "Ryan", "synonyms", "computer", "Franklin", "Ward", "state", "Hull", "entire", "settings", "Rodden", "resources", "connectivity", "display", "costs", "examples", "Dey", "survey", "person", "place", "interaction", "influence", "personalized mobile game recommendation system", "vidual mobile phone user behavior", "relevant contextual informa- tion", "mobile notification management systems", "related contextual infor- mation", "various smart phone applications", "individual mobile phone users", "personalized context-aware recommendation", "user mobile web navigation", "smart mobile applications", "smart mobile phone", "phone usage behavior", "standardized communications rules", "International Telecommunication Union", "sive commercial industry", "accurate context-aware predictions", "daily life activities", "text communication service", "contextual behavioral patterns", "automatic spam filtering", "phone call activities", "user diverse activities", "smartphone notification logs", "short text messages", "contextual usage patterns", "short message service", "Contextual smartphone data", "spam text messages", "contextual patterns", "unique phone", "individual users", "User navigation", "short messages", "major activities", "cellular phones", "game log", "various types", "voice communication", "various kinds", "spam messages", "particular user", "web log", "web searching", "data source", "meta data", "log data", "outgoing calls", "incoming calls", "other dimensions", "day situation", "social relationship", "numerous growth", "dramatic increasing", "good time", "bad time", "rapid development", "smartphone apps", "Such logs", "social networks", "predictive suggestions", "portal structure", "context data", "tual information", "context source", "SMS log", "real world", "contact number", "decisions", "everything", "lives", "ability", "caller", "callee", "exchange", "protocols", "81 billion", "task", "people", "Multimedia", "Facebook", "Gmail", "Youtube", "Skype", "notifications", "events", "news", "reminders", "alerts", "games", "promotional", "emails", "Twitter", "LinkedIN", "WhatsApp", "Viver", "intelligent", "entertainment", "chat", "misc", "TV", "netting", "travel", "sport", "banking", "needs", "action", "adventure", "puzzle", "RPG", "strategy", "Context‑aware rule learning strategies", "time‑series smartphone data Time", "Time interval type Number", "temporal context based rules", "existing time-series segmentation approaches", "mobile phone data", "Google play API", "geographical loca- tion", "context-aware rule learning", "segment details References", "personalized log data", "A time series", "user phone calls", "smart mobile phones", "mobile user behavior", "behavioral data clusters", "time-based behavior modeling", "life log data", "static time segments", "related contextual information", "existing rule", "existing strategies", "important context", "contextual rule", "learning rules", "IoT data", "data points", "exact time", "temporal information", "temporal, spatial", "behavioral rules", "user experience", "computing capabilities", "various real", "life purposes", "life-logging device", "technical sense", "surrounding environment", "core components", "life- logs", "SMS headers", "App use", "physical activities", "open challenges", "diverse activities", "effective modeling", "open problem", "Various types", "different applications", "remaining hours", "Unequal 3 Morning", "Unequal 4 Morning", "Unequal 5 Morning", "Bluetooth devices", "Equal 4 Night", "6:00 a", "ubiquity", "opportunity", "memories", "sensors", "content", "sapp", "WiFi", "proximity", "opportunities", "future", "directions", "discovery", "behaviors", "sequence", "research", "Table 1", "afternoon", "Song", "Rawassizadeh", "Mukherji", "Bayir", "Paireekreng", "day", "overnight", "Jayarajah", "Do", "Xu", "Mehrotra", "Zhu", "dynamic segmenta- tion technique", "high confidence temporal rules", "high confidence rules", "four different time segments", "various mobile applications", "dynamic segmen- tation", "mobile phone users", "individual smartphone users", "four time slots", "same interval length", "two broad categories", "users’ behavioral activities", "dynamic segmentation techniques", "unequal interval based", "various time intervals", "Such static segmentation", "unequal time interval", "temporal coverage", "different lengths", "temporal contexts", "arbitrary categories", "two types", "behavioral characteristics", "population behavior", "ioral evidence", "usage records", "seg- ments", "single parameter", "corresponding segmentation", "base period", "static generation", "time period", "late morning", "meaningful results", "variable number", "N1 number", "one case", "N2 number", "forenoon", "night", "Oulasvirta", "Next", "Yu", "Naboulsi", "Dashdorj", "Shin", "11:00 a", "S5", "S8", "Farrahi", "ii", "researchers", "Table", "periods", "1 h", "works", "approaches", "goal", "change", "patterns", "authors", "Tmax", "24-h", "BP", "2:00", "5:00", "7:00", "9:00", "important machine learning tech- niques", "time- series data Lu", "time-series log data Halvey", "Base technique Description References", "data shape based approaches", "squared error-based clustering algorithm", "time-series data analysis", "time-series data Hartono", "time-series data Das", "cluster- ing algorithms", "mean value calculation", "temporal context Khail", "different initial points", "user behavior patterns", "dynamic time segments", "base time period", "Various calendar schedules", "corresponding time boundaries", "time segments decreases", "large time segments", "different value", "large value", "one user", "initial partitions", "time segmentation", "time interval", "predefined value", "clustering algorithms", "division result", "finest granularity", "iable length", "particular event", "personal calendars", "multiple thresholds", "sliding window", "several applications", "specific application", "convergence centroids", "segmentation Do", "upper boundary", "particular segment", "segments Ozer", "genetic algorithm", "K-means algorithm", "users’ behavior", "users’ preferences", "Single parameter", "predefined number", "Tmax/BP", "5 min", "assumptions", "problems", "sense", "clusters", "K.", "outliers", "30 min", "2-h", "Karatzoglou", "small", "Phithakkitnukoon", "Dekel", "Zulkernain", "Seo", "Multi-thresholds", "lower", "Zhang", "Shokoohi", "Keogh", "Kandasamy", "characteristic", "1", "2:30", "single linkage agglomerative clustering algorithm", "bottom-up hierarchical data processing", "popular agglomerative clustering", "other characteristic mismatches", "device clustering method", "similar K-medoids method", "Existing hierarchical algorithms", "agglomerative methods", "complete linkage", "device methods", "other clusters", "users’ behave", "different contexts", "time segments", "nearest neighbor", "proximity matrix", "new cluster", "matrix value", "behavioral clusters", "outlier problem", "reason", "K-means", "presence", "size", "practice", "simplest", "distance", "siv" ], "merged_content": "\nContext‑aware rule learning \nfrom smartphone data: survey, challenges \nand future directions\nIqbal H. Sarker1,2*\n\nIntroduction\nIn recent days, smartphones have become an essential part of our daily life and con-\nsidered as highly personal devices of individuals. These devices are also known as one \nof the most important IoT (Internet of Things) devices, because of their capabilities \nto interconnect their users with the Internet, and corresponding data processing [1]. \nSmartphones are also considered as “next generation, multifunctional cell phones that \nfacilitates data processing as well as enhanced wireless connectivity” [2]. The cellular net-\nwork coverage has reached 96.8% of the world population, and this number even reaches \n100% of the population in the developed countries [3]. In recent statistics, according to \nGoogle Trends [4] we have shown in Fig.  1, that users’ interest on “Mobile Phones” is \nmore and more than other platforms like “Desktop Computer”, “Laptop Computer” or \n\nAbstract \n\nSmartphones are considered as one of the most essential and highly personal devices \nof individuals in our current world. Due to the popularity of context-aware technol-\nogy and recent developments in smartphones, these devices can collect and process \nraw contextual data about users’ surrounding environment and their corresponding \nbehavioral activities with their phones. Thus, smartphone data analytics and building \ndata-driven context-aware systems have gained wide attention from both academia \nand industry in recent days. In order to build intelligent context-aware applications on \nsmartphones, effectively learning a set of context-aware rules from smartphone data \nis the key. This requires advanced data analytical techniques with high precision and \nintelligent decision making strategies based on contexts. In comparison to traditional \napproaches, machine learning based techniques provide more effective and efficient \nresults for smartphone data analytics and corresponding context-aware rule learning. \nThus, this article first makes a survey on previous work in the area of contextual smart-\nphone data analytics and then presents a discussion of challenges and future directions \nfor effectively learning context-aware rules from smartphone data, in order to build \nrule-based automated and intelligent systems.\n\nKeywords: Smartphone data, Machine learning, Data science, Clustering, \nClassification, Association, Rule learning, Personalization, Time-series, User behavior \nmodeling, Predictive analytics, Context-aware computing, Mobile and IoT services, \nIntelligent systems\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nSURVEY PAPER\n\nSarker J Big Data (2019) 6:95 \nhttps://doi.org/10.1186/s40537‑019‑0258‑4\n\n*Correspondence: \nmsarker@swin.edu.au \n1 Swinburne University \nof Technology, \nMelbourne VIC-3122, \nAustralia\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-019-0258-4&domain=pdf\n\n\nPage 2 of 25Sarker J Big Data (2019) 6:95 \n\n“Tablet Computer” for the last 5 years from 2014 to 2019. Figure 1 represents timestamp \ninformation in terms of particular date in x-axis and corresponding search interests in \nthe range of 0 to 100 in terms of popularity relative to the highest point on the chart in \ny-axis. For instance, a value of 100 (maximum) in y-axis represents the peak popularity \nfor a particular term, while 0 (minimum) means the term was lowest in terms of popu-\nlarity [4].\n\nDue to the advanced features and recent developments in smartphones, these devices \ncan collect raw contextual data about users’ surrounding environment and their corre-\nsponding behavioral activities with their phones in a daily basis [5]. As a result, smart-\nphone data becomes a great source to understand users’ behavioral activity patterns in \ndifferent contexts, and to derive useful information, i.e., context-aware rules, for the pur-\npose of building rule-based intelligent context-aware systems. A context-aware rule has \ntwo parts, which follows “IF-THEN” logical structure to formulate [6]. The antecedent \npart represents users’ surrounding contextual information, e.g., temporal context, spa-\ntial context, social contexts, or others relevant contextual information and the conse-\nquent part represents their corresponding behavioral activities or usage. Let’s consider \nan example of a context-aware mobile notification management system for a smart-\nphone user Alice. A context-aware rule for such system could be “The user typically \ndismisses mobile notifications while at work; however, accepts the notifications in the \nevening from her family members, even though she is in work”. A set of such context-\naware behavioral rules including general and specific exceptions, may vary from user-to-\nuser according to their preferences. In addition to the personalized services mentioned \nabove, the relevant context-aware rules in different surrounding contexts could be appli-\ncable to other broad application areas, like context-aware  software and IoT services, \nintelligent eHealth services, and context-aware smart city services, intelligent cybersecu-\nrity services etc. utilizing the relevant contextual data of that particular domain. Overall, \nthis study is typically for those data science and machine learning researchers, and prac-\ntitioners who particularly want to work on data-driven intelligent context-aware systems \nand services based on machine learning rules.\n\nEffectively learning context-aware rules from smartphone data is challenging because \nof many reasons, ranging from understanding raw data to applications. A number of \nresearch [7–9] has been done on mining context-aware rules from smartphone data for \nvarious purposes. However, to effectively learn such rules for the purpose of building \n\nFig. 1 Users’ interest trends over time, where x-axis and y-axis represent a particular timestamp and \ncorresponding search interests in numeric values in terms of world-wide popularity respectively\n\n 100 30 50 20 80 40 10 60 O 90 70 13-Apr-2014 13-Jun-2014 13-Aug-2014 13-Oct-2014 13-Dec-2014 13-Feb-2015 13-Apr-2015 13-Jun-2015 13-Aug-2015 13-Oct-2015 Desktop 13-Dec-2015 13-Feb-2016 13-Apr-2016 -Tablet 13-Jun-2016 13-Aug-2016 13-Oct-2016 Laptop 13-Dec-2016 13-Feb-2017 13-Apr-2017 13-Jun-2017 -Mobile Phone 13-Aug-2017 13-Oct-2017 13-Dec-2017 13-Feb-2018 13-Apr-2018 13-Jun-2018 13-Aug-2018 13-Oct-2018 13-Dec-2018 13-Feb-2019 \n\n\n\nPage 3 of 25Sarker J Big Data (2019) 6:95 \n\nintelligent context-aware systems, a deeper analysis in contextual data patterns and \nlearning according to individuals’ usage is needed. Thus, advanced data analysis based \non machine learning techniques, can be used to make effective and efficient decision-\nmaking capabilities in different context-aware test cases for smartphones. Several \nmachine learning and data mining techniques, such as contextual data clustering, fea-\nture optimization and selection, rule-based classification and association analysis, incre-\nmental learning for dynamic updating and management, and corresponding rule-based \nprediction model can be designed to provide smartphone data analytic solutions. The \nreason is that such machine learning techniques can be more accurate, and more precise \nfor analyzing huge amount of contextual data. The aim of these advanced analytic tech-\nniques is to discover information, hidden patterns, and unknown correlations among the \ncontexts and eventually generate context-aware rules. For instance, a detailed analysis \nof time-series data and corresponding data clustering based on similar behavioral pat-\nterns, could lead to capture the diverse behaviors of an individual’s activities, thereby \nenabling more optimal time-based context-aware rules than the traditional approaches \n[10]. Thus, intelligent data-driven decisions using machine learning techniques can \nprofit better decision making capability over the traditional approaches while consider-\ning the multi-dimensional contexts.\n\nBased on our survey and analysis on existing research, little work has been done in \nterms of how machine learning techniques significantly impact on contextual smart-\nphone data and to learn corresponding context-aware rules. To address this short-\ncoming, this article first makes a survey on previous work in the area of contextual \nsmartphone data analytics in several perspectives involved in context-aware rules, such \nas time-series modeling that is also known as a discretization of temporal context, rule \ndiscovery techniques, and incremental learning and rule updation techniques, which has \nbeen highlighted in our earlier work [6]. After that this article presents a brief discussion \non challenges and future directions to overcome these issues. Based on our discussion, \nfinally we suggest a machine learning based context-aware rule learning framework for \nthe purpose of effectively learning context-aware rules from smartphone data, in order \nto build rule-based automated and intelligent systems.\n\nThe contributions of this paper are summarized as follows.\n\n• We first make a brief survey on previous work in the area of smartphone data analyt-\nics in several perspectives related to context-aware rule learning and summarize the \nshortcomings of these research.\n\n• We then present a brief discussion on the challenges and future directions to over-\ncome the issues to learn context-aware rules from smartphone data.\n\n• Finally, we suggest a machine learning based context-aware rule learning framework \nand briefly discuss the role of various layers associated with the framework, for the \npurpose of building rule-based intelligent context-aware systems.\n\nTo the best of our knowledge, this is the first article surveying context-aware rule learn-\ning strategies from smrtphone data. The remainder of the paper is organized as follows. \n“Background: contexts and smartphone data” section presents background information \non contexts and contextual smartphone data. “Context-aware rule learning strategies” \n\n\n\nPage 4 of 25Sarker J Big Data (2019) 6:95 \n\nsection  surveys previous work in various perspectives related to context-aware rule \nlearning. “Challenges and future directions” section briefly discusses the challenges and \nfuture directions of research regarding context-aware rule learning from smartphone \ndata. In “Suggested machine learning based framework” section we suggest a machine \nlearning based context-aware rule learning framework and discuss various layers with \ntheir roles while learning rules. Context-aware rule based applications section summa-\nrizes a number of real world applications based on context-aware rules. Finally, “Conclu-\nsion” section concludes this paper.\n\nBackground: contexts and smartphone data\nThis section reviews background information on the main characteristics of contexts \nand contextual smartphone data that address learning context-aware rules for the pur-\npose of building rule-based intelligent systems.\n\nCharacteristics of contexts\n\nThe term context can be used with a variety of different meanings in different purposes. \nThe notion of context has been used in numerous areas, including Pervasive and Ubiq-\nuitous Computing, Human Computer Interaction, Computer-Supported Collaborative \nWork, and Ambient Intelligence [11]. In this section, first we briefly review what is con-\ntext in the area of mobile and context-aware computing. In Ubiquitous and Pervasive \nComputing area, early works on context-awareness referred to context as primarily \nthe location of people and objects [12]. In recent works, context has been extended to \ninclude a broader collection of factors, such as physical and social aspects of an entity, \nas well as the activities of users [11]. Having examined the definitions and categories of \ncontext given by the pervasive and ubiquitous computing community, this section seeks \nto define our view of context within the scope of smartphone data analytics. As the defi-\nnitions of context to pervasive and ubiquitous computing area are also broad, this dis-\ncussion is intended to be illustrative rather than exhaustive.\n\nSeveral studies have attempted to define and represent the context from different \nperspectives. For instance, the user’s location information, the surrounding people and \nobjects around the user, and the changes to those objects are considered as contexts by \nSchilit et al. [12]. Brown et al. [13] also define contexts as user’s locational information, \ntemporal information, the surrounding people around the user, temperature, etc. Simi-\nlarly, the user’s locational information, environmental information, temporal informa-\ntion, user’s identity, are also taken into account as contexts by Ryan et  al. [14]. Other \ndefinitions of context have simply provided synonyms for context such as context as the \nenvironment or social situation. A number of researchers are taken into account the \ncontext as the environmental information of the user. For instance, in [15], the environ-\nmental information that the user’s computer knows about are taken into account as con-\ntext by Brown et al., whereas the social situation of the user is considered as a context \nin Franklin et al. [16]. On the other hand, a number of other researchers consider it to \nbe the environment related to the applications. For instance, Ward et al. [17] consider \nthe state of the surrounding information of the applications as contexts. Hull et al. [18] \ndefine context as the aspects of the current situation of the user and include the entire \n\n\n\nPage 5 of 25Sarker J Big Data (2019) 6:95 \n\nenvironment. The settings of applications are also treated as context in Rodden et  al. \n[19].\n\nAccording to Schilit et  al. [20] the important aspects of context are: (i) where you \nare, (ii) whom you are with, and (iii) what resources are nearby. The information of the \nchanging environment is taken into account as context in their definition. In addition to \nthe user environment (e.g., user location, nearby people around the user, and the cur-\nrent social situation of the user), they also include the computing environment and the \nphysical environment. For instance, connectivity, available processors, user input and \ndisplay, network capacity, and costs of computing can be the examples of the computing \nenvironment, while the noise level, temperature, the lighting level, can be the examples \nof the physical environment. Dey et al. [21] present a survey of alternative view of con-\ntext, which are largely imprecise and indirect, typically defining context by synonym or \nexample. Finally, they offer the following definition of context, which is perhaps now the \nmost widely accepted. According to Dey et al. [21] “Context is any information that can \nbe used to characterize the situation of an entity. An entity is person, place or object \nthat is considered relevant to the interaction between a user and an application, includ-\ning the user and the application themselves”. Thus, based on the definition of Dey et al. \n[21], we can define context in the scope of this work as “Context is any information that \ncan be used to characterize users’ day-to-day situations that have an influence on their \nsmartphone usage”. An example of relevant contexts could be temporal context, spatial \ncontext, or social context etc. that might have an influence to make individuals’ diverse \ndecisions on smartphone usage in their daily life activities.\n\nContextual smartphone data\n\nWe live in the age of data [22], where everything that surrounds us is linked to a data \nsource and everything in our lives is captured digitally. Mobile or cellular phones have \nbecome increasingly ubiquitous and powerful to log user diverse activities for under-\nstanding their preferences and phone usage behavior. For instance, smart mobile phones \nhave the ability to log various types of context data related to a user’s phone call activities \nabout when the user makes outgoing calls, or accepts, rejects, and misses the incoming \ncalls [23–26]. In addition to such call related meta data, other dimensions of contex-\ntual information such as user location [27], user’s day-to-day situation [28], the social \nrelationship between the caller an callee identified by the individual’s unique phone \ncontact number [29] are also recorded by the smart mobile phones. Thus, call log data \ncollected by the smart mobile phone can be used as a context source to modeling indi-\nvidual mobile phone user behavior in smart context-aware mobile communication sys-\ntems [30]. In addition to voice communication, short message service (SMS) is known \nas text communication service allows the exchange of short text messages of individual \nmobile phone users, using standardized communications rules or protocols. According \nto the International Telecommunication Union [31], short messages have become a mas-\nsive commercial industry, worth over 81 billion dollars globally. The numerous growth \nin the number of mobile phone users in the world has lead to a dramatic increasing of \nspam messages [32]. The SMS log contains all the message including the spam and non-\nspam text messages [32, 33], which can be used in the task of automatic spam filtering \n[25, 32], or predicting good time or bad time to deliver such messages [33].\n\n\n\nPage 6 of 25Sarker J Big Data (2019) 6:95 \n\nWith the rapid development of smartphones, people use these devices for using vari-\nous categories of apps such as Multimedia, Facebook, Gmail, Youtube, Skype, Game [9, \n34]. Thus, smartphone apps log contains these usage with relevant contextual informa-\ntion [8, 9, 35–37]. Such logs can be used for mining the contextual behavioral patterns of \nindividual mobile phone users that is, which app is preferred by a particular user under \na certain context to provide personalized context-aware recommendation. In the real \nworld, a variety of smart mobile applications use notifications in order to inform the \nusers about various kinds of events, news or just to send them reminders or alerts. For \ninstance, the notifications of inviting games on social networks, social or promotional \nemails, or a number of predictive suggestions by various smart phone applications, \ne.g., Twitter, Facebook, LinkedIN, WhatsApp, Viver, Skype, Youtube [7]. The extracted \ncontextual patterns from smartphone notification logs can be used to build intelligent \nmobile notification management systems according to their preferences.\n\nUser navigation in the web in another major activities of individual users. Thus, web \nlog contains the information about user mobile web navigation, web searching, e-mail, \nentertainment, chat, misc, news, TV, netting, travel, sport, banking, and related contex-\ntual information [38–40]. Mining contextual usage patterns from such log data, can be \nused to make accurate context-aware predictions about user navigation and to adapt the \nportal structure according to the needs of users. Similarly, game log contains the infor-\nmation about playing various types such games such as action, adventure, casual, puzzle, \nRPG, strategy, sports etc. of individual mobile phone users, and related contextual infor-\nmation [41]. The extracted contextual patterns from such logs data, can be used to build \npersonalized mobile game recommendation system for individual mobile phone users \naccording to their own preferences.\n\nThe ubiquity of smart mobile phones and their computing capabilities for various real \nlife purposes provide an opportunity of using these devices as a life-logging device, i.e., \npersonal e-memories [42]. In a more technical sense, life-logs sense and store individ-\nual’s contextual information from their surrounding environment through a variety of \nsensors available in their smart mobile phones, which are the core components of life-\nlogs such as user phone calls, SMS headers (no content), App use (e.g., Skype, What-\nsapp, Youtube etc.), physical activities form Google play API, and related contextual \ninformation such as WiFi and Bluetooth devices in user’s proximity, geographical loca-\ntion, temporal information [42]. The extracted contextual patterns or behavioral rules of \nindividual mobile phone users utilizing such life log data, can be used to improve user \nexperience in their daily life. In addition to these personalized log data, smartphones are \nalso capable for collecting and processing IoT data [1]. Based on such smartphone data \nhaving contextual information, in this paper, we briefly review the existing rule learn-\ning strategies and discuss the open challenges and opportunities by highlighting future \ndirections for context-aware rule learning.\n\nContext‑aware rule learning strategies\nIn this section, we review existing strategies related to learning rules based on contex-\ntual information in various perspectives. This includes time-series modeling that cre-\nates behavioral data clusters for generating temporal context based rules, contextual rule \n\n\n\nPage 7 of 25Sarker J Big Data (2019) 6:95 \n\ndiscovery by taking into account multi-dimensional contexts, such as temporal, spatial \nor social contexts, and incremental learning to dynamic updating of rules.\n\nModeling time‑series smartphone data\n\nTime is the most important context that impacts on mobile user behavior for making \ndecisions [38]. Individual’s behaviors vary over time in the real world and the mobile \nphones record the exact time of all diverse activities of the users with their mobile \nphones. A time series is a sequence of data points ordered in time [43]. However, to use \nsuch time-series data into behavioral rules, an effective modeling of temporal context \nis needed. Thus, time-series segmentation becomes one of the research focuses in this \nstudy as exact time in mobile phone data is not very informative to mine behavioral rules \nof individual mobile phone users. According to [44], time-based behavior modeling is an \nopen problem. Hence, we summarize the existing time-series segmentation approaches \n\nTable 1 Various types of static time segments used in different applications\n\nTime interval type Number \nof segments\n\nUsed time interval and segment details References\n\nEqual 3 Morning [7:00–12:00], afternoon [13:00–18:00] and \nevening [19:00–24:00]\n\nSong et al. [46]\n\nEqual 3 [0:00–7:59], [8:00–15:59] and [16:00–23:59] Rawassizadeh et al. [47]\n\nEqual 4 Morning [6:00–12:00], afternoon [12:00–18:00], \nevening [18:00–24:00] and night [0:00–6:00]\n\nMukherji et al. [48]\n\nEqual 4 Morning [6:00–12:00], afternoon [12:00–18:00], \nevening [18:00–24:00] and night [0:00–6:00]\n\nBayir et al. [49]\n\nEqual 4 Morning, afternoon, evening and night Paireekreng et al. [41]\n\nEqual 4 Morning [6:00–11:59], day [12:00–17:59], evening \n[18:00–23:59], overnight [0:00–5:59]\n\nJayarajah et al. [50]\n\nEqual 4 Night [0:00–6:00 a.m.], morning [6:00 a.m.–12:00 \np.m.], afternoon [12:00–6:00 p.m.], and evening \n[6:00 p.m.–0:00 a.m.]\n\nDo et al. [51]\n\nUnequal 3 Morning (beginning at 6:00 a.m. and ending at \nnoon), afternoon (ending at 6:00 p.m.), night (all \nremaining hours)\n\nXu et al. [52]\n\nUnequal 4 Morning [6:00–12:00], afternoon [12:00–16:00], \nevening [16:00–20:00] and night [20:00–24:00 \nand 0:00–6:00]\n\nMehrotra et al. [7]\n\nUnequal 5 Morning [7:00–11:00], noon [11:00–14:00], after-\nnoon [14:00–18:00] and so on\n\nZhu et al. [9]\n\nUnequal 5 Morning, forenoon, afternoon, evening, and night Oulasvirta et al. [53]\n\nUnequal 5 Morning [7:00–11:00], noon [11:00–14:00], after-\nnoon [14:00–18:00], evening [18:00–21:00], and \nnight [21:00–Next day 7:00]\n\nYu et al. [54]\n\nUnequal > 5 Early morning, morning, late morning, midnight \nand so on\n\nNaboulsi et al. [55]\n\nUnequal > 5 Early morning, morning, late morning, midnight \nand so on\n\nDashdorj et al. [56]\n\nUnequal > 5 Early morning, morning, late morning, midnight \nand so on\n\nShin et al. [57]\n\nUnequal 8 S1[0:00–7:00 a.m.], S2[7:00–9:00 a.m.], S3[9:00–\n11:00 a.m.], S4[11:00 a.m.–2:00 p.m.], S5[2:00–\n5:00 p.m.], S6[5:00–7:00 p.m.], S7[7:00–9:00 p.m.] \nand S8[9:00 p.m.–12:00 a.m.]\n\nFarrahi et al. [58]\n\n\n\nPage 8 of 25Sarker J Big Data (2019) 6:95 \n\ninto two broad categories; (i) static segmentation, and (ii) dynamic segmentation, that \nare used in various mobile applications.\n\nStatic segmentation\n\nA static segmentation is easy to understand and can be useful to analyze population \nbehavior comparing across the mobile phone users. In order to generate segments, \nrecently, most of the researchers (shown in Table 1) take into account only the temporal \ncoverage (24-h-a-day) and statically segment time into arbitrary categories (e.g., morn-\ning) or periods (e.g., 1 h). Such static segmentation of time mainly focuses on time inter-\nvals. According to [45], there are mainly two types of time intervals: one is equal and \nanother one is unequal. For instance, four different time segments, i.e., morning [6:00–\n12:00], afternoon [12:00–18:00], evening [18:00–24:00] and night [0:00–6:00] can be an \nexample of equal interval based segmentation because of their same interval length. On \nthe other hand, another four time slots such as morning [6:00–12:00], afternoon [12:00–\n16:00], evening [16:00–20:00] and night [20:00–24:00 and 0:00–6:00] can be an example \nof unequal interval based segmentation. For this example, different lengths of time inter-\nval are used to do the segmentation. In Table 1, we have summarized a number of works \nthat use static segmentation considering either equal or unequal time interval in various \npurposes.\n\nAlthough, various time intervals and corresponding segmentation summarized in \nTable 1 are used in different purposes, these approaches take into account a fixed num-\nber of segments for all users. However, while performing such segmentation users’ behav-\nioral evidence that differs from user-to-user over time in the real world, is not taken into \naccount. Thus, these static generation of segments may not suitable for producing high \nconfidence temporal rules for individual smartphone users. For instance, N1 number \nof segments might give meaningful results for one case, while N2 number of segments \ncould give better results for another case, where N1 = N2 . Therefore, a dynamic segmen-\ntation of time rather than statically generation could be able to reflect individuals’ behav-\nioral evidence over time and can play a role to produce high confidence rules according \nto their usage records.\n\nDynamic segmentation\n\nAs discussed above, a segmentation technique that generates variable number of seg-\nments would be more meaningful to model users’ behavior. Thus, dynamic segmenta-\ntion technique rather than static segmentation can be used in order to achieve the goal. \nIn a dynamic segmentation, the number of segments are not fixed and predefined; may \nchange depending on their behavioral characteristics, patterns or preferences. Several \ndynamic segmentation techniques in terms of generating variable number of segments \nexist for modeling users’ behavioral activities in temporal contexts. A number of authors \nsimply take into account a single parameter, e.g., interval length or base period, to gener-\nate the segments. The number of time segments varies according to this period. If Tmax \nrepresents the whole time period of 24-h-a-day and BP is a base period, then the num-\nber of segments will be Tmax/BP [10]. If the base period increases, the number of time \nsegments decreases and vice-versa. For instance, if the base period is 5 min, then the \nnumber of segments will be the division result of 24-h-a-day and 5. In this example, a \n\n\n\nPage 9 of 25Sarker J Big Data (2019) 6:95 \n\nbase period, e.g., 5 min, is assumed as the finest granularity to distinguish day-to-day \nactivities of an individual. If the base period incremented to 15 min, then the number \nof segments decreases, where 15 min can be assumed as the finest granularity. Thus the \nnumber of segments varies based on the base time period. Similarly, individuals’ calen-\ndar schedules and corresponding time boundaries can also be used to determine var-\niable length of time segments, in order to model users’ behavior in temporal context, \nwhich may vary according to users’ preferences [59]. For instance, one user may have a \nparticular event between 1 and 2 p.m., while another may have in another time bound-\nary between 1:30 and 2:30 p.m.. Thus, the time segmentation varies according to their \ndaily life activities scheduled in their personal calendars. Similarly, multiple thresholds, \nsliding window, data shape based approaches are used in several applications, shown \nin Table 2. In addition to these approaches, a number of authors use machine learning \ntechniques such as clustering, genetic algorithm etc. In Table  2, we have summarized \na number of works that use such type of dynamic segmentation techniques in various \npurposes.\n\nClustering highlighted in Table  2 is one of the important machine learning tech-\nniques in forming large time segments where certain user behavior patterns are taken \ninto account. Usually, clustering algorithms are designed with certain assumptions and \nfavor certain type of problems. In this sense, it is not accurate to say ‘best’ in the con-\ntext of clustering algorithms; it depends on specific application [75]. Among the cluster-\ning algorithms the K-means algorithm is the best-known squared error-based clustering \nalgorithm [76]. However, this algorithm needs to specify the initial partitions and fixed \nnumber of clusters K. The convergence centroids also vary with different initial points. \nSometimes this algorithm is influenced by outliers because of mean value calculation. \n\nTable 2 Various types of dynamic time segments used in different applications\n\nBase technique Description References\n\nSingle parameter A predefined value of time interval, e.g., 15 min \nis used to generate segments\n\nOzer et al. [60]\n\nA different value of time interval, e.g., 30 min is \nused for segmentation\n\nDo et al. [61], Farrahi et al. [62]\n\nA relatively large value of the parameter, e.g., \n2-h is used to generate time segments\n\nKaratzoglou et al. [63]\n\nAnother large value of time interval, e.g., 3-h is \nused for segmentation to make the number \nof segments small\n\nPhithakkitnukoon et al. [64]\n\nCalendar Various calendar schedules and corresponding \ntime boundaries are used to model users’ \nbehavior in temporal context\n\nKhail et al. [65], Dekel et al. [66], Zulkernain \net al. [67], Seo et al. [68], Sarker et al. [28, \n59]\n\nMulti-thresholds To identify the lower and upper boundary \nof a particular segment for the purpose of \nsegmenting time-series log data\n\nHalvey et al. [38]\n\nData shape A data shape based time-series data analysis Zhang et al. [45], Shokoohi et al. [69]\n\nSliding window A sliding window is used to analyze time-series \ndata\n\nHartono et al. [70], Keogh et al. [71]\n\nClustering A predefined number of clusters is used to \ndiscover rules from time-series data\n\nDas et al. [72]\n\nGenetic algorithm A genetic algorithm is used to analyze time-\nseries data\n\nLu et al. [73], Kandasamy et al. [74]\n\n\n\nPage 10 of 25Sarker J Big Data (2019) 6:95 \n\nMore importantly, the characteristic of this algorithm might not be directly applicable \nfor the purpose of learning  context-aware rules. The reason is that users’ behave dif-\nferently in different contexts, which also may vary from user-to-user in the real world. \nThus, it’s difficult to assume a number of clusters K to capture their diverse behaviors \neffectively. Another similar K-medoids method [77] is more robust than K-means algo-\nrithm in the presence of outliers because a medoid is less influenced by outliers than a \nmean. Though it minimizes the outlier problem but the other characteristic mismatches \nexist between K-means and the problem of time-series modeling.\n\nAs the size and number of time segments depend on the user’s behavior and it differs \nfrom user-to-user, a bottom-up hierarchical data processing can help to make behavioral \nclusters. Existing hierarchical algorithms are mainly classified as agglomerative methods \nand device methods. However, the device clustering method is not commonly used in \npractice [75]. The simplest and most popular agglomerative clustering is single linkage \n[78] and complete linkage [79]. Another method, nearest neighbor [75], is also similar to \nthe single linkage agglomerative clustering algorithm. All these hierarchical algorithms \nuse a proximity matrix which is generated by computing the distance between a new \ncluster and other clusters. Then according to the matrix value these algorithms succes-\nsiv", "text": [ "100 30 50 20 80 40 10 60 O 90 70 13-Apr-2014 13-Jun-2014 13-Aug-2014 13-Oct-2014 13-Dec-2014 13-Feb-2015 13-Apr-2015 13-Jun-2015 13-Aug-2015 13-Oct-2015 Desktop 13-Dec-2015 13-Feb-2016 13-Apr-2016 -Tablet 13-Jun-2016 13-Aug-2016 13-Oct-2016 Laptop 13-Dec-2016 13-Feb-2017 13-Apr-2017 13-Jun-2017 -Mobile Phone 13-Aug-2017 13-Oct-2017 13-Dec-2017 13-Feb-2018 13-Apr-2018 13-Jun-2018 13-Aug-2018 13-Oct-2018 13-Dec-2018 13-Feb-2019", "Real World Applications and Services Application 1 Application 2 Application N Dynamic Updating and Management Recency Analysis and Layer 4 Mining Rule Updation Rule Discovery Contextual Rule-based Rule Layer 3 Preferences Learning Generalization Context Discretization Time Series Layer 2 Contextual Modeling Data Clustering Contextual Data Acquisition Layer 1 Smartphone External Sensors Logs Sources", "Published online: 31 October 2019" ], "layoutText": [ "{\"language\":\"en\",\"text\":\"100 30 50 20 80 40 10 60 O 90 70 13-Apr-2014 13-Jun-2014 13-Aug-2014 13-Oct-2014 13-Dec-2014 13-Feb-2015 13-Apr-2015 13-Jun-2015 13-Aug-2015 13-Oct-2015 Desktop 13-Dec-2015 13-Feb-2016 13-Apr-2016 -Tablet 13-Jun-2016 13-Aug-2016 13-Oct-2016 Laptop 13-Dec-2016 13-Feb-2017 13-Apr-2017 13-Jun-2017 -Mobile Phone 13-Aug-2017 13-Oct-2017 13-Dec-2017 13-Feb-2018 13-Apr-2018 13-Jun-2018 13-Aug-2018 13-Oct-2018 13-Dec-2018 13-Feb-2019\",\"lines\":[{\"boundingBox\":[{\"x\":0,\"y\":25},{\"x\":37,\"y\":23},{\"x\":37,\"y\":41},{\"x\":0,\"y\":41}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":11,\"y\":247},{\"x\":33,\"y\":246},{\"x\":33,\"y\":263},{\"x\":11,\"y\":264}],\"text\":\"30\"},{\"boundingBox\":[{\"x\":11,\"y\":183},{\"x\":33,\"y\":183},{\"x\":34,\"y\":199},{\"x\":11,\"y\":199}],\"text\":\"50\"},{\"boundingBox\":[{\"x\":10,\"y\":278},{\"x\":37,\"y\":278},{\"x\":37,\"y\":296},{\"x\":10,\"y\":296}],\"text\":\"20\"},{\"boundingBox\":[{\"x\":11,\"y\":88},{\"x\":33,\"y\":88},{\"x\":34,\"y\":105},{\"x\":12,\"y\":105}],\"text\":\"80\"},{\"boundingBox\":[{\"x\":11,\"y\":213},{\"x\":37,\"y\":214},{\"x\":36,\"y\":231},{\"x\":11,\"y\":231}],\"text\":\"40\"},{\"boundingBox\":[{\"x\":11,\"y\":309},{\"x\":38,\"y\":309},{\"x\":38,\"y\":326},{\"x\":12,\"y\":325}],\"text\":\"10\"},{\"boundingBox\":[{\"x\":10,\"y\":152},{\"x\":37,\"y\":151},{\"x\":37,\"y\":168},{\"x\":10,\"y\":168}],\"text\":\"60\"},{\"boundingBox\":[{\"x\":23,\"y\":357},{\"x\":22,\"y\":337},{\"x\":36,\"y\":336},{\"x\":37,\"y\":356}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":10,\"y\":57},{\"x\":38,\"y\":56},{\"x\":37,\"y\":74},{\"x\":10,\"y\":74}],\"text\":\"90\"},{\"boundingBox\":[{\"x\":14,\"y\":119},{\"x\":38,\"y\":119},{\"x\":37,\"y\":136},{\"x\":13,\"y\":136}],\"text\":\"70\"},{\"boundingBox\":[{\"x\":50,\"y\":463},{\"x\":49,\"y\":363},{\"x\":70,\"y\":363},{\"x\":70,\"y\":463}],\"text\":\"13-Apr-2014\"},{\"boundingBox\":[{\"x\":96,\"y\":462},{\"x\":95,\"y\":366},{\"x\":116,\"y\":366},{\"x\":116,\"y\":462}],\"text\":\"13-Jun-2014\"},{\"boundingBox\":[{\"x\":142,\"y\":463},{\"x\":141,\"y\":365},{\"x\":162,\"y\":365},{\"x\":163,\"y\":463}],\"text\":\"13-Aug-2014\"},{\"boundingBox\":[{\"x\":187,\"y\":467},{\"x\":187,\"y\":365},{\"x\":207,\"y\":365},{\"x\":208,\"y\":466}],\"text\":\"13-Oct-2014\"},{\"boundingBox\":[{\"x\":233,\"y\":463},{\"x\":233,\"y\":365},{\"x\":253,\"y\":365},{\"x\":254,\"y\":463}],\"text\":\"13-Dec-2014\"},{\"boundingBox\":[{\"x\":279,\"y\":467},{\"x\":278,\"y\":364},{\"x\":298,\"y\":364},{\"x\":299,\"y\":466}],\"text\":\"13-Feb-2015\"},{\"boundingBox\":[{\"x\":323,\"y\":467},{\"x\":322,\"y\":364},{\"x\":343,\"y\":364},{\"x\":344,\"y\":466}],\"text\":\"13-Apr-2015\"},{\"boundingBox\":[{\"x\":368,\"y\":464},{\"x\":368,\"y\":363},{\"x\":388,\"y\":363},{\"x\":389,\"y\":464}],\"text\":\"13-Jun-2015\"},{\"boundingBox\":[{\"x\":414,\"y\":464},{\"x\":413,\"y\":364},{\"x\":435,\"y\":364},{\"x\":436,\"y\":464}],\"text\":\"13-Aug-2015\"},{\"boundingBox\":[{\"x\":459,\"y\":464},{\"x\":459,\"y\":364},{\"x\":480,\"y\":364},{\"x\":480,\"y\":464}],\"text\":\"13-Oct-2015\"},{\"boundingBox\":[{\"x\":447,\"y\":1},{\"x\":528,\"y\":2},{\"x\":527,\"y\":19},{\"x\":447,\"y\":17}],\"text\":\"Desktop\"},{\"boundingBox\":[{\"x\":506,\"y\":467},{\"x\":506,\"y\":365},{\"x\":526,\"y\":365},{\"x\":527,\"y\":466}],\"text\":\"13-Dec-2015\"},{\"boundingBox\":[{\"x\":552,\"y\":466},{\"x\":552,\"y\":364},{\"x\":572,\"y\":364},{\"x\":572,\"y\":467}],\"text\":\"13-Feb-2016\"},{\"boundingBox\":[{\"x\":596,\"y\":467},{\"x\":596,\"y\":364},{\"x\":617,\"y\":364},{\"x\":618,\"y\":466}],\"text\":\"13-Apr-2016\"},{\"boundingBox\":[{\"x\":599,\"y\":1},{\"x\":668,\"y\":1},{\"x\":668,\"y\":17},{\"x\":599,\"y\":17}],\"text\":\"-Tablet\"},{\"boundingBox\":[{\"x\":642,\"y\":462},{\"x\":642,\"y\":364},{\"x\":662,\"y\":364},{\"x\":662,\"y\":462}],\"text\":\"13-Jun-2016\"},{\"boundingBox\":[{\"x\":688,\"y\":463},{\"x\":687,\"y\":364},{\"x\":709,\"y\":364},{\"x\":710,\"y\":463}],\"text\":\"13-Aug-2016\"},{\"boundingBox\":[{\"x\":734,\"y\":464},{\"x\":734,\"y\":364},{\"x\":754,\"y\":364},{\"x\":755,\"y\":464}],\"text\":\"13-Oct-2016\"},{\"boundingBox\":[{\"x\":738,\"y\":1},{\"x\":812,\"y\":1},{\"x\":812,\"y\":18},{\"x\":738,\"y\":18}],\"text\":\"Laptop\"},{\"boundingBox\":[{\"x\":780,\"y\":463},{\"x\":780,\"y\":363},{\"x\":800,\"y\":363},{\"x\":800,\"y\":463}],\"text\":\"13-Dec-2016\"},{\"boundingBox\":[{\"x\":825,\"y\":467},{\"x\":825,\"y\":365},{\"x\":846,\"y\":365},{\"x\":846,\"y\":466}],\"text\":\"13-Feb-2017\"},{\"boundingBox\":[{\"x\":870,\"y\":467},{\"x\":869,\"y\":364},{\"x\":890,\"y\":364},{\"x\":891,\"y\":466}],\"text\":\"13-Apr-2017\"},{\"boundingBox\":[{\"x\":915,\"y\":463},{\"x\":915,\"y\":365},{\"x\":935,\"y\":365},{\"x\":936,\"y\":463}],\"text\":\"13-Jun-2017\"},{\"boundingBox\":[{\"x\":837,\"y\":0},{\"x\":1024,\"y\":1},{\"x\":1024,\"y\":19},{\"x\":837,\"y\":18}],\"text\":\"-Mobile Phone\"},{\"boundingBox\":[{\"x\":960,\"y\":464},{\"x\":960,\"y\":365},{\"x\":982,\"y\":364},{\"x\":982,\"y\":464}],\"text\":\"13-Aug-2017\"},{\"boundingBox\":[{\"x\":1007,\"y\":465},{\"x\":1006,\"y\":365},{\"x\":1027,\"y\":365},{\"x\":1027,\"y\":464}],\"text\":\"13-Oct-2017\"},{\"boundingBox\":[{\"x\":1050,\"y\":464},{\"x\":1050,\"y\":365},{\"x\":1072,\"y\":365},{\"x\":1072,\"y\":464}],\"text\":\"13-Dec-2017\"},{\"boundingBox\":[{\"x\":1099,\"y\":467},{\"x\":1098,\"y\":363},{\"x\":1119,\"y\":363},{\"x\":1120,\"y\":466}],\"text\":\"13-Feb-2018\"},{\"boundingBox\":[{\"x\":1143,\"y\":466},{\"x\":1143,\"y\":363},{\"x\":1164,\"y\":363},{\"x\":1164,\"y\":467}],\"text\":\"13-Apr-2018\"},{\"boundingBox\":[{\"x\":1189,\"y\":462},{\"x\":1189,\"y\":362},{\"x\":1209,\"y\":362},{\"x\":1210,\"y\":462}],\"text\":\"13-Jun-2018\"},{\"boundingBox\":[{\"x\":1235,\"y\":463},{\"x\":1234,\"y\":363},{\"x\":1256,\"y\":363},{\"x\":1257,\"y\":463}],\"text\":\"13-Aug-2018\"},{\"boundingBox\":[{\"x\":1279,\"y\":464},{\"x\":1278,\"y\":363},{\"x\":1299,\"y\":363},{\"x\":1299,\"y\":464}],\"text\":\"13-Oct-2018\"},{\"boundingBox\":[{\"x\":1324,\"y\":462},{\"x\":1324,\"y\":363},{\"x\":1345,\"y\":363},{\"x\":1346,\"y\":462}],\"text\":\"13-Dec-2018\"},{\"boundingBox\":[{\"x\":1372,\"y\":467},{\"x\":1372,\"y\":365},{\"x\":1392,\"y\":365},{\"x\":1392,\"y\":466}],\"text\":\"13-Feb-2019\"}],\"words\":[{\"boundingBox\":[{\"x\":0,\"y\":24},{\"x\":35,\"y\":23},{\"x\":36,\"y\":41},{\"x\":0,\"y\":42}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":11,\"y\":247},{\"x\":32,\"y\":246},{\"x\":32,\"y\":263},{\"x\":11,\"y\":264}],\"text\":\"30\"},{\"boundingBox\":[{\"x\":11,\"y\":183},{\"x\":33,\"y\":183},{\"x\":33,\"y\":199},{\"x\":11,\"y\":199}],\"text\":\"50\"},{\"boundingBox\":[{\"x\":11,\"y\":278},{\"x\":36,\"y\":278},{\"x\":36,\"y\":296},{\"x\":11,\"y\":296}],\"text\":\"20\"},{\"boundingBox\":[{\"x\":11,\"y\":88},{\"x\":34,\"y\":88},{\"x\":34,\"y\":105},{\"x\":11,\"y\":105}],\"text\":\"80\"},{\"boundingBox\":[{\"x\":11,\"y\":213},{\"x\":36,\"y\":213},{\"x\":36,\"y\":231},{\"x\":11,\"y\":230}],\"text\":\"40\"},{\"boundingBox\":[{\"x\":12,\"y\":309},{\"x\":37,\"y\":309},{\"x\":37,\"y\":326},{\"x\":12,\"y\":325}],\"text\":\"10\"},{\"boundingBox\":[{\"x\":10,\"y\":151},{\"x\":36,\"y\":151},{\"x\":36,\"y\":168},{\"x\":10,\"y\":168}],\"text\":\"60\"},{\"boundingBox\":[{\"x\":23,\"y\":356},{\"x\":22,\"y\":342},{\"x\":36,\"y\":341},{\"x\":37,\"y\":355}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":10,\"y\":56},{\"x\":37,\"y\":56},{\"x\":38,\"y\":74},{\"x\":10,\"y\":74}],\"text\":\"90\"},{\"boundingBox\":[{\"x\":13,\"y\":119},{\"x\":38,\"y\":119},{\"x\":38,\"y\":136},{\"x\":13,\"y\":136}],\"text\":\"70\"},{\"boundingBox\":[{\"x\":51,\"y\":464},{\"x\":50,\"y\":363},{\"x\":71,\"y\":363},{\"x\":70,\"y\":463}],\"text\":\"13-Apr-2014\"},{\"boundingBox\":[{\"x\":97,\"y\":462},{\"x\":96,\"y\":366},{\"x\":116,\"y\":367},{\"x\":116,\"y\":462}],\"text\":\"13-Jun-2014\"},{\"boundingBox\":[{\"x\":143,\"y\":463},{\"x\":141,\"y\":367},{\"x\":163,\"y\":367},{\"x\":163,\"y\":463}],\"text\":\"13-Aug-2014\"},{\"boundingBox\":[{\"x\":190,\"y\":465},{\"x\":187,\"y\":366},{\"x\":208,\"y\":367},{\"x\":206,\"y\":465}],\"text\":\"13-Oct-2014\"},{\"boundingBox\":[{\"x\":234,\"y\":464},{\"x\":234,\"y\":365},{\"x\":254,\"y\":365},{\"x\":254,\"y\":464}],\"text\":\"13-Dec-2014\"},{\"boundingBox\":[{\"x\":281,\"y\":467},{\"x\":279,\"y\":364},{\"x\":299,\"y\":365},{\"x\":298,\"y\":467}],\"text\":\"13-Feb-2015\"},{\"boundingBox\":[{\"x\":325,\"y\":467},{\"x\":323,\"y\":365},{\"x\":344,\"y\":365},{\"x\":342,\"y\":467}],\"text\":\"13-Apr-2015\"},{\"boundingBox\":[{\"x\":369,\"y\":464},{\"x\":368,\"y\":364},{\"x\":389,\"y\":364},{\"x\":388,\"y\":464}],\"text\":\"13-Jun-2015\"},{\"boundingBox\":[{\"x\":415,\"y\":464},{\"x\":414,\"y\":365},{\"x\":435,\"y\":365},{\"x\":436,\"y\":465}],\"text\":\"13-Aug-2015\"},{\"boundingBox\":[{\"x\":461,\"y\":465},{\"x\":460,\"y\":365},{\"x\":481,\"y\":365},{\"x\":480,\"y\":465}],\"text\":\"13-Oct-2015\"},{\"boundingBox\":[{\"x\":448,\"y\":2},{\"x\":528,\"y\":3},{\"x\":528,\"y\":20},{\"x\":448,\"y\":18}],\"text\":\"Desktop\"},{\"boundingBox\":[{\"x\":509,\"y\":467},{\"x\":506,\"y\":366},{\"x\":527,\"y\":366},{\"x\":526,\"y\":467}],\"text\":\"13-Dec-2015\"},{\"boundingBox\":[{\"x\":555,\"y\":467},{\"x\":552,\"y\":364},{\"x\":573,\"y\":366},{\"x\":570,\"y\":467}],\"text\":\"13-Feb-2016\"},{\"boundingBox\":[{\"x\":599,\"y\":467},{\"x\":596,\"y\":365},{\"x\":617,\"y\":365},{\"x\":615,\"y\":467}],\"text\":\"13-Apr-2016\"},{\"boundingBox\":[{\"x\":600,\"y\":1},{\"x\":668,\"y\":2},{\"x\":667,\"y\":18},{\"x\":600,\"y\":18}],\"text\":\"-Tablet\"},{\"boundingBox\":[{\"x\":643,\"y\":463},{\"x\":642,\"y\":365},{\"x\":663,\"y\":366},{\"x\":662,\"y\":463}],\"text\":\"13-Jun-2016\"},{\"boundingBox\":[{\"x\":690,\"y\":464},{\"x\":688,\"y\":364},{\"x\":710,\"y\":365},{\"x\":709,\"y\":463}],\"text\":\"13-Aug-2016\"},{\"boundingBox\":[{\"x\":736,\"y\":465},{\"x\":734,\"y\":365},{\"x\":755,\"y\":365},{\"x\":754,\"y\":465}],\"text\":\"13-Oct-2016\"},{\"boundingBox\":[{\"x\":743,\"y\":2},{\"x\":812,\"y\":2},{\"x\":811,\"y\":19},{\"x\":743,\"y\":18}],\"text\":\"Laptop\"},{\"boundingBox\":[{\"x\":781,\"y\":464},{\"x\":780,\"y\":364},{\"x\":801,\"y\":365},{\"x\":800,\"y\":464}],\"text\":\"13-Dec-2016\"},{\"boundingBox\":[{\"x\":828,\"y\":467},{\"x\":825,\"y\":365},{\"x\":846,\"y\":366},{\"x\":844,\"y\":467}],\"text\":\"13-Feb-2017\"},{\"boundingBox\":[{\"x\":872,\"y\":467},{\"x\":870,\"y\":365},{\"x\":891,\"y\":366},{\"x\":889,\"y\":467}],\"text\":\"13-Apr-2017\"},{\"boundingBox\":[{\"x\":916,\"y\":463},{\"x\":916,\"y\":366},{\"x\":936,\"y\":366},{\"x\":936,\"y\":464}],\"text\":\"13-Jun-2017\"},{\"boundingBox\":[{\"x\":838,\"y\":1},{\"x\":957,\"y\":2},{\"x\":957,\"y\":18},{\"x\":839,\"y\":19}],\"text\":\"-Mobile\"},{\"boundingBox\":[{\"x\":960,\"y\":2},{\"x\":1024,\"y\":2},{\"x\":1024,\"y\":19},{\"x\":960,\"y\":18}],\"text\":\"Phone\"},{\"boundingBox\":[{\"x\":962,\"y\":464},{\"x\":960,\"y\":365},{\"x\":982,\"y\":365},{\"x\":982,\"y\":464}],\"text\":\"13-Aug-2017\"},{\"boundingBox\":[{\"x\":1008,\"y\":465},{\"x\":1007,\"y\":365},{\"x\":1028,\"y\":366},{\"x\":1027,\"y\":465}],\"text\":\"13-Oct-2017\"},{\"boundingBox\":[{\"x\":1052,\"y\":464},{\"x\":1051,\"y\":366},{\"x\":1073,\"y\":367},{\"x\":1071,\"y\":464}],\"text\":\"13-Dec-2017\"},{\"boundingBox\":[{\"x\":1101,\"y\":467},{\"x\":1099,\"y\":364},{\"x\":1120,\"y\":366},{\"x\":1118,\"y\":467}],\"text\":\"13-Feb-2018\"},{\"boundingBox\":[{\"x\":1146,\"y\":467},{\"x\":1144,\"y\":364},{\"x\":1165,\"y\":365},{\"x\":1161,\"y\":467}],\"text\":\"13-Apr-2018\"},{\"boundingBox\":[{\"x\":1191,\"y\":463},{\"x\":1189,\"y\":363},{\"x\":1210,\"y\":364},{\"x\":1209,\"y\":463}],\"text\":\"13-Jun-2018\"},{\"boundingBox\":[{\"x\":1236,\"y\":463},{\"x\":1235,\"y\":364},{\"x\":1257,\"y\":364},{\"x\":1256,\"y\":463}],\"text\":\"13-Aug-2018\"},{\"boundingBox\":[{\"x\":1280,\"y\":464},{\"x\":1279,\"y\":363},{\"x\":1300,\"y\":364},{\"x\":1299,\"y\":464}],\"text\":\"13-Oct-2018\"},{\"boundingBox\":[{\"x\":1326,\"y\":463},{\"x\":1325,\"y\":364},{\"x\":1346,\"y\":365},{\"x\":1346,\"y\":463}],\"text\":\"13-Dec-2018\"},{\"boundingBox\":[{\"x\":1374,\"y\":467},{\"x\":1373,\"y\":366},{\"x\":1393,\"y\":367},{\"x\":1391,\"y\":467}],\"text\":\"13-Feb-2019\"}]}", "{\"language\":\"en\",\"text\":\"Real World Applications and Services Application 1 Application 2 Application N Dynamic Updating and Management Recency Analysis and Layer 4 Mining Rule Updation Rule Discovery Contextual Rule-based Rule Layer 3 Preferences Learning Generalization Context Discretization Time Series Layer 2 Contextual Modeling Data Clustering Contextual Data Acquisition Layer 1 Smartphone External Sensors Logs Sources\",\"lines\":[{\"boundingBox\":[{\"x\":244,\"y\":23},{\"x\":859,\"y\":23},{\"x\":859,\"y\":53},{\"x\":244,\"y\":53}],\"text\":\"Real World Applications and Services\"},{\"boundingBox\":[{\"x\":92,\"y\":102},{\"x\":303,\"y\":101},{\"x\":303,\"y\":131},{\"x\":92,\"y\":133}],\"text\":\"Application 1\"},{\"boundingBox\":[{\"x\":433,\"y\":103},{\"x\":649,\"y\":102},{\"x\":649,\"y\":132},{\"x\":433,\"y\":133}],\"text\":\"Application 2\"},{\"boundingBox\":[{\"x\":764,\"y\":103},{\"x\":982,\"y\":101},{\"x\":982,\"y\":131},{\"x\":764,\"y\":133}],\"text\":\"Application N\"},{\"boundingBox\":[{\"x\":153,\"y\":234},{\"x\":754,\"y\":234},{\"x\":754,\"y\":267},{\"x\":153,\"y\":267}],\"text\":\"Dynamic Updating and Management\"},{\"boundingBox\":[{\"x\":47,\"y\":303},{\"x\":412,\"y\":303},{\"x\":412,\"y\":331},{\"x\":47,\"y\":333}],\"text\":\"Recency Analysis and\"},{\"boundingBox\":[{\"x\":902,\"y\":287},{\"x\":1030,\"y\":286},{\"x\":1031,\"y\":313},{\"x\":902,\"y\":315}],\"text\":\"Layer 4\"},{\"boundingBox\":[{\"x\":171,\"y\":335},{\"x\":284,\"y\":337},{\"x\":284,\"y\":364},{\"x\":171,\"y\":361}],\"text\":\"Mining\"},{\"boundingBox\":[{\"x\":521,\"y\":318},{\"x\":758,\"y\":320},{\"x\":758,\"y\":349},{\"x\":521,\"y\":348}],\"text\":\"Rule Updation\"},{\"boundingBox\":[{\"x\":338,\"y\":455},{\"x\":586,\"y\":457},{\"x\":586,\"y\":486},{\"x\":338,\"y\":483}],\"text\":\"Rule Discovery\"},{\"boundingBox\":[{\"x\":63,\"y\":521},{\"x\":243,\"y\":521},{\"x\":243,\"y\":548},{\"x\":63,\"y\":548}],\"text\":\"Contextual\"},{\"boundingBox\":[{\"x\":319,\"y\":522},{\"x\":508,\"y\":522},{\"x\":508,\"y\":548},{\"x\":319,\"y\":548}],\"text\":\"Rule-based\"},{\"boundingBox\":[{\"x\":659,\"y\":521},{\"x\":737,\"y\":523},{\"x\":737,\"y\":548},{\"x\":659,\"y\":546}],\"text\":\"Rule\"},{\"boundingBox\":[{\"x\":901,\"y\":509},{\"x\":1030,\"y\":507},{\"x\":1030,\"y\":536},{\"x\":901,\"y\":538}],\"text\":\"Layer 3\"},{\"boundingBox\":[{\"x\":54,\"y\":554},{\"x\":254,\"y\":554},{\"x\":254,\"y\":581},{\"x\":54,\"y\":580}],\"text\":\"Preferences\"},{\"boundingBox\":[{\"x\":340,\"y\":553},{\"x\":486,\"y\":555},{\"x\":485,\"y\":584},{\"x\":340,\"y\":581}],\"text\":\"Learning\"},{\"boundingBox\":[{\"x\":578,\"y\":554},{\"x\":816,\"y\":554},{\"x\":817,\"y\":580},{\"x\":578,\"y\":581}],\"text\":\"Generalization\"},{\"boundingBox\":[{\"x\":246,\"y\":666},{\"x\":606,\"y\":666},{\"x\":606,\"y\":693},{\"x\":246,\"y\":693}],\"text\":\"Context Discretization\"},{\"boundingBox\":[{\"x\":128,\"y\":732},{\"x\":326,\"y\":732},{\"x\":326,\"y\":759},{\"x\":128,\"y\":760}],\"text\":\"Time Series\"},{\"boundingBox\":[{\"x\":901,\"y\":714},{\"x\":1029,\"y\":714},{\"x\":1030,\"y\":742},{\"x\":901,\"y\":743}],\"text\":\"Layer 2\"},{\"boundingBox\":[{\"x\":551,\"y\":734},{\"x\":733,\"y\":734},{\"x\":733,\"y\":760},{\"x\":551,\"y\":760}],\"text\":\"Contextual\"},{\"boundingBox\":[{\"x\":152,\"y\":765},{\"x\":305,\"y\":766},{\"x\":304,\"y\":794},{\"x\":152,\"y\":791}],\"text\":\"Modeling\"},{\"boundingBox\":[{\"x\":514,\"y\":767},{\"x\":769,\"y\":768},{\"x\":769,\"y\":796},{\"x\":514,\"y\":794}],\"text\":\"Data Clustering\"},{\"boundingBox\":[{\"x\":202,\"y\":886},{\"x\":656,\"y\":886},{\"x\":656,\"y\":916},{\"x\":202,\"y\":916}],\"text\":\"Contextual Data Acquisition\"},{\"boundingBox\":[{\"x\":899,\"y\":935},{\"x\":1027,\"y\":934},{\"x\":1027,\"y\":963},{\"x\":899,\"y\":964}],\"text\":\"Layer 1\"},{\"boundingBox\":[{\"x\":75,\"y\":961},{\"x\":279,\"y\":962},{\"x\":279,\"y\":988},{\"x\":75,\"y\":987}],\"text\":\"Smartphone\"},{\"boundingBox\":[{\"x\":632,\"y\":956},{\"x\":771,\"y\":956},{\"x\":771,\"y\":981},{\"x\":632,\"y\":981}],\"text\":\"External\"},{\"boundingBox\":[{\"x\":379,\"y\":975},{\"x\":518,\"y\":976},{\"x\":518,\"y\":1001},{\"x\":379,\"y\":1000}],\"text\":\"Sensors\"},{\"boundingBox\":[{\"x\":137,\"y\":993},{\"x\":217,\"y\":994},{\"x\":217,\"y\":1021},{\"x\":136,\"y\":1020}],\"text\":\"Logs\"},{\"boundingBox\":[{\"x\":634,\"y\":987},{\"x\":772,\"y\":989},{\"x\":772,\"y\":1013},{\"x\":634,\"y\":1011}],\"text\":\"Sources\"}],\"words\":[{\"boundingBox\":[{\"x\":245,\"y\":24},{\"x\":325,\"y\":24},{\"x\":324,\"y\":53},{\"x\":245,\"y\":53}],\"text\":\"Real\"},{\"boundingBox\":[{\"x\":330,\"y\":24},{\"x\":429,\"y\":24},{\"x\":428,\"y\":54},{\"x\":330,\"y\":54}],\"text\":\"World\"},{\"boundingBox\":[{\"x\":434,\"y\":24},{\"x\":637,\"y\":24},{\"x\":636,\"y\":54},{\"x\":434,\"y\":54}],\"text\":\"Applications\"},{\"boundingBox\":[{\"x\":643,\"y\":24},{\"x\":711,\"y\":24},{\"x\":710,\"y\":53},{\"x\":642,\"y\":54}],\"text\":\"and\"},{\"boundingBox\":[{\"x\":717,\"y\":24},{\"x\":858,\"y\":25},{\"x\":857,\"y\":51},{\"x\":716,\"y\":53}],\"text\":\"Services\"},{\"boundingBox\":[{\"x\":92,\"y\":103},{\"x\":277,\"y\":101},{\"x\":279,\"y\":131},{\"x\":92,\"y\":133}],\"text\":\"Application\"},{\"boundingBox\":[{\"x\":285,\"y\":101},{\"x\":301,\"y\":101},{\"x\":303,\"y\":130},{\"x\":287,\"y\":131}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":435,\"y\":104},{\"x\":617,\"y\":103},{\"x\":617,\"y\":133},{\"x\":434,\"y\":134}],\"text\":\"Application\"},{\"boundingBox\":[{\"x\":623,\"y\":103},{\"x\":648,\"y\":102},{\"x\":648,\"y\":133},{\"x\":623,\"y\":133}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":765,\"y\":104},{\"x\":946,\"y\":102},{\"x\":946,\"y\":132},{\"x\":764,\"y\":133}],\"text\":\"Application\"},{\"boundingBox\":[{\"x\":952,\"y\":102},{\"x\":981,\"y\":102},{\"x\":981,\"y\":131},{\"x\":952,\"y\":132}],\"text\":\"N\"},{\"boundingBox\":[{\"x\":153,\"y\":237},{\"x\":296,\"y\":235},{\"x\":297,\"y\":268},{\"x\":154,\"y\":268}],\"text\":\"Dynamic\"},{\"boundingBox\":[{\"x\":302,\"y\":235},{\"x\":454,\"y\":235},{\"x\":454,\"y\":268},{\"x\":303,\"y\":268}],\"text\":\"Updating\"},{\"boundingBox\":[{\"x\":460,\"y\":235},{\"x\":526,\"y\":235},{\"x\":526,\"y\":268},{\"x\":460,\"y\":268}],\"text\":\"and\"},{\"boundingBox\":[{\"x\":532,\"y\":235},{\"x\":754,\"y\":238},{\"x\":754,\"y\":267},{\"x\":532,\"y\":268}],\"text\":\"Management\"},{\"boundingBox\":[{\"x\":48,\"y\":305},{\"x\":195,\"y\":303},{\"x\":194,\"y\":333},{\"x\":47,\"y\":332}],\"text\":\"Recency\"},{\"boundingBox\":[{\"x\":200,\"y\":303},{\"x\":341,\"y\":303},{\"x\":341,\"y\":332},{\"x\":200,\"y\":333}],\"text\":\"Analysis\"},{\"boundingBox\":[{\"x\":347,\"y\":303},{\"x\":411,\"y\":304},{\"x\":411,\"y\":331},{\"x\":347,\"y\":332}],\"text\":\"and\"},{\"boundingBox\":[{\"x\":902,\"y\":288},{\"x\":998,\"y\":287},{\"x\":998,\"y\":314},{\"x\":903,\"y\":316}],\"text\":\"Layer\"},{\"boundingBox\":[{\"x\":1003,\"y\":287},{\"x\":1030,\"y\":287},{\"x\":1030,\"y\":314},{\"x\":1003,\"y\":314}],\"text\":\"4\"},{\"boundingBox\":[{\"x\":171,\"y\":336},{\"x\":285,\"y\":338},{\"x\":284,\"y\":365},{\"x\":172,\"y\":361}],\"text\":\"Mining\"},{\"boundingBox\":[{\"x\":522,\"y\":319},{\"x\":599,\"y\":319},{\"x\":599,\"y\":349},{\"x\":522,\"y\":349}],\"text\":\"Rule\"},{\"boundingBox\":[{\"x\":605,\"y\":320},{\"x\":759,\"y\":321},{\"x\":758,\"y\":350},{\"x\":605,\"y\":349}],\"text\":\"Updation\"},{\"boundingBox\":[{\"x\":339,\"y\":456},{\"x\":417,\"y\":456},{\"x\":417,\"y\":482},{\"x\":339,\"y\":482}],\"text\":\"Rule\"},{\"boundingBox\":[{\"x\":422,\"y\":456},{\"x\":587,\"y\":457},{\"x\":586,\"y\":487},{\"x\":422,\"y\":482}],\"text\":\"Discovery\"},{\"boundingBox\":[{\"x\":64,\"y\":522},{\"x\":243,\"y\":522},{\"x\":243,\"y\":549},{\"x\":64,\"y\":548}],\"text\":\"Contextual\"},{\"boundingBox\":[{\"x\":319,\"y\":522},{\"x\":508,\"y\":522},{\"x\":508,\"y\":549},{\"x\":320,\"y\":548}],\"text\":\"Rule-based\"},{\"boundingBox\":[{\"x\":659,\"y\":522},{\"x\":737,\"y\":523},{\"x\":737,\"y\":548},{\"x\":659,\"y\":546}],\"text\":\"Rule\"},{\"boundingBox\":[{\"x\":901,\"y\":510},{\"x\":1000,\"y\":508},{\"x\":1000,\"y\":537},{\"x\":901,\"y\":538}],\"text\":\"Layer\"},{\"boundingBox\":[{\"x\":1006,\"y\":508},{\"x\":1030,\"y\":508},{\"x\":1030,\"y\":537},{\"x\":1006,\"y\":537}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":54,\"y\":554},{\"x\":253,\"y\":555},{\"x\":253,\"y\":581},{\"x\":54,\"y\":580}],\"text\":\"Preferences\"},{\"boundingBox\":[{\"x\":341,\"y\":554},{\"x\":485,\"y\":555},{\"x\":485,\"y\":584},{\"x\":341,\"y\":582}],\"text\":\"Learning\"},{\"boundingBox\":[{\"x\":579,\"y\":554},{\"x\":816,\"y\":554},{\"x\":816,\"y\":581},{\"x\":579,\"y\":581}],\"text\":\"Generalization\"},{\"boundingBox\":[{\"x\":247,\"y\":667},{\"x\":377,\"y\":666},{\"x\":377,\"y\":694},{\"x\":247,\"y\":694}],\"text\":\"Context\"},{\"boundingBox\":[{\"x\":382,\"y\":666},{\"x\":606,\"y\":666},{\"x\":606,\"y\":694},{\"x\":382,\"y\":694}],\"text\":\"Discretization\"},{\"boundingBox\":[{\"x\":130,\"y\":733},{\"x\":214,\"y\":732},{\"x\":214,\"y\":760},{\"x\":130,\"y\":760}],\"text\":\"Time\"},{\"boundingBox\":[{\"x\":220,\"y\":732},{\"x\":325,\"y\":733},{\"x\":325,\"y\":759},{\"x\":219,\"y\":760}],\"text\":\"Series\"},{\"boundingBox\":[{\"x\":901,\"y\":715},{\"x\":999,\"y\":715},{\"x\":999,\"y\":743},{\"x\":902,\"y\":744}],\"text\":\"Layer\"},{\"boundingBox\":[{\"x\":1005,\"y\":715},{\"x\":1029,\"y\":714},{\"x\":1030,\"y\":743},{\"x\":1005,\"y\":743}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":552,\"y\":734},{\"x\":733,\"y\":735},{\"x\":732,\"y\":760},{\"x\":552,\"y\":761}],\"text\":\"Contextual\"},{\"boundingBox\":[{\"x\":152,\"y\":765},{\"x\":304,\"y\":766},{\"x\":304,\"y\":795},{\"x\":152,\"y\":791}],\"text\":\"Modeling\"},{\"boundingBox\":[{\"x\":515,\"y\":767},{\"x\":597,\"y\":767},{\"x\":596,\"y\":793},{\"x\":515,\"y\":793}],\"text\":\"Data\"},{\"boundingBox\":[{\"x\":602,\"y\":767},{\"x\":769,\"y\":768},{\"x\":769,\"y\":797},{\"x\":602,\"y\":793}],\"text\":\"Clustering\"},{\"boundingBox\":[{\"x\":203,\"y\":888},{\"x\":383,\"y\":887},{\"x\":382,\"y\":916},{\"x\":203,\"y\":916}],\"text\":\"Contextual\"},{\"boundingBox\":[{\"x\":388,\"y\":887},{\"x\":469,\"y\":887},{\"x\":469,\"y\":917},{\"x\":388,\"y\":916}],\"text\":\"Data\"},{\"boundingBox\":[{\"x\":475,\"y\":887},{\"x\":656,\"y\":888},{\"x\":655,\"y\":917},{\"x\":474,\"y\":917}],\"text\":\"Acquisition\"},{\"boundingBox\":[{\"x\":899,\"y\":935},{\"x\":1003,\"y\":934},{\"x\":1002,\"y\":964},{\"x\":900,\"y\":965}],\"text\":\"Layer\"},{\"boundingBox\":[{\"x\":1009,\"y\":934},{\"x\":1028,\"y\":934},{\"x\":1027,\"y\":964},{\"x\":1008,\"y\":964}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":76,\"y\":961},{\"x\":279,\"y\":962},{\"x\":279,\"y\":989},{\"x\":76,\"y\":988}],\"text\":\"Smartphone\"},{\"boundingBox\":[{\"x\":633,\"y\":956},{\"x\":770,\"y\":956},{\"x\":770,\"y\":982},{\"x\":633,\"y\":982}],\"text\":\"External\"},{\"boundingBox\":[{\"x\":380,\"y\":976},{\"x\":518,\"y\":978},{\"x\":517,\"y\":1001},{\"x\":380,\"y\":1001}],\"text\":\"Sensors\"},{\"boundingBox\":[{\"x\":137,\"y\":993},{\"x\":217,\"y\":994},{\"x\":217,\"y\":1021},{\"x\":137,\"y\":1019}],\"text\":\"Logs\"},{\"boundingBox\":[{\"x\":634,\"y\":987},{\"x\":773,\"y\":990},{\"x\":772,\"y\":1013},{\"x\":634,\"y\":1012}],\"text\":\"Sources\"}]}", "{\"language\":\"en\",\"text\":\"Published online: 31 October 2019\",\"lines\":[{\"boundingBox\":[{\"x\":0,\"y\":16},{\"x\":992,\"y\":16},{\"x\":992,\"y\":69},{\"x\":0,\"y\":69}],\"text\":\"Published online: 31 October 2019\"}],\"words\":[{\"boundingBox\":[{\"x\":1,\"y\":18},{\"x\":282,\"y\":17},{\"x\":281,\"y\":70},{\"x\":0,\"y\":69}],\"text\":\"Published\"},{\"boundingBox\":[{\"x\":292,\"y\":17},{\"x\":499,\"y\":17},{\"x\":498,\"y\":70},{\"x\":292,\"y\":70}],\"text\":\"online:\"},{\"boundingBox\":[{\"x\":509,\"y\":17},{\"x\":587,\"y\":17},{\"x\":587,\"y\":70},{\"x\":509,\"y\":70}],\"text\":\"31\"},{\"boundingBox\":[{\"x\":597,\"y\":17},{\"x\":838,\"y\":17},{\"x\":838,\"y\":70},{\"x\":597,\"y\":70}],\"text\":\"October\"},{\"boundingBox\":[{\"x\":848,\"y\":17},{\"x\":990,\"y\":18},{\"x\":990,\"y\":69},{\"x\":848,\"y\":70}],\"text\":\"2019\"}]}" ] }, { "@search.score": 5.981037, "content": "\nMobile marketing recommendation method \nbased on user location feedback\nChunyong Yin1 , Shilei Ding1 and Jin Wang2*\n\nIntroduction\nIn recent years, the e-commerce industry has developed rapidly with the popularization \nof the Internet. At this time, famous e-commerce platforms such as Alibaba and Ama-\nzon were born. E-commerce moved physical store products to a virtual network plat-\nform. On the one hand, it is convenient for users to buy various products without leaving \nthe home. On the other hand, it is also convenient for sellers to sell their own goods \nand reduce costs. However, the various products have made it more difficult for users \nto select products. E-commerce platform can generate a large amount of user location \nfeedback data which contains a wealth of user preference information [1]. It is significant \nto predict the location of the next consumer’s consumption from these behavioral data. \nAt present, most of the recommended methods focus on the user-product binary matrix \nand directly model their binary relationships [2]. The users’ location information and \nshopping location information are considered as the third factor. In this case, you can \nonly use the limited check-in data. The users’ location feedback behavior and the timeli-\nness of behavior are often overlooked.\n\nThe mobile recommendation system takes advantage of the mobile network environ-\nment in terms of information recommendation and overcomes the disadvantages. Filter-\ning irrelevant information by predicting potential mobile user preferences and providing \n\nAbstract \n\nLocation-based mobile marketing recommendation has become one of the hot spots \nin e-commerce. The current mobile marketing recommendation system only treats \nlocation information as a recommended attribute, which weakens the role of users and \nshopping location information in the recommendation. This paper focuses on location \nfeedback data of user and proposes a location-based mobile marketing recommenda-\ntion model by convolutional neural network (LBCNN). First, the users’ location-based \nbehaviors are divided into different time windows. For each window, the extractor \nachieves users’ timing preference characteristics from different dimensions. Next, we \nuse the convolutional model in the convolutional neural network model to train a \nclassifier. The experimental results show that the model proposed in this paper is better \nthan the traditional recommendation models in the terms of accuracy rate and recall \nrate, both of which increase nearly 10%.\n\nKeywords: Location feedback, Mobile marketing, Convolutional neural network, \nSequential behavior\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nRESEARCH\n\nYin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \nhttps://doi.org/10.1186/s13673-019-0177-6\n\n*Correspondence: \njinwang@csust.edu.cn \n2 School of Computer & \nCommunication Engineering, \nChangsha University \nof Science & Technology, \nChangsha 410004, China\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0001-5764-2432\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13673-019-0177-6&domain=pdf\n\n\nPage 2 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\nmobile users with results that meet users’ individual needs gradually become an effec-\ntive means to alleviate “mobile information overload” [3]. Mobile users have different \npreferences in different geographical locations. For this problem, how to use location \ninformation to obtain mobile users’ preferences and provide accurate personalized \nrecommendations has become a hot topic in mobile recommendation research [4]. \nAlthough there are many researches based on location recommendation, they mainly \nfocus on service resources without positional relevance. To solve the shortcomings of \nresearch on location relevance of service resources is few [5], Zhu et  al. [6] proposed \nthe method which is based on the user’s context information to analyze the user’s pref-\nerences and retrograde. Their approach is to derive user preferences by proposing two \ndifferent assumptions and then recommending user models based on preference analy-\nsis. Yin et al. [7] proposed LA-LDA. The method is a location-aware based generation \nprobability model, which uses scoring based on location to model user information and \nrecommend to users. However, these methods only treat location information as an \nattribute without considering the spatial information of users or items and weaken loca-\ntion information’s role in the recommendation. There are some studies determine user \npreferences by the distance between the mobile user and the merchant [8], but only set \nthe area based on the proximity of the distance and ignore the spatial activities of the \nmobile user [9]. However, these methods were limited to the analysis of user informa-\ntion and product information, and did not carefully consider the importance of user and \nbusiness location information. Therefore, the user preference model based on location \nrecommendation they created has some gap.\n\nConsidering the core of mobile marketing recommendation is location movement, \nLian et al. [10] proposed an implied feature-based cognitive feature collaborative filter-\ning (ICCF) framework, which avoids the impact of negative samples by combining con-\nventional methods and semantic content. In terms of algorithms, the author proposed \nan improved algorithm that can expand according to data size and feature size. To deter-\nmine the relevance of the project to user needs, Lee et al. [11] developed context infor-\nmation analysis and collaborative filtering methods for multimedia recommendations in \nmobile environments. Nevertheless, these methods only used small-scale training data \nand could not achieve accurate prediction of long-term interest for users. In this paper, \ndeep learning and time stamps are used to compensate for these shortcomings.\n\nWith great achievements in visual and speech tasks, the Deep Learning (DL) model \nhas become a novel field of study [12]. Because of the interventional optimization of \ndeep learning algorithms, artificial intelligence has made great breakthroughs in many \naspects. It is well known that models obtained through deep learning and machine learn-\ning models have very similar effects, which learns advanced abstract features from the \noriginal input features by simulating the network structure of the human nervous sys-\ntem. Experiments show that the deep model can express the characteristics of the data \nbetter than the shallow model [13]. Weight sharing by convolution makes CNN similar \nto biological neural networks, which reduces the difficulty of network structure and the \nnumber of weights. The structure of CNN is roughly divided into two layers. It is well \nknown that the first layer is a convolutional layer. Each neuron’s input is connected to the \nprevious layer through a convolution kernel and the local features are extracted. Next \nlayer is a pooling layer. In this layer, the neurons in the network are connected through \n\n\n\nPage 3 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\na convolution kernel to extract the overall features. Convolutional neural networks have \ngreat advantages in processing two-dimensional features [14], such as images.\n\nBased on our detailed comparative analysis, this paper proposes a location-based \nmobile marketing recommendation model by convolutional neural network (LBCNN). \nFirstly, we use user-product information as a training sample, and treat this problem as \na two-class problem. The category of the problem is divided into the purchase behav-\nior and the purchase behavior of the product at the next moment. In order to capture \nthe user’s timing preference characteristics, we divide the behavior of the merchandise \naccording to a certain length of time window and dig deeper into the behavior charac-\nteristics of each time window. Secondly, we consider the users’ timing preferences and \noverall preferences for the product. Then, the features of time window are used to train \nconvolutional neural network models. Finally, we input the sample features of the test \nset into the model and generate the Top-K sample as the location-based purchase fore-\ncast results [15].\n\nRemain of the paper is divided into four sections. Related work is shown in “Related \nwork” section. Necessary definitions and specific implementation of the location-based \nmobile marketing recommendation model by convolutional neural network (LBCNN) \nare shown in “Location-based mobile marketing recommendation model by CNN” sec-\ntion. In “Experimental analysis” section, experimental analysis is introduced. “Conclu-\nsion” section summarizes the strengths and weaknesses of the paper and proposes plans \nfor future progress.\n\nRelated work\nIn the current chapter, we will review existing methods for recommending systems \nthat can be broadly divided into three parts: content filtering, collaborative filtering \nand hybrid methods. We also discuss the establishment of feature models based on \ntime series to clearly represent the differences between our research and other existing \nmethods.\n\nTraditional recommendation method\n\nIn the general products recommendation system, the similarity between users is calcu-\nlated by the user’s interest feature vector. Then, the system recommends some products \nwith similarity greater than a certain threshold or the similar Top-N products to the tar-\nget user. This is a traditional recommendation algorithm based on content and the rec-\nommendation is based on comparing users.\n\na. Content‑based recommendation method\n\nContent-based information filtering has proven to be an effective application for \nlocating text documents related to topics. In particular, we need to focus on the \napplication of content-based information filtering in the recommendation system. \nContent-based methods allow for accurate comparisons between different texts \nor projects, so the recommended results are similar to the historical content of the \nuser’s consumption. The content-based recommendation algorithm involves the fol-\nlowing aspects. User description file describes the user’s preferences, which can be \nfilled by the user and dynamically updated based on the user’s feedback information \n\n\n\nPage 4 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\n(purchasing, reading, clicking, etc.) during the operation of the system. The project \nprofile describes the content characteristics of each project, which constitutes the \nfeature vector of the project. In addition, the similarity calculation is the similarity \nbetween the user’s description file and the item feature vector.\n\nThe similarity calculation of the content-based recommendation algorithm usually \nadopts the cosine similarity algorithm. The algorithm needs to calculate the similarity \nbetween the feature vector of user u and the feature vector of item i. The calculation \nformula is as shown in Formula (1).\n\nwhere ⇀u denotes the user feature vector, \n⇀\n\ni denotes the project feature vector, \n⇀\n\n|u| is the \nmodulus of the user feature vector and \n\n⇀\n\n|i| is the model of the project feature vector.\nRepresentative content-based recommendation systems mainly include Lops, \n\nGemmis, and Semeraro [16]. Compared to other methods, content-based recom-\nmendations have no cold-start issues and recommendations are easy to understand. \nHowever, the content filtering based recommendation method has various draw-\nbacks, such as strongly relying on the availability of content and ignoring the context \ninformation of the recommended party. The content-based recommendation method \nalso has certain requirements for the format of the project. Besides, it is difficult to \ndistinguish the merits of the project. The same type of project may have the same type \nof features, which are difficult to reflect the quality of the project.\n\nb. Collaborative filtering method\n\nThe recommendation based on collaborative filtering solves the recommendation \nproblem by using the information of similar users in the same partition to analyze and \nrecommend new content that has not been scored or seen by the target user.\n\nRegarding the traditional collaborative filtering method based on memory, we \nunderstand that this method is based on the different relationships between users and \nprojects. According to expert research, the traditional collaborative filtering method \nbased on memory should be divided into the following three steps.\n\nStep 1: collection of user behavior data, this step represents the user’s past behav-\nior with a m * n matrix R. The matrix Umn represents the feedback that the user m \nhas on the recommended object n. Rating is a range of values and different values \nrepresent how much the user likes the recommended object.\n\nStep 2: establishment of a user neighbor: establish mutual user relationships by \nanalyzing all user historical behavior data.\n\n(1)sim(u, i) =\n\n⇀\nu ·\n\n⇀\n\ni\n\n⇀\n\n|u|\n⇀\n\n|i|\n\nU =\n\n\n\n\n\n\n\nU11 U12 . . . U1n\n\nU21 U22 . . . U2n\n\n. . . . . . . . . . . .\n\nUm1 Um2 . . . Umn\n\n\n\n\n\n\n.\n\n\n\nPage 5 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\nStep 3: generate recommendation results: find the most likely N objects from the rec-\nommended items selected by similar user sets.\n\nTherefore, recommendations are made by mining common features in similar users’ pref-\nerence information [17]. The normal methods in this classification include k-nearest neigh-\nbor (k-NN), matrix decomposition, and semi-supervised learning. According to the survey, \nAmazon uses an item-by-item collaborative filtering method to recommend personalized \nonline stores for each customer.\n\nCompared to other method, collaborative filtering has the ability to filter out informa-\ntion that can be automatically recognized by the machine and effectively use feedback from \nother similar users. However, collaborative filtering requires more ratings for the project, \nso it is affected by the issue of rating sparsity. In addition, this method does not provide a \nstandard recommendation for new users and new projects, which is called a cold start issue.\n\nc. Hybrid recommendation method\n\nThe hybrid recommendation method combines the above techniques in different ways to \nimprove the recommended performance and optimize the shortcomings of the conven-\ntional method. Projects that cannot be recommended for collaborative filtering are gener-\nally addressed by combining them with content-based filtering [18].\n\nThe core of this method is to independently calculate the recommendation results of the \ntwo types of recommendation algorithms, and then mix the results. There are two specific \nhybrid methods. One method is to mix the predicted scores of the two algorithms linearly. \nAnother hybrid method is to set up an evaluation standard, compare the recommended \nresults of the two algorithms, and take the recommendation results of the higher evaluation \nalgorithms. In general, the hybrid recommendation achieves a certain degree of compensa-\ntion between different recommendation algorithms. However, the hybrid recommendation \nalgorithm still needs improvement in complexity.\n\nd. Recommendation based on association rules\n\nThe association rule algorithm is a traditional data mining method that has been widely \nused in business for many years. The core idea is to analyze the rules of user historical \nbehavior data to recommend more similar behavioral items [19]. Rules can be either user-\ndefined or dynamically generated by using rule algorithms. The effect of the algorithm \ndepends mainly on the quantity and quality of the rules so the focus of the algorithm is on \nhow to develop high quality rules.\n\nDefine N as the total number of transactions, R is the total project and U and V are two \ndisjoint sets of items (U∩V ≠ ∅, U∈R, V∈R). The association rule is essentially an IF–Then \nstatement, here is expressed by U → V. The strength of the association rule U → V can be \nmeasured by two criteria: support and confidence. S is the ratio containing U and V data \nwhich both represent the number of transactions, which is shown in Formula (2).\n\nC is the ratio of U, V data to the only U data which represents the number of transac-\ntions, as shown in Formula (3)\n\n(2)S(U → V ) =\nN (U ∪ V )\n\nN\n.\n\n\n\nPage 6 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\nThe recommendation process of the algorithm is shown in below.\nFirstly, according to the items of interest to the user, the user’s interest in other \n\nunknown items is predicted by rules. Secondly, compare the support of the rules. Finally, \nthe recommended items of TOP-N are obtained to the user.\n\nThe recommendation system based on association rules includes three parts: the key-\nword, the presentation and the user interface. The keyword layer is a set of keyword \nattributes and dependencies between keywords. The description layer connects the \nkeyword layer and the user layer and the main function is to describe the user and the \nresource. The user interface layer is the layer that interacts directly with the user. How-\never, the system becomes more and more difficult to manage as the rules increasing. In \naddition, there is a strong dependence on the quality of the rules and a cold start prob-\nlem is existed.\n\nMost of the recommendation systems use collaborative filtering algorithm to recom-\nmend for users. However, the traditional algorithm can only analyze ready-made data \nsimply, and most systems simply preprocess the data. In our method, we preprocess the \ndataset by extending the time information of the data to a time label. The next section is \nan explanation of the specific implementation.\n\nConstruction of time series behavior’s preference features\n\nThe timing recommendation model is based primarily on the Markov chain. This model \nmakes full use of timing behavior data to predict the next purchase behavior based on \nthe user’s last behavior. The advantage of this model is that it can generate good recom-\nmendations by timing behavior.\n\nAs shown in Fig. 1, the prediction problem of product purchase can be expressed as \npredicts the user’s purchase behavior at time T by a user behavior record set D before \ntime T [20]. Different actions occur at different times. For example, user1 visit location \na and b when user1 purchasing b and c at T − 3. We need to predict T-time consumer \nbehavior based on different timing behavior characteristics.\n\nAccording to relevant professional research, we divide the data sets of user behav-\nior into three groups in a pre-processing manner. By the feature statistics method, the \n\n(3)C(U → V ) =\nN (U ∪ V )\n\nN\n.\n\nFig. 1 The time series of user position feedback\n\n\n\n\n\nPage 7 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\nfeatures are divided into two types, as shown in Table 1. “True” indicates that the feature \ngroup has corresponding features. Conversely, “False” means no such feature. Next we \nexplain these features.\n\na. Counting feature\n\nFor each feature statistics window, we use the behavioral counting feature and the de-\nduplication counting feature. The behavior count is a cumulative measure of the num-\nber of behaviors that occurred in and before the current window. For the location visit \nbehavior, it represents the number of visits to the product location by the user, the total \nnumber of visits by the user and the total number of visits to the merchandise. The de-\nduplication count feature is similar to the behavioral count, but only the number of non-\nrepetitive behavioral data is counted.\n\nb. Mean feature\n\nIn order to describe the activity of the user and the popularity of the product better, \nthis article derives a series of mean-type features based on the counting features. Take \nthe location visit behavior as an example, the user characteristics group includes the \nuser’s average number of visiting to the product. The average number of visiting to \nthe product by user i is calculated as shown in Formula (4).\n\nc. Ratio feature\n\nThe ratio of user-product behavior to the total behavior of the user and the product \nis also an aspect affecting the user’s degree of preference for the product. In the time \nwindow t, the method to calculate the ratio of the user’s visit to the products’ total \nvisit is shown in Formula (5).\n\nOur work presents a mobile marketing recommendation model is trained by adding \nthe time axis to the user position features. Contrary to current research, it is highly \nusable and low difficulty of achievement for real-world work applications. Consider-\ning the speed of calculation, we study the method of directly embedding time series \ninformation into the collaborative filtering calculation process to improve the recom-\nmendation quality. Specific information will be covered in the following sections.\n\n(4)avgui(t, i, visit) =\naction_count(t,U ,Ui, visit)\n\nuser_unique_item(t,U ,Ui, visit)\n.\n\n(5)rate_ui_in_u(t, i, j, visit) =\naction_count(t,UI ,Ui, Ij, visit)\n\naction_count(t,U ,Ui, visit)\n.\n\nTable 1 Characteristic system diagram (True/False)\n\nFeature group Counting feature Mean feature Ratio feature\n\nUser-product True False True\n\nUser feature True True False\n\nProduct feature True True False\n\n\n\nPage 8 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\nLocation‑based mobile marketing recommendation model by CNN\nCreating the model is one of the most important aspects, which is an evaluation crite-\nrion to make sure correctness of the next step. This section mainly describes the rel-\nevant definitions of LBCNN that are shown in “Relevant definitions of the LBCNN” \nsection, and specific implementation of the model is shown in “Specific implementa-\ntion of the model” section.\n\nRelevant definitions of the LBCNN\n\nIn order to get better feature expression, we consider the user’s timing sensitivity of the \nproduct preferences and the user’s overall preferences comprehensively. This paper uses a \nconvolutional neural network as the basis to build location-based mobile marketing recom-\nmendation model. In the next step, we give the relevant definition.\n\na. Definition 1 (Model framework): based on the above analysis and user’s timing behav-\nior preference feature. We use the convolutional neural network model shown in Fig. 2. The \nmodel is divided into four layers that are input layer, multi-window convolution layer, pool-\ning layer and output layer. The input layer is a well-constructed input feature which trans-\nforms the input features into a two-dimensional plane by time series. Each time window is \nexpressed as an eigenvector. The multi-window convolutional layer convolves the input fea-\nture plane through different lengths of time windows to obtain different feature maps. The \npooling layer reduces the dimension of the feature map to obtain a pooled feature vector. \nThe output layer and the pooling layer are fully connected network structures.\n\nb. Definition 2 (Convolution layer): assume that there are N time windows of the feature \nand each time window has K user preference feature for the commodity. Then input sam-\nple × can be expressed as a matrix of T × K. The feature map in the convolutional layer is \ncalculated by the input layer and the convolution kernel. The window length of the convolu-\ntion kernel is h. xi,i+j represents the eigenvector added by time window i and time window \ni + j. The convolution kernel w can be expressed as a vector of h × K. Feature map f = [f1, f2, \n…, fT−h+1]. The i-th feature fi is calculated according to Formula (6):\n\n(6)fi = σ(w · xi,i+h−1 + b)\n\nFig. 2 The framework of the LBCNN\n\n\n\n\n\nPage 9 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\nwhere b is an offset term and a real number. σ(x) is a nonlinear activation function. This \npaper uses ReLu and Tanh as an activation function. Relu is shown in Formula (7):\n\nc. Definition 3 (Max-pooling): the pooling layer is to scale the feature map while reduc-\ning the complexity of the network. The maximum features of the convolution kernel can \nbe obtained according to the maximum pooling operation. The feature map obtained \nat the kth product of the convolutional kernel is fk = [fk,1, fk,2, …, fk,T−h +1]. The pooling \noperation can be expressed as Formula (8):\n\nd. Definition 4 (Probability distribution): there are M convolution kernels and the output \nlayer has C categories [19]. The weight parameter θ of the output layer is a C × M matrix. \nThe pooled feature f̂ of x is an M-dimensional vector. The probability that x belongs to \nthe i-th category can be expressed as Formula (9):\n\nwhere bk represents the k-th offset of the fully connected layer. The loss function of the \nmodel can be obtained by the likelihood probability value, as shown in Formula (10):\n\nwhere T is the training data set, yi is the real category of the i-th sample, xi is the charac-\nteristic of the i-th sample and θ is the model’s parameters. We learn model parameters \nby minimizing the loss function. The training method adopts the improved gradient \ndescent method proposed by Zeiler. In addition, we have adopted Dropout process-\ning on the convolutional layer to prevent over-fitting of the trained model [21]. The \nDropout method randomizes the neurons in the convolutional layer to 0 with a certain \nprobability.\n\ne. Definition 5 (Latent factor): the value of the latent factor vector is true [22]. Whether \nan item belongs to a class is determined entirely by the user’s behavior. We assume that \ntwo items are liked by many users at the same time, then these two items have a high \nprobability of belonging to the same class. The weight of an item in a class can also be \ncalculated by itself. The implicit semantic model calculates the user’s (u) interest in the \nitem (i) are shown in Formula (11):\n\n(7)\nReLu = max(0, x).\n\nTanh(x) =\nex − e−x\n\nex + e−x\n.\n\n(8)Pool_feature(j) = down(fi).\n\n(9)p(i|x, θ) =\ne(θi·\n\n⌢\nf +bi)\n\n∑C\nk−1 e\n\n(θk ·\n⌢\nf +bk )\n\n(10)J (θ) = −\n\nk\n∑\n\ni=1\n\nlog(p(yi|x, θ))\n\n(11)R(u, i) = rui = pTu qi =\n\nF\n∑\n\nf=1\n\npu,kqi,k\n\n\n\nPage 10 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\nwhere p is the relationship between the user interest and the kth implicit class. q is the \nrelationship between the kth implicit class and the item i. F is the number of hidden \nclasses, and r is the user’s interest in the item.\n\nSpecific implementation of the model\n\nWe can draw from Fig. 3 that the proposed model is divided into two processes. The first \nprocess is the training process and includes two parts. The top module shows how to gener-\nate CNN inputs and outputs from historical data. The other module in the training process \nshows that the traditional CNN parameters are trained by provided data. The second pro-\ncess finished a new location-based marketing resources recommendation. The recommen-\ndation process can work through the CNN parameters provided by the training process.\n\nTo achieve the features of users and location-based mobile marketing resources, the \nlatent factor model (LFM) is used. In traditional LFM, L2-norm regularization is often used \nto optimize training results. However, using L2-norm regularization often leads to excessive \nsmoothing problems. In our model, LFM results are used to represent the characteristics of \nthe training data. In this kind of thinking, we can learn from the training method of regres-\nsion coefficient in regression analysis, and construct a loss function. Therefore, it is more \nreasonable to use sparseness before the specification results. Based on these analyses, we \npropose an improved matrix decomposition method and try to normalize the solution by \n\nFig. 3 Location-based mobile marketing recommendation model by convolutional neural network\n\n\n\n\n\nPage 11 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\nusing the premise of verifying the sparseness of the matrix. The model is presented as For-\nmula (12):\n\nThe next question is how to calculate these two parameters p and q. For the calculation \nof this linear model, this paper uses the gradient descent method. In the Formula (12), \n puk is a user bias item that represents the average of a user’s rating. qik is an item offset \nitem that represents the average of an item being scored. The offset term is an intrinsic \nproperty that indicates whether the item is popular with the public or a user is harsh \non the item. For positive samples, we specify ru,i = 1 based on experience and negative \nsample ru,i = 0, which is shown in Formula (11). The latter λ is a regularization term to \nprevent overfitting.\n\na. Description of the training section\n\nIn Fig. 3, If you want to train CNN, the first thing you need to solve is its input and out-\nput problems. For input, a language model is usually used.\n\nIn terms of output, we propose an improvement in model training by LFM, which is \nconstrained by the regularization of the L1-norm [23]. LFM training data is a historical \nscore between the user and the location-based marketing resources. The rating score can \nbe explicit because it is based on a user tag or an implied tag and it is predicted from the \nuser’s behavior. In this model, in order to ensure that the trained model is representative, \nthe training data we input is to select the existing authoritative standard training set.\n\nb. Description of the recommended part\n\nOnce the LBCNN model structure is established and the model parameters are trained \nusing the training data set, the recommended real-time performance can be achieved. \nThe real-time performance is based on the update of network model parameters in the \nbackground, and it uses some past behavior data and information of the recommended \npeople and products.\n\nUser information and product information can be obtained in advance and digitized. \nIn the offline training model phase, digitized user information, product information, and \nbehavior information are utilized [24]. The same model is trained for the same type of \nusers, and the parameters of the model are periodically updated within a certain period \nof time. In the real-time recommendation stage, real-time recommendation can be real-\nized only by integrating the collected behavior data with the previous data and inputting \nit into the model.\n\nExperimental analysis\nIn order to verify the advantages of convolutional neural network in capturing user’s \ntiming preferences for product and mining users’ temporal behavior characteristics, \nwe compare several commonly used classification models under the same conditions of \ntraining features. They are Linear Logistic Regression Classification Model (LR), Support \n\n(12)J (U ,V ) =\n∑\n\nu,i∈K\n\n(\n\nru,i −\n\nk\n∑\n\nk=1\n\npu,kqi,k\n\n)2\n\n+ ��puk�\n2 + ��qik�\n\n2.\n\n\n\nPage 12 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\nVector Machine (SVM), Random Forest Model (RF) and Gradient Boosting Regression \nTree Model (GBDT) [25]. We also compare the products that have been visited for the \nlast 8 h. Experimental tool is sklearn kit. The hyper parameter settings for each model \nduring the experiment are:\n\na. LR: select L2 regular and the regularization coefficient is 0.1.\nb. SVM: choose radial basis kernel function (RBF) and gamma of kernel function is \n\n0.005.\nc. RF: the number of trees is 200, the entropy is selected as the feature segmentation \n\nstandard and the random feature ratio is 0.5.\nd. GBDT: the number of trees is 100, the learning rate is 0.1 and the maximum depth of \n\nthe tree is 3.\n\nDescription of the data set\n\nThe experiment in our paper uses the dataset disclosed according to the Alibaba Group’s \nmobile recommendation algorithm contest held in 2015. This data set contains 1 month \nof user behavior data and product information. The user’s behavior data includes 10 mil-\nlion users’ various behaviors on 2,876,947 items. Behavior types include clicks, shopping \ncarts and purchases. In addition, each behavior record identifies behavior time that is \naccurate to the hour. The product information includes product category information, \nand identifies whether the product is an online to offline type. In a real business sce-\nnario, we often need to build a personalized recommendation model for a subset of all \nproducts. ", "metadata_storage_path": "aHR0cHM6Ly9zdG9yYWdlYWNjb3VudGxpZ2lyay5ibG9iLmNvcmUud2luZG93cy5uZXQvcGFwZXIvczEzNjczLTAxOS0wMTc3LTYucGRm0", "metadata_author": "Chunyong Yin ", "metadata_title": "Mobile marketing recommendation method based on user location feedback", "keyphrases": [ "Creative Commons Attribution 4.0 International License", "current mobile marketing recommendation system", "Sequential behavior Open Access", "Mobile marketing recommendation method", "users’ timing preference characteristics", "Location-based mobile marketing recommendation", "potential mobile user preferences", "Creative Commons license", "convolutional neural network model", "users’ location feedback behavior", "user location feedback data", "mobile recommendation system", "traditional recommendation models", "user preference information", "creat iveco mmons", "users’ location-based behaviors", "mobile information overload", "user-product binary matrix", "original author(s", "different geographical locations", "shopping location information", "famous e-commerce platforms", "physical store products", "different time windows", "users’ location information", "Hum. Cent. Comput", "mobile network", "information recommendation", "different preferences", "mobile users", "convolutional model", "author information", "irrelevant information", "binary relationships", "different dimensions", "behavioral data", "Chunyong Yin1", "Shilei Ding1", "Jin Wang2", "recent years", "e-commerce industry", "one hand", "various products", "other hand", "large amount", "next consumer", "recommended methods", "third factor", "limited check", "timeli- ness", "hot spots", "accuracy rate", "recall rate", "unrestricted use", "appropriate credit", "RESEARCH Yin", "Inf. Sci.", "Communication Engineering", "Full list", "individual needs", "doi.org", "orcid.org", "experimental results", "Changsha University", "Introduction", "popularization", "Internet", "Alibaba", "zon", "home", "sellers", "goods", "costs", "wealth", "consumption", "case", "advantage", "terms", "Abstract", "attribute", "role", "paper", "LBCNN", "extractor", "classifier", "Keywords", "article", "distribution", "reproduction", "medium", "source", "link", "changes", "Correspondence", "jinwang", "csust", "2 School", "Computer", "Science", "Technology", "China", "creativecommons", "licenses", "crossmark", "crossref", "dialog", "Page", "17Yin", "means", "problem", "implied feature-based cognitive feature collaborative filter- ing", "location-aware based generation probability model", "collaborative filtering methods", "biological neural networks", "Convolutional neural networks", "advanced abstract features", "loca- tion information", "accurate personalized recommendations", "small-scale training data", "detailed comparative analysis", "user informa- tion", "mobile marketing recommendation", "original input features", "business location information", "user preference model", "mobile recommendation research", "feature size", "deep learning algorithms", "mobile users’ preferences", "ing models", "deep model", "DL) model", "shallow model", "multimedia recommendations", "mobile environments", "accurate prediction", "local features", "overall features", "two-dimensional features", "user preferences", "location recommendation", "convolutional layer", "spatial information", "product information", "user information", "hot topic", "service resources", "different assumptions", "spatial activities", "location movement", "negative samples", "semantic content", "data size", "mation analysis", "long-term interest", "time stamps", "great achievements", "speech tasks", "novel field", "interventional optimization", "artificial intelligence", "great breakthroughs", "similar effects", "Weight sharing", "great advantages", "user needs", "positional relevance", "location relevance", "first layer", "previous layer", "convolution kernel", "pooling layer", "context information", "ventional methods", "user models", "many researches", "two layers", "network structure", "shortcomings", "Zhu", "retrograde", "approach", "Yin", "LA-LDA", "scoring", "items", "studies", "distance", "merchant", "area", "proximity", "importance", "gap", "core", "Lian", "impact", "author", "project", "Lee", "visual", "study", "aspects", "machine", "Experiments", "characteristics", "CNN", "difficulty", "number", "weights", "neuron", "Hum", "Cent", "Comput", "images", "location-based", "Location-based mobile marketing recommendation model", "Content‑based recommendation method", "convolutional neural network models", "Representative content-based recommendation systems", "general products recommendation system", "Traditional recommendation method", "traditional recommendation algorithm", "content-based recommendation algorithm", "similar Top-N products", "timing preference characteristics", "interest feature vector", "Content-based information filtering", "Experimental analysis” section", "item feature vector", "project feature vector", "other existing methods", "cosine similarity algorithm", "Related work” section", "user feature vector", "users’ timing preferences", "User description file", "feature models", "Content-based methods", "content filtering", "content characteristics", "other methods", "user-product information", "collaborative filtering", "feedback information", "hybrid methods", "historical content", "training sample", "next moment", "time window", "overall preferences", "Top-K sample", "four sections", "Necessary definitions", "specific implementation", "future progress", "current chapter", "three parts", "time series", "text documents", "accurate comparisons", "different texts", "lowing aspects", "Hum. Cent", "project profile", "user u", "similarity calculation", "sample features", "cast results", "effective application", "calculation formula", "two-class problem", "purchase behavior", "category", "order", "merchandise", "length", "test", "Remain", "sion", "strengths", "weaknesses", "plans", "establishment", "differences", "research", "threshold", "a.", "topics", "projects", "purchasing", "operation", "addition", "modulus", "Lops", "Gemmis", "Semeraro", "content filtering based recommendation method", "m * n matrix R", "traditional data mining method", "user historical behavior data", "traditional collaborative filtering method", "likely N objects", "user behavior data", "various draw- backs", "nearest neigh- bor", "conven- tional method", "association rule algorithm", "higher evaluation algorithms", "hybrid recommendation algorithm", "similar user sets", "mutual user relationships", "content-based recommendation method", "Hybrid recommendation method", "other similar users", "different recommendation algorithms", "The matrix Umn", "content-based filtering", "other method", "hybrid method", "matrix decomposition", "different relationships", "One method", "new content", "evaluation standard", "association rules", "recommendation problem", "standard recommendation", "two algorithms", "different ways", "target user", "user neighbor", "cold-start issues", "same type", "same partition", "expert research", "three steps", "recommendation results", "ommended items", "normal methods", "supervised learning", "online stores", "informa- tion", "new users", "two types", "two specific", "compensa- tion", "different values", "common features", "rating sparsity", "erence information", "new projects", "mendations", "availability", "context", "party", "requirements", "merits", "quality", "memory", "following", "collection", "feedback", "range", "U11", "U1n", "U2n", "Um1", "classification", "survey", "Amazon", "personalized", "customer", "ratings", "techniques", "performance", "ally", "degree", "improvement", "complexity", "different timing behavior characteristics", "good recom- mendations", "relevant professional research", "T-time consumer behavior", "historical behavior data", "timing behavior data", "next purchase behavior", "user position feedback", "similar behavioral items", "collaborative filtering algorithm", "Forecast User3 Shopping", "user behavior record", "User2 Visit location", "time series behavior", "feature statistics method", "timing recommendation model", "user interface layer", "U, V data", "high quality rules", "last behavior", "Different actions", "different times", "U data", "Forecast Shopping", "next section", "product purchase", "recommendation process", "recommendation systems", "c Shopping", "ready-made data", "data sets", "many years", "core idea", "rule algorithms", "total project", "disjoint sets", "association rule", "two criteria", "transac- tions", "key- word", "description layer", "main function", "strong dependence", "most systems", "time information", "time label", "Markov chain", "full use", "prediction problem", "time T", "three groups", "processing manner", "feature group", "Counting feature", "keyword layer", "c Result", "user layer", "unknown items", "traditional algorithm", "preference features", "corresponding features", "total number", "∩V", "V.", "∪ V", "business", "effect", "quantity", "focus", "transactions", "statement", "strength", "support", "confidence", "ratio", "Formula", "interest", "other", "TOP-N", "presentation", "attributes", "dependencies", "keywords", "resource", "users", "dataset", "explanation", "Construction", "Fig.", "example", "user1", "Table", "False", "Location‑based mobile marketing recommendation model", "Counting feature Mean feature Ratio feature", "Table 1 Characteristic system diagram", "collaborative filtering calculation process", "True False Product feature", "False True User feature", "location-based mobile marketing", "duplication counting feature", "behavioral counting feature", "pooled feature vector", "different feature maps", "multi-window convolutional layer", "repetitive behavioral data", "duplication count feature", "feature statistics window", "Specific implementa- tion", "multi-window convolution layer", "pool- ing layer", "ior preference feature", "location visit behavior", "real-world work applications", "products’ total visit", "user characteristics group", "user position features", "Feature group", "counting features", "feature expression", "input feature", "product location", "network structures", "behavior count", "Model framework", "different lengths", "total behavior", "input layer", "output layer", "current window", "mean-type features", "cumulative measure", "time axis", "current research", "low difficulty", "mendation quality", "following sections", "important aspects", "crite- rion", "next step", "evant definitions", "timing sensitivity", "timing behav", "four layers", "user-product behavior", "The model", "Specific information", "product preferences", "average number", "relevant definition", "two-dimensional plane", "behaviors", "visits", "activity", "popularity", "method", "usable", "achievement", "speed", "avgui", "Ij", "True/False", "evaluation", "correctness", "basis", "above", "analysis", "eigenvector", "Input feature convolutional layer Multi-window Max-pooling Output layer layer", "K user preference feature", "convolu- tion kernel", "M convolution kernels", "gradient descent method", "N time windows", "convolution kernel w", "nonlinear activation function", "kth implicit class", "traditional CNN parameters", "implicit semantic model", "maximum pooling operation", "i-th feature fi", "training data set", "C × M matrix", "latent factor vector", "likelihood probability value", "Convolution layer", "convolutional kernel", "feature map", "pooled feature", "kth product", "training method", "maximum features", "CNN inputs", "Dropout method", "i-th category", "loss function", "i-th sample", "Time series", "same time", "window length", "offset term", "C categories", "M-dimensional vector", "k-th offset", "real category", "two items", "many users", "same class", "pTu qi", "hidden classes", "Specific implementation", "two processes", "two parts", "top module", "historical data", "other module", "second pro", "training process", "Probability distribution", "real number", "weight parameter", "model parameters", "user interest", "K.", "∑C", "Definition 2", "commodity", "j.", "framework", "ReLu", "Tanh", "network", "fk", "bk", "teristic", "Zeiler", "fitting", "neurons", "behavior", "high", "Pool_feature", "rui", "relationship", "outputs", "σ", "θ", "existing Users Latent location- historical factor based location data model marketing resources", "new resources process Language CNN Results Recommending model User preferences", "User feature resources Training process preferences model Input Output CNN", "existing authoritative standard training set", "new location-based marketing resources recommendation", "location-based mobile marketing resources", "latent factor model", "offline training model phase", "new location-based Features", "language model", "LBCNN model structure", "convolutional neural network", "historical score", "real-time recommendation stage", "LFM training data", "past behavior data", "network model parameters", "user bias item", "dation process", "CNN parameters", "matrix decomposition method", "Time series Features", "training results", "model training", "item offset item", "specification results", "LFM results", "previous data", "linear model", "same model", "training section", "user tag", "User information", "real-time performance", "sion coefficient", "regression analysis", "next question", "two parameters", "intrinsic property", "positive samples", "negative sample", "first thing", "implied tag", "Experimental analysis", "behavior information", "traditional LFM", "L2-norm regularization", "regularization term", "smoothing problems", "rating score", "excessive", "kind", "thinking", "sparseness", "analyses", "solution", "premise", "mula", "calculation", "puk", "average", "qik", "public", "experience", "latter", "overfitting", "Description", "L1-norm", "part", "update", "background", "people", "products", "advance", "period", "inputting", "advantages", "λ", "10 mil- lion users’ various behaviors", "mining users’ temporal behavior characteristics", "Linear Logistic Regression Classification Model", "Gradient Boosting Regression Tree Model", "mobile recommendation algorithm contest", "radial basis kernel function", "personalized recommendation model", "Random Forest Model", "hyper parameter settings", "feature segmentation standard", "random feature ratio", "product category information", "classification models", "Behavior types", "behavior record", "behavior time", "data set", "timing preferences", "same conditions", "training features", "Vector Machine", "Experimental tool", "sklearn kit", "L2 regular", "regularization coefficient", "learning rate", "maximum depth", "Alibaba Group", "offline type", "c. RF", "several", "LR", "Support", "��puk", "��qik", "SVM", "GBDT", "8 h", "RBF", "gamma", "trees", "entropy", "1 month", "2,876,947 items", "clicks", "shopping", "carts", "purchases", "hour", "online", "nario", "subset", "∑" ], "merged_content": "\nMobile marketing recommendation method \nbased on user location feedback\nChunyong Yin1 , Shilei Ding1 and Jin Wang2*\n\nIntroduction\nIn recent years, the e-commerce industry has developed rapidly with the popularization \nof the Internet. At this time, famous e-commerce platforms such as Alibaba and Ama-\nzon were born. E-commerce moved physical store products to a virtual network plat-\nform. On the one hand, it is convenient for users to buy various products without leaving \nthe home. On the other hand, it is also convenient for sellers to sell their own goods \nand reduce costs. However, the various products have made it more difficult for users \nto select products. E-commerce platform can generate a large amount of user location \nfeedback data which contains a wealth of user preference information [1]. It is significant \nto predict the location of the next consumer’s consumption from these behavioral data. \nAt present, most of the recommended methods focus on the user-product binary matrix \nand directly model their binary relationships [2]. The users’ location information and \nshopping location information are considered as the third factor. In this case, you can \nonly use the limited check-in data. The users’ location feedback behavior and the timeli-\nness of behavior are often overlooked.\n\nThe mobile recommendation system takes advantage of the mobile network environ-\nment in terms of information recommendation and overcomes the disadvantages. Filter-\ning irrelevant information by predicting potential mobile user preferences and providing \n\nAbstract \n\nLocation-based mobile marketing recommendation has become one of the hot spots \nin e-commerce. The current mobile marketing recommendation system only treats \nlocation information as a recommended attribute, which weakens the role of users and \nshopping location information in the recommendation. This paper focuses on location \nfeedback data of user and proposes a location-based mobile marketing recommenda-\ntion model by convolutional neural network (LBCNN). First, the users’ location-based \nbehaviors are divided into different time windows. For each window, the extractor \nachieves users’ timing preference characteristics from different dimensions. Next, we \nuse the convolutional model in the convolutional neural network model to train a \nclassifier. The experimental results show that the model proposed in this paper is better \nthan the traditional recommendation models in the terms of accuracy rate and recall \nrate, both of which increase nearly 10%.\n\nKeywords: Location feedback, Mobile marketing, Convolutional neural network, \nSequential behavior\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nRESEARCH\n\nYin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \nhttps://doi.org/10.1186/s13673-019-0177-6\n\n*Correspondence: \njinwang@csust.edu.cn \n2 School of Computer & \nCommunication Engineering, \nChangsha University \nof Science & Technology, \nChangsha 410004, China\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0001-5764-2432\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13673-019-0177-6&domain=pdf\n\n\nPage 2 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\nmobile users with results that meet users’ individual needs gradually become an effec-\ntive means to alleviate “mobile information overload” [3]. Mobile users have different \npreferences in different geographical locations. For this problem, how to use location \ninformation to obtain mobile users’ preferences and provide accurate personalized \nrecommendations has become a hot topic in mobile recommendation research [4]. \nAlthough there are many researches based on location recommendation, they mainly \nfocus on service resources without positional relevance. To solve the shortcomings of \nresearch on location relevance of service resources is few [5], Zhu et  al. [6] proposed \nthe method which is based on the user’s context information to analyze the user’s pref-\nerences and retrograde. Their approach is to derive user preferences by proposing two \ndifferent assumptions and then recommending user models based on preference analy-\nsis. Yin et al. [7] proposed LA-LDA. The method is a location-aware based generation \nprobability model, which uses scoring based on location to model user information and \nrecommend to users. However, these methods only treat location information as an \nattribute without considering the spatial information of users or items and weaken loca-\ntion information’s role in the recommendation. There are some studies determine user \npreferences by the distance between the mobile user and the merchant [8], but only set \nthe area based on the proximity of the distance and ignore the spatial activities of the \nmobile user [9]. However, these methods were limited to the analysis of user informa-\ntion and product information, and did not carefully consider the importance of user and \nbusiness location information. Therefore, the user preference model based on location \nrecommendation they created has some gap.\n\nConsidering the core of mobile marketing recommendation is location movement, \nLian et al. [10] proposed an implied feature-based cognitive feature collaborative filter-\ning (ICCF) framework, which avoids the impact of negative samples by combining con-\nventional methods and semantic content. In terms of algorithms, the author proposed \nan improved algorithm that can expand according to data size and feature size. To deter-\nmine the relevance of the project to user needs, Lee et al. [11] developed context infor-\nmation analysis and collaborative filtering methods for multimedia recommendations in \nmobile environments. Nevertheless, these methods only used small-scale training data \nand could not achieve accurate prediction of long-term interest for users. In this paper, \ndeep learning and time stamps are used to compensate for these shortcomings.\n\nWith great achievements in visual and speech tasks, the Deep Learning (DL) model \nhas become a novel field of study [12]. Because of the interventional optimization of \ndeep learning algorithms, artificial intelligence has made great breakthroughs in many \naspects. It is well known that models obtained through deep learning and machine learn-\ning models have very similar effects, which learns advanced abstract features from the \noriginal input features by simulating the network structure of the human nervous sys-\ntem. Experiments show that the deep model can express the characteristics of the data \nbetter than the shallow model [13]. Weight sharing by convolution makes CNN similar \nto biological neural networks, which reduces the difficulty of network structure and the \nnumber of weights. The structure of CNN is roughly divided into two layers. It is well \nknown that the first layer is a convolutional layer. Each neuron’s input is connected to the \nprevious layer through a convolution kernel and the local features are extracted. Next \nlayer is a pooling layer. In this layer, the neurons in the network are connected through \n\n\n\nPage 3 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\na convolution kernel to extract the overall features. Convolutional neural networks have \ngreat advantages in processing two-dimensional features [14], such as images.\n\nBased on our detailed comparative analysis, this paper proposes a location-based \nmobile marketing recommendation model by convolutional neural network (LBCNN). \nFirstly, we use user-product information as a training sample, and treat this problem as \na two-class problem. The category of the problem is divided into the purchase behav-\nior and the purchase behavior of the product at the next moment. In order to capture \nthe user’s timing preference characteristics, we divide the behavior of the merchandise \naccording to a certain length of time window and dig deeper into the behavior charac-\nteristics of each time window. Secondly, we consider the users’ timing preferences and \noverall preferences for the product. Then, the features of time window are used to train \nconvolutional neural network models. Finally, we input the sample features of the test \nset into the model and generate the Top-K sample as the location-based purchase fore-\ncast results [15].\n\nRemain of the paper is divided into four sections. Related work is shown in “Related \nwork” section. Necessary definitions and specific implementation of the location-based \nmobile marketing recommendation model by convolutional neural network (LBCNN) \nare shown in “Location-based mobile marketing recommendation model by CNN” sec-\ntion. In “Experimental analysis” section, experimental analysis is introduced. “Conclu-\nsion” section summarizes the strengths and weaknesses of the paper and proposes plans \nfor future progress.\n\nRelated work\nIn the current chapter, we will review existing methods for recommending systems \nthat can be broadly divided into three parts: content filtering, collaborative filtering \nand hybrid methods. We also discuss the establishment of feature models based on \ntime series to clearly represent the differences between our research and other existing \nmethods.\n\nTraditional recommendation method\n\nIn the general products recommendation system, the similarity between users is calcu-\nlated by the user’s interest feature vector. Then, the system recommends some products \nwith similarity greater than a certain threshold or the similar Top-N products to the tar-\nget user. This is a traditional recommendation algorithm based on content and the rec-\nommendation is based on comparing users.\n\na. Content‑based recommendation method\n\nContent-based information filtering has proven to be an effective application for \nlocating text documents related to topics. In particular, we need to focus on the \napplication of content-based information filtering in the recommendation system. \nContent-based methods allow for accurate comparisons between different texts \nor projects, so the recommended results are similar to the historical content of the \nuser’s consumption. The content-based recommendation algorithm involves the fol-\nlowing aspects. User description file describes the user’s preferences, which can be \nfilled by the user and dynamically updated based on the user’s feedback information \n\n\n\nPage 4 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\n(purchasing, reading, clicking, etc.) during the operation of the system. The project \nprofile describes the content characteristics of each project, which constitutes the \nfeature vector of the project. In addition, the similarity calculation is the similarity \nbetween the user’s description file and the item feature vector.\n\nThe similarity calculation of the content-based recommendation algorithm usually \nadopts the cosine similarity algorithm. The algorithm needs to calculate the similarity \nbetween the feature vector of user u and the feature vector of item i. The calculation \nformula is as shown in Formula (1).\n\nwhere ⇀u denotes the user feature vector, \n⇀\n\ni denotes the project feature vector, \n⇀\n\n|u| is the \nmodulus of the user feature vector and \n\n⇀\n\n|i| is the model of the project feature vector.\nRepresentative content-based recommendation systems mainly include Lops, \n\nGemmis, and Semeraro [16]. Compared to other methods, content-based recom-\nmendations have no cold-start issues and recommendations are easy to understand. \nHowever, the content filtering based recommendation method has various draw-\nbacks, such as strongly relying on the availability of content and ignoring the context \ninformation of the recommended party. The content-based recommendation method \nalso has certain requirements for the format of the project. Besides, it is difficult to \ndistinguish the merits of the project. The same type of project may have the same type \nof features, which are difficult to reflect the quality of the project.\n\nb. Collaborative filtering method\n\nThe recommendation based on collaborative filtering solves the recommendation \nproblem by using the information of similar users in the same partition to analyze and \nrecommend new content that has not been scored or seen by the target user.\n\nRegarding the traditional collaborative filtering method based on memory, we \nunderstand that this method is based on the different relationships between users and \nprojects. According to expert research, the traditional collaborative filtering method \nbased on memory should be divided into the following three steps.\n\nStep 1: collection of user behavior data, this step represents the user’s past behav-\nior with a m * n matrix R. The matrix Umn represents the feedback that the user m \nhas on the recommended object n. Rating is a range of values and different values \nrepresent how much the user likes the recommended object.\n\nStep 2: establishment of a user neighbor: establish mutual user relationships by \nanalyzing all user historical behavior data.\n\n(1)sim(u, i) =\n\n⇀\nu ·\n\n⇀\n\ni\n\n⇀\n\n|u|\n⇀\n\n|i|\n\nU =\n\n\n\n\n\n\n\nU11 U12 . . . U1n\n\nU21 U22 . . . U2n\n\n. . . . . . . . . . . .\n\nUm1 Um2 . . . Umn\n\n\n\n\n\n\n.\n\n\n\nPage 5 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\nStep 3: generate recommendation results: find the most likely N objects from the rec-\nommended items selected by similar user sets.\n\nTherefore, recommendations are made by mining common features in similar users’ pref-\nerence information [17]. The normal methods in this classification include k-nearest neigh-\nbor (k-NN), matrix decomposition, and semi-supervised learning. According to the survey, \nAmazon uses an item-by-item collaborative filtering method to recommend personalized \nonline stores for each customer.\n\nCompared to other method, collaborative filtering has the ability to filter out informa-\ntion that can be automatically recognized by the machine and effectively use feedback from \nother similar users. However, collaborative filtering requires more ratings for the project, \nso it is affected by the issue of rating sparsity. In addition, this method does not provide a \nstandard recommendation for new users and new projects, which is called a cold start issue.\n\nc. Hybrid recommendation method\n\nThe hybrid recommendation method combines the above techniques in different ways to \nimprove the recommended performance and optimize the shortcomings of the conven-\ntional method. Projects that cannot be recommended for collaborative filtering are gener-\nally addressed by combining them with content-based filtering [18].\n\nThe core of this method is to independently calculate the recommendation results of the \ntwo types of recommendation algorithms, and then mix the results. There are two specific \nhybrid methods. One method is to mix the predicted scores of the two algorithms linearly. \nAnother hybrid method is to set up an evaluation standard, compare the recommended \nresults of the two algorithms, and take the recommendation results of the higher evaluation \nalgorithms. In general, the hybrid recommendation achieves a certain degree of compensa-\ntion between different recommendation algorithms. However, the hybrid recommendation \nalgorithm still needs improvement in complexity.\n\nd. Recommendation based on association rules\n\nThe association rule algorithm is a traditional data mining method that has been widely \nused in business for many years. The core idea is to analyze the rules of user historical \nbehavior data to recommend more similar behavioral items [19]. Rules can be either user-\ndefined or dynamically generated by using rule algorithms. The effect of the algorithm \ndepends mainly on the quantity and quality of the rules so the focus of the algorithm is on \nhow to develop high quality rules.\n\nDefine N as the total number of transactions, R is the total project and U and V are two \ndisjoint sets of items (U∩V ≠ ∅, U∈R, V∈R). The association rule is essentially an IF–Then \nstatement, here is expressed by U → V. The strength of the association rule U → V can be \nmeasured by two criteria: support and confidence. S is the ratio containing U and V data \nwhich both represent the number of transactions, which is shown in Formula (2).\n\nC is the ratio of U, V data to the only U data which represents the number of transac-\ntions, as shown in Formula (3)\n\n(2)S(U → V ) =\nN (U ∪ V )\n\nN\n.\n\n\n\nPage 6 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\nThe recommendation process of the algorithm is shown in below.\nFirstly, according to the items of interest to the user, the user’s interest in other \n\nunknown items is predicted by rules. Secondly, compare the support of the rules. Finally, \nthe recommended items of TOP-N are obtained to the user.\n\nThe recommendation system based on association rules includes three parts: the key-\nword, the presentation and the user interface. The keyword layer is a set of keyword \nattributes and dependencies between keywords. The description layer connects the \nkeyword layer and the user layer and the main function is to describe the user and the \nresource. The user interface layer is the layer that interacts directly with the user. How-\never, the system becomes more and more difficult to manage as the rules increasing. In \naddition, there is a strong dependence on the quality of the rules and a cold start prob-\nlem is existed.\n\nMost of the recommendation systems use collaborative filtering algorithm to recom-\nmend for users. However, the traditional algorithm can only analyze ready-made data \nsimply, and most systems simply preprocess the data. In our method, we preprocess the \ndataset by extending the time information of the data to a time label. The next section is \nan explanation of the specific implementation.\n\nConstruction of time series behavior’s preference features\n\nThe timing recommendation model is based primarily on the Markov chain. This model \nmakes full use of timing behavior data to predict the next purchase behavior based on \nthe user’s last behavior. The advantage of this model is that it can generate good recom-\nmendations by timing behavior.\n\nAs shown in Fig. 1, the prediction problem of product purchase can be expressed as \npredicts the user’s purchase behavior at time T by a user behavior record set D before \ntime T [20]. Different actions occur at different times. For example, user1 visit location \na and b when user1 purchasing b and c at T − 3. We need to predict T-time consumer \nbehavior based on different timing behavior characteristics.\n\nAccording to relevant professional research, we divide the data sets of user behav-\nior into three groups in a pre-processing manner. By the feature statistics method, the \n\n(3)C(U → V ) =\nN (U ∪ V )\n\nN\n.\n\nFig. 1 The time series of user position feedback\n\n Time series T-3 T-2 7-1 T User1 Visit location: a, b Visit location: a, b Forecast Shopping: b, c Shopping: b, c Result ? User2 Visit location: a, b Forecast Shopping: b, c Result ? Visit location: a, b Forecast User3 Shopping: b, c Result ? \n\n\n\nPage 7 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\nfeatures are divided into two types, as shown in Table 1. “True” indicates that the feature \ngroup has corresponding features. Conversely, “False” means no such feature. Next we \nexplain these features.\n\na. Counting feature\n\nFor each feature statistics window, we use the behavioral counting feature and the de-\nduplication counting feature. The behavior count is a cumulative measure of the num-\nber of behaviors that occurred in and before the current window. For the location visit \nbehavior, it represents the number of visits to the product location by the user, the total \nnumber of visits by the user and the total number of visits to the merchandise. The de-\nduplication count feature is similar to the behavioral count, but only the number of non-\nrepetitive behavioral data is counted.\n\nb. Mean feature\n\nIn order to describe the activity of the user and the popularity of the product better, \nthis article derives a series of mean-type features based on the counting features. Take \nthe location visit behavior as an example, the user characteristics group includes the \nuser’s average number of visiting to the product. The average number of visiting to \nthe product by user i is calculated as shown in Formula (4).\n\nc. Ratio feature\n\nThe ratio of user-product behavior to the total behavior of the user and the product \nis also an aspect affecting the user’s degree of preference for the product. In the time \nwindow t, the method to calculate the ratio of the user’s visit to the products’ total \nvisit is shown in Formula (5).\n\nOur work presents a mobile marketing recommendation model is trained by adding \nthe time axis to the user position features. Contrary to current research, it is highly \nusable and low difficulty of achievement for real-world work applications. Consider-\ning the speed of calculation, we study the method of directly embedding time series \ninformation into the collaborative filtering calculation process to improve the recom-\nmendation quality. Specific information will be covered in the following sections.\n\n(4)avgui(t, i, visit) =\naction_count(t,U ,Ui, visit)\n\nuser_unique_item(t,U ,Ui, visit)\n.\n\n(5)rate_ui_in_u(t, i, j, visit) =\naction_count(t,UI ,Ui, Ij, visit)\n\naction_count(t,U ,Ui, visit)\n.\n\nTable 1 Characteristic system diagram (True/False)\n\nFeature group Counting feature Mean feature Ratio feature\n\nUser-product True False True\n\nUser feature True True False\n\nProduct feature True True False\n\n\n\nPage 8 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\nLocation‑based mobile marketing recommendation model by CNN\nCreating the model is one of the most important aspects, which is an evaluation crite-\nrion to make sure correctness of the next step. This section mainly describes the rel-\nevant definitions of LBCNN that are shown in “Relevant definitions of the LBCNN” \nsection, and specific implementation of the model is shown in “Specific implementa-\ntion of the model” section.\n\nRelevant definitions of the LBCNN\n\nIn order to get better feature expression, we consider the user’s timing sensitivity of the \nproduct preferences and the user’s overall preferences comprehensively. This paper uses a \nconvolutional neural network as the basis to build location-based mobile marketing recom-\nmendation model. In the next step, we give the relevant definition.\n\na. Definition 1 (Model framework): based on the above analysis and user’s timing behav-\nior preference feature. We use the convolutional neural network model shown in Fig. 2. The \nmodel is divided into four layers that are input layer, multi-window convolution layer, pool-\ning layer and output layer. The input layer is a well-constructed input feature which trans-\nforms the input features into a two-dimensional plane by time series. Each time window is \nexpressed as an eigenvector. The multi-window convolutional layer convolves the input fea-\nture plane through different lengths of time windows to obtain different feature maps. The \npooling layer reduces the dimension of the feature map to obtain a pooled feature vector. \nThe output layer and the pooling layer are fully connected network structures.\n\nb. Definition 2 (Convolution layer): assume that there are N time windows of the feature \nand each time window has K user preference feature for the commodity. Then input sam-\nple × can be expressed as a matrix of T × K. The feature map in the convolutional layer is \ncalculated by the input layer and the convolution kernel. The window length of the convolu-\ntion kernel is h. xi,i+j represents the eigenvector added by time window i and time window \ni + j. The convolution kernel w can be expressed as a vector of h × K. Feature map f = [f1, f2, \n…, fT−h+1]. The i-th feature fi is calculated according to Formula (6):\n\n(6)fi = σ(w · xi,i+h−1 + b)\n\nFig. 2 The framework of the LBCNN\n\n Time series- ------ -- - - - - - -- -- Input feature convolutional layer Multi-window Max-pooling Output layer layer \n\n\n\nPage 9 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\nwhere b is an offset term and a real number. σ(x) is a nonlinear activation function. This \npaper uses ReLu and Tanh as an activation function. Relu is shown in Formula (7):\n\nc. Definition 3 (Max-pooling): the pooling layer is to scale the feature map while reduc-\ning the complexity of the network. The maximum features of the convolution kernel can \nbe obtained according to the maximum pooling operation. The feature map obtained \nat the kth product of the convolutional kernel is fk = [fk,1, fk,2, …, fk,T−h +1]. The pooling \noperation can be expressed as Formula (8):\n\nd. Definition 4 (Probability distribution): there are M convolution kernels and the output \nlayer has C categories [19]. The weight parameter θ of the output layer is a C × M matrix. \nThe pooled feature f̂ of x is an M-dimensional vector. The probability that x belongs to \nthe i-th category can be expressed as Formula (9):\n\nwhere bk represents the k-th offset of the fully connected layer. The loss function of the \nmodel can be obtained by the likelihood probability value, as shown in Formula (10):\n\nwhere T is the training data set, yi is the real category of the i-th sample, xi is the charac-\nteristic of the i-th sample and θ is the model’s parameters. We learn model parameters \nby minimizing the loss function. The training method adopts the improved gradient \ndescent method proposed by Zeiler. In addition, we have adopted Dropout process-\ning on the convolutional layer to prevent over-fitting of the trained model [21]. The \nDropout method randomizes the neurons in the convolutional layer to 0 with a certain \nprobability.\n\ne. Definition 5 (Latent factor): the value of the latent factor vector is true [22]. Whether \nan item belongs to a class is determined entirely by the user’s behavior. We assume that \ntwo items are liked by many users at the same time, then these two items have a high \nprobability of belonging to the same class. The weight of an item in a class can also be \ncalculated by itself. The implicit semantic model calculates the user’s (u) interest in the \nitem (i) are shown in Formula (11):\n\n(7)\nReLu = max(0, x).\n\nTanh(x) =\nex − e−x\n\nex + e−x\n.\n\n(8)Pool_feature(j) = down(fi).\n\n(9)p(i|x, θ) =\ne(θi·\n\n⌢\nf +bi)\n\n∑C\nk−1 e\n\n(θk ·\n⌢\nf +bk )\n\n(10)J (θ) = −\n\nk\n∑\n\ni=1\n\nlog(p(yi|x, θ))\n\n(11)R(u, i) = rui = pTu qi =\n\nF\n∑\n\nf=1\n\npu,kqi,k\n\n\n\nPage 10 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\nwhere p is the relationship between the user interest and the kth implicit class. q is the \nrelationship between the kth implicit class and the item i. F is the number of hidden \nclasses, and r is the user’s interest in the item.\n\nSpecific implementation of the model\n\nWe can draw from Fig. 3 that the proposed model is divided into two processes. The first \nprocess is the training process and includes two parts. The top module shows how to gener-\nate CNN inputs and outputs from historical data. The other module in the training process \nshows that the traditional CNN parameters are trained by provided data. The second pro-\ncess finished a new location-based marketing resources recommendation. The recommen-\ndation process can work through the CNN parameters provided by the training process.\n\nTo achieve the features of users and location-based mobile marketing resources, the \nlatent factor model (LFM) is used. In traditional LFM, L2-norm regularization is often used \nto optimize training results. However, using L2-norm regularization often leads to excessive \nsmoothing problems. In our model, LFM results are used to represent the characteristics of \nthe training data. In this kind of thinking, we can learn from the training method of regres-\nsion coefficient in regression analysis, and construct a loss function. Therefore, it is more \nreasonable to use sparseness before the specification results. Based on these analyses, we \npropose an improved matrix decomposition method and try to normalize the solution by \n\nFig. 3 Location-based mobile marketing recommendation model by convolutional neural network\n\n The existing Users Latent location- historical factor based location data model marketing resources Time series Features of User feature resources Training process preferences model Input Output CNN A new location-based Features of marketing resources new resources process Language CNN Results Recommending model User preferences \n\n\n\nPage 11 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\nusing the premise of verifying the sparseness of the matrix. The model is presented as For-\nmula (12):\n\nThe next question is how to calculate these two parameters p and q. For the calculation \nof this linear model, this paper uses the gradient descent method. In the Formula (12), \n puk is a user bias item that represents the average of a user’s rating. qik is an item offset \nitem that represents the average of an item being scored. The offset term is an intrinsic \nproperty that indicates whether the item is popular with the public or a user is harsh \non the item. For positive samples, we specify ru,i = 1 based on experience and negative \nsample ru,i = 0, which is shown in Formula (11). The latter λ is a regularization term to \nprevent overfitting.\n\na. Description of the training section\n\nIn Fig. 3, If you want to train CNN, the first thing you need to solve is its input and out-\nput problems. For input, a language model is usually used.\n\nIn terms of output, we propose an improvement in model training by LFM, which is \nconstrained by the regularization of the L1-norm [23]. LFM training data is a historical \nscore between the user and the location-based marketing resources. The rating score can \nbe explicit because it is based on a user tag or an implied tag and it is predicted from the \nuser’s behavior. In this model, in order to ensure that the trained model is representative, \nthe training data we input is to select the existing authoritative standard training set.\n\nb. Description of the recommended part\n\nOnce the LBCNN model structure is established and the model parameters are trained \nusing the training data set, the recommended real-time performance can be achieved. \nThe real-time performance is based on the update of network model parameters in the \nbackground, and it uses some past behavior data and information of the recommended \npeople and products.\n\nUser information and product information can be obtained in advance and digitized. \nIn the offline training model phase, digitized user information, product information, and \nbehavior information are utilized [24]. The same model is trained for the same type of \nusers, and the parameters of the model are periodically updated within a certain period \nof time. In the real-time recommendation stage, real-time recommendation can be real-\nized only by integrating the collected behavior data with the previous data and inputting \nit into the model.\n\nExperimental analysis\nIn order to verify the advantages of convolutional neural network in capturing user’s \ntiming preferences for product and mining users’ temporal behavior characteristics, \nwe compare several commonly used classification models under the same conditions of \ntraining features. They are Linear Logistic Regression Classification Model (LR), Support \n\n(12)J (U ,V ) =\n∑\n\nu,i∈K\n\n(\n\nru,i −\n\nk\n∑\n\nk=1\n\npu,kqi,k\n\n)2\n\n+ ��puk�\n2 + ��qik�\n\n2.\n\n\n\nPage 12 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\nVector Machine (SVM), Random Forest Model (RF) and Gradient Boosting Regression \nTree Model (GBDT) [25]. We also compare the products that have been visited for the \nlast 8 h. Experimental tool is sklearn kit. The hyper parameter settings for each model \nduring the experiment are:\n\na. LR: select L2 regular and the regularization coefficient is 0.1.\nb. SVM: choose radial basis kernel function (RBF) and gamma of kernel function is \n\n0.005.\nc. RF: the number of trees is 200, the entropy is selected as the feature segmentation \n\nstandard and the random feature ratio is 0.5.\nd. GBDT: the number of trees is 100, the learning rate is 0.1 and the maximum depth of \n\nthe tree is 3.\n\nDescription of the data set\n\nThe experiment in our paper uses the dataset disclosed according to the Alibaba Group’s \nmobile recommendation algorithm contest held in 2015. This data set contains 1 month \nof user behavior data and product information. The user’s behavior data includes 10 mil-\nlion users’ various behaviors on 2,876,947 items. Behavior types include clicks, shopping \ncarts and purchases. In addition, each behavior record identifies behavior time that is \naccurate to the hour. The product information includes product category information, \nand identifies whether the product is an online to offline type. In a real business sce-\nnario, we often need to build a personalized recommendation model for a subset of all \nproducts. ", "text": [ "Time series T-3 T-2 7-1 T User1 Visit location: a, b Visit location: a, b Forecast Shopping: b, c Shopping: b, c Result ? User2 Visit location: a, b Forecast Shopping: b, c Result ? Visit location: a, b Forecast User3 Shopping: b, c Result ?", "Time series- ------ -- - - - - - -- -- Input feature convolutional layer Multi-window Max-pooling Output layer layer", "The existing Users Latent location- historical factor based location data model marketing resources Time series Features of User feature resources Training process preferences model Input Output CNN A new location-based Features of marketing resources new resources process Language CNN Results Recommending model User preferences", "User behavior data Pageviews > No Retain data 30,000 Yes Purchase >3 Yes- Retain data 0 No Eliminate data", "Validation accuracy -Testing accuary Training accuracy 98 96 94 92 Accuracy rate/% 06 88 86 84 82 1 2 3 4 5 6 7 Number of iterations", "Published online: 01 May 2019" ], "layoutText": [ "{\"language\":\"en\",\"text\":\"Time series T-3 T-2 7-1 T User1 Visit location: a, b Visit location: a, b Forecast Shopping: b, c Shopping: b, c Result ? User2 Visit location: a, b Forecast Shopping: b, c Result ? Visit location: a, b Forecast User3 Shopping: b, c Result ?\",\"lines\":[{\"boundingBox\":[{\"x\":890,\"y\":2},{\"x\":1149,\"y\":3},{\"x\":1149,\"y\":48},{\"x\":890,\"y\":48}],\"text\":\"Time series\"},{\"boundingBox\":[{\"x\":485,\"y\":101},{\"x\":549,\"y\":101},{\"x\":549,\"y\":145},{\"x\":484,\"y\":145}],\"text\":\"T-3\"},{\"boundingBox\":[{\"x\":904,\"y\":99},{\"x\":982,\"y\":100},{\"x\":979,\"y\":146},{\"x\":903,\"y\":144}],\"text\":\"T-2\"},{\"boundingBox\":[{\"x\":1324,\"y\":101},{\"x\":1400,\"y\":101},{\"x\":1399,\"y\":146},{\"x\":1322,\"y\":146}],\"text\":\"7-1\"},{\"boundingBox\":[{\"x\":1812,\"y\":104},{\"x\":1848,\"y\":105},{\"x\":1845,\"y\":145},{\"x\":1810,\"y\":144}],\"text\":\"T\"},{\"boundingBox\":[{\"x\":41,\"y\":316},{\"x\":166,\"y\":315},{\"x\":167,\"y\":359},{\"x\":41,\"y\":362}],\"text\":\"User1\"},{\"boundingBox\":[{\"x\":310,\"y\":283},{\"x\":715,\"y\":285},{\"x\":715,\"y\":333},{\"x\":310,\"y\":329}],\"text\":\"Visit location: a, b\"},{\"boundingBox\":[{\"x\":1161,\"y\":283},{\"x\":1566,\"y\":284},{\"x\":1565,\"y\":334},{\"x\":1161,\"y\":332}],\"text\":\"Visit location: a, b\"},{\"boundingBox\":[{\"x\":1729,\"y\":285},{\"x\":1921,\"y\":285},{\"x\":1921,\"y\":328},{\"x\":1729,\"y\":328}],\"text\":\"Forecast\"},{\"boundingBox\":[{\"x\":310,\"y\":348},{\"x\":634,\"y\":348},{\"x\":634,\"y\":402},{\"x\":310,\"y\":402}],\"text\":\"Shopping: b, c\"},{\"boundingBox\":[{\"x\":1164,\"y\":347},{\"x\":1480,\"y\":347},{\"x\":1480,\"y\":402},{\"x\":1164,\"y\":402}],\"text\":\"Shopping: b, c\"},{\"boundingBox\":[{\"x\":1738,\"y\":348},{\"x\":1916,\"y\":347},{\"x\":1917,\"y\":391},{\"x\":1738,\"y\":392}],\"text\":\"Result ?\"},{\"boundingBox\":[{\"x\":43,\"y\":518},{\"x\":176,\"y\":517},{\"x\":177,\"y\":562},{\"x\":44,\"y\":564}],\"text\":\"User2\"},{\"boundingBox\":[{\"x\":738,\"y\":484},{\"x\":1141,\"y\":484},{\"x\":1141,\"y\":535},{\"x\":738,\"y\":533}],\"text\":\"Visit location: a, b\"},{\"boundingBox\":[{\"x\":1729,\"y\":487},{\"x\":1923,\"y\":487},{\"x\":1923,\"y\":529},{\"x\":1729,\"y\":530}],\"text\":\"Forecast\"},{\"boundingBox\":[{\"x\":740,\"y\":550},{\"x\":1056,\"y\":551},{\"x\":1055,\"y\":606},{\"x\":739,\"y\":604}],\"text\":\"Shopping: b, c\"},{\"boundingBox\":[{\"x\":1737,\"y\":549},{\"x\":1915,\"y\":548},{\"x\":1915,\"y\":596},{\"x\":1737,\"y\":597}],\"text\":\"Result ?\"},{\"boundingBox\":[{\"x\":1159,\"y\":687},{\"x\":1566,\"y\":688},{\"x\":1565,\"y\":736},{\"x\":1159,\"y\":733}],\"text\":\"Visit location: a, b\"},{\"boundingBox\":[{\"x\":1728,\"y\":689},{\"x\":1921,\"y\":689},{\"x\":1922,\"y\":732},{\"x\":1728,\"y\":732}],\"text\":\"Forecast\"},{\"boundingBox\":[{\"x\":39,\"y\":720},{\"x\":172,\"y\":719},{\"x\":172,\"y\":763},{\"x\":40,\"y\":764}],\"text\":\"User3\"},{\"boundingBox\":[{\"x\":1162,\"y\":753},{\"x\":1481,\"y\":753},{\"x\":1481,\"y\":805},{\"x\":1162,\"y\":806}],\"text\":\"Shopping: b, c\"},{\"boundingBox\":[{\"x\":1737,\"y\":753},{\"x\":1915,\"y\":752},{\"x\":1915,\"y\":796},{\"x\":1737,\"y\":797}],\"text\":\"Result ?\"}],\"words\":[{\"boundingBox\":[{\"x\":896,\"y\":3},{\"x\":1009,\"y\":3},{\"x\":1010,\"y\":49},{\"x\":898,\"y\":49}],\"text\":\"Time\"},{\"boundingBox\":[{\"x\":1018,\"y\":3},{\"x\":1149,\"y\":3},{\"x\":1149,\"y\":49},{\"x\":1019,\"y\":49}],\"text\":\"series\"},{\"boundingBox\":[{\"x\":484,\"y\":101},{\"x\":549,\"y\":101},{\"x\":549,\"y\":145},{\"x\":484,\"y\":145}],\"text\":\"T-3\"},{\"boundingBox\":[{\"x\":903,\"y\":99},{\"x\":980,\"y\":100},{\"x\":979,\"y\":146},{\"x\":903,\"y\":144}],\"text\":\"T-2\"},{\"boundingBox\":[{\"x\":1325,\"y\":101},{\"x\":1400,\"y\":101},{\"x\":1400,\"y\":146},{\"x\":1325,\"y\":146}],\"text\":\"7-1\"},{\"boundingBox\":[{\"x\":1814,\"y\":104},{\"x\":1846,\"y\":105},{\"x\":1845,\"y\":145},{\"x\":1813,\"y\":144}],\"text\":\"T\"},{\"boundingBox\":[{\"x\":41,\"y\":316},{\"x\":166,\"y\":315},{\"x\":167,\"y\":360},{\"x\":41,\"y\":362}],\"text\":\"User1\"},{\"boundingBox\":[{\"x\":314,\"y\":283},{\"x\":416,\"y\":284},{\"x\":417,\"y\":331},{\"x\":315,\"y\":329}],\"text\":\"Visit\"},{\"boundingBox\":[{\"x\":425,\"y\":284},{\"x\":621,\"y\":285},{\"x\":622,\"y\":333},{\"x\":426,\"y\":331}],\"text\":\"location:\"},{\"boundingBox\":[{\"x\":630,\"y\":285},{\"x\":672,\"y\":285},{\"x\":673,\"y\":333},{\"x\":631,\"y\":333}],\"text\":\"a,\"},{\"boundingBox\":[{\"x\":681,\"y\":285},{\"x\":714,\"y\":285},{\"x\":715,\"y\":333},{\"x\":682,\"y\":333}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1162,\"y\":284},{\"x\":1267,\"y\":284},{\"x\":1266,\"y\":332},{\"x\":1162,\"y\":331}],\"text\":\"Visit\"},{\"boundingBox\":[{\"x\":1276,\"y\":284},{\"x\":1469,\"y\":284},{\"x\":1469,\"y\":334},{\"x\":1275,\"y\":332}],\"text\":\"location:\"},{\"boundingBox\":[{\"x\":1478,\"y\":284},{\"x\":1518,\"y\":284},{\"x\":1518,\"y\":334},{\"x\":1478,\"y\":334}],\"text\":\"a,\"},{\"boundingBox\":[{\"x\":1528,\"y\":284},{\"x\":1565,\"y\":285},{\"x\":1564,\"y\":335},{\"x\":1527,\"y\":334}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1731,\"y\":285},{\"x\":1922,\"y\":286},{\"x\":1921,\"y\":328},{\"x\":1729,\"y\":328}],\"text\":\"Forecast\"},{\"boundingBox\":[{\"x\":312,\"y\":348},{\"x\":532,\"y\":349},{\"x\":532,\"y\":403},{\"x\":311,\"y\":401}],\"text\":\"Shopping:\"},{\"boundingBox\":[{\"x\":542,\"y\":349},{\"x\":587,\"y\":349},{\"x\":588,\"y\":401},{\"x\":543,\"y\":402}],\"text\":\"b,\"},{\"boundingBox\":[{\"x\":597,\"y\":349},{\"x\":631,\"y\":349},{\"x\":633,\"y\":400},{\"x\":598,\"y\":401}],\"text\":\"c\"},{\"boundingBox\":[{\"x\":1165,\"y\":347},{\"x\":1381,\"y\":348},{\"x\":1380,\"y\":403},{\"x\":1164,\"y\":400}],\"text\":\"Shopping:\"},{\"boundingBox\":[{\"x\":1391,\"y\":348},{\"x\":1436,\"y\":348},{\"x\":1435,\"y\":402},{\"x\":1391,\"y\":403}],\"text\":\"b,\"},{\"boundingBox\":[{\"x\":1446,\"y\":348},{\"x\":1480,\"y\":349},{\"x\":1480,\"y\":400},{\"x\":1446,\"y\":401}],\"text\":\"c\"},{\"boundingBox\":[{\"x\":1739,\"y\":349},{\"x\":1879,\"y\":348},{\"x\":1880,\"y\":392},{\"x\":1739,\"y\":392}],\"text\":\"Result\"},{\"boundingBox\":[{\"x\":1887,\"y\":348},{\"x\":1916,\"y\":348},{\"x\":1917,\"y\":392},{\"x\":1888,\"y\":392}],\"text\":\"?\"},{\"boundingBox\":[{\"x\":43,\"y\":518},{\"x\":174,\"y\":517},{\"x\":175,\"y\":563},{\"x\":43,\"y\":564}],\"text\":\"User2\"},{\"boundingBox\":[{\"x\":739,\"y\":485},{\"x\":841,\"y\":485},{\"x\":841,\"y\":534},{\"x\":739,\"y\":533}],\"text\":\"Visit\"},{\"boundingBox\":[{\"x\":850,\"y\":485},{\"x\":1044,\"y\":485},{\"x\":1044,\"y\":535},{\"x\":850,\"y\":534}],\"text\":\"location:\"},{\"boundingBox\":[{\"x\":1053,\"y\":485},{\"x\":1095,\"y\":485},{\"x\":1095,\"y\":536},{\"x\":1054,\"y\":535}],\"text\":\"a,\"},{\"boundingBox\":[{\"x\":1104,\"y\":485},{\"x\":1139,\"y\":485},{\"x\":1139,\"y\":536},{\"x\":1105,\"y\":536}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1731,\"y\":488},{\"x\":1923,\"y\":487},{\"x\":1924,\"y\":530},{\"x\":1729,\"y\":531}],\"text\":\"Forecast\"},{\"boundingBox\":[{\"x\":741,\"y\":550},{\"x\":956,\"y\":551},{\"x\":956,\"y\":606},{\"x\":740,\"y\":602}],\"text\":\"Shopping:\"},{\"boundingBox\":[{\"x\":967,\"y\":551},{\"x\":1011,\"y\":551},{\"x\":1011,\"y\":605},{\"x\":966,\"y\":606}],\"text\":\"b,\"},{\"boundingBox\":[{\"x\":1021,\"y\":552},{\"x\":1056,\"y\":552},{\"x\":1056,\"y\":604},{\"x\":1021,\"y\":605}],\"text\":\"c\"},{\"boundingBox\":[{\"x\":1737,\"y\":551},{\"x\":1878,\"y\":549},{\"x\":1879,\"y\":597},{\"x\":1737,\"y\":596}],\"text\":\"Result\"},{\"boundingBox\":[{\"x\":1887,\"y\":549},{\"x\":1914,\"y\":549},{\"x\":1915,\"y\":596},{\"x\":1888,\"y\":596}],\"text\":\"?\"},{\"boundingBox\":[{\"x\":1163,\"y\":688},{\"x\":1266,\"y\":687},{\"x\":1266,\"y\":734},{\"x\":1163,\"y\":733}],\"text\":\"Visit\"},{\"boundingBox\":[{\"x\":1274,\"y\":687},{\"x\":1472,\"y\":688},{\"x\":1472,\"y\":736},{\"x\":1275,\"y\":734}],\"text\":\"location:\"},{\"boundingBox\":[{\"x\":1481,\"y\":688},{\"x\":1522,\"y\":689},{\"x\":1522,\"y\":737},{\"x\":1481,\"y\":736}],\"text\":\"a,\"},{\"boundingBox\":[{\"x\":1531,\"y\":689},{\"x\":1566,\"y\":689},{\"x\":1566,\"y\":737},{\"x\":1531,\"y\":737}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1731,\"y\":690},{\"x\":1922,\"y\":690},{\"x\":1921,\"y\":733},{\"x\":1729,\"y\":733}],\"text\":\"Forecast\"},{\"boundingBox\":[{\"x\":40,\"y\":722},{\"x\":172,\"y\":720},{\"x\":173,\"y\":764},{\"x\":42,\"y\":765}],\"text\":\"User3\"},{\"boundingBox\":[{\"x\":1163,\"y\":754},{\"x\":1381,\"y\":754},{\"x\":1382,\"y\":806},{\"x\":1162,\"y\":806}],\"text\":\"Shopping:\"},{\"boundingBox\":[{\"x\":1391,\"y\":754},{\"x\":1435,\"y\":754},{\"x\":1437,\"y\":806},{\"x\":1392,\"y\":806}],\"text\":\"b,\"},{\"boundingBox\":[{\"x\":1446,\"y\":754},{\"x\":1480,\"y\":754},{\"x\":1481,\"y\":805},{\"x\":1447,\"y\":805}],\"text\":\"c\"},{\"boundingBox\":[{\"x\":1738,\"y\":754},{\"x\":1879,\"y\":753},{\"x\":1879,\"y\":797},{\"x\":1738,\"y\":798}],\"text\":\"Result\"},{\"boundingBox\":[{\"x\":1887,\"y\":753},{\"x\":1914,\"y\":752},{\"x\":1915,\"y\":796},{\"x\":1888,\"y\":797}],\"text\":\"?\"}]}", "{\"language\":\"en\",\"text\":\"Time series- ------ -- - - - - - -- -- Input feature convolutional layer Multi-window Max-pooling Output layer layer\",\"lines\":[{\"boundingBox\":[{\"x\":46,\"y\":254},{\"x\":45,\"y\":535},{\"x\":0,\"y\":535},{\"x\":0,\"y\":254}],\"text\":\"Time series-\"},{\"boundingBox\":[{\"x\":828,\"y\":195},{\"x\":923,\"y\":148},{\"x\":926,\"y\":156},{\"x\":831,\"y\":202}],\"text\":\"------\"},{\"boundingBox\":[{\"x\":877,\"y\":416},{\"x\":907,\"y\":403},{\"x\":910,\"y\":408},{\"x\":879,\"y\":421}],\"text\":\"--\"},{\"boundingBox\":[{\"x\":908,\"y\":402},{\"x\":923,\"y\":396},{\"x\":925,\"y\":401},{\"x\":910,\"y\":407}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":827,\"y\":438},{\"x\":848,\"y\":429},{\"x\":850,\"y\":437},{\"x\":829,\"y\":446}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":847,\"y\":429},{\"x\":862,\"y\":424},{\"x\":864,\"y\":431},{\"x\":849,\"y\":436}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":861,\"y\":422},{\"x\":878,\"y\":417},{\"x\":880,\"y\":422},{\"x\":862,\"y\":428}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":1437,\"y\":405},{\"x\":1456,\"y\":396},{\"x\":1458,\"y\":407},{\"x\":1439,\"y\":415}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":1392,\"y\":433},{\"x\":1430,\"y\":413},{\"x\":1435,\"y\":423},{\"x\":1395,\"y\":443}],\"text\":\"-- --\"},{\"boundingBox\":[{\"x\":194,\"y\":780},{\"x\":498,\"y\":779},{\"x\":498,\"y\":830},{\"x\":194,\"y\":831}],\"text\":\"Input feature\"},{\"boundingBox\":[{\"x\":1004,\"y\":805},{\"x\":1328,\"y\":802},{\"x\":1328,\"y\":848},{\"x\":1005,\"y\":851}],\"text\":\"convolutional\"},{\"boundingBox\":[{\"x\":1106,\"y\":874},{\"x\":1231,\"y\":874},{\"x\":1231,\"y\":922},{\"x\":1106,\"y\":921}],\"text\":\"layer\"},{\"boundingBox\":[{\"x\":997,\"y\":732},{\"x\":1997,\"y\":752},{\"x\":1996,\"y\":812},{\"x\":996,\"y\":782}],\"text\":\"Multi-window Max-pooling Output layer\"},{\"boundingBox\":[{\"x\":1445,\"y\":808},{\"x\":1567,\"y\":808},{\"x\":1565,\"y\":855},{\"x\":1446,\"y\":854}],\"text\":\"layer\"}],\"words\":[{\"boundingBox\":[{\"x\":46,\"y\":263},{\"x\":47,\"y\":382},{\"x\":1,\"y\":382},{\"x\":1,\"y\":264}],\"text\":\"Time\"},{\"boundingBox\":[{\"x\":47,\"y\":391},{\"x\":43,\"y\":533},{\"x\":2,\"y\":534},{\"x\":1,\"y\":391}],\"text\":\"series-\"},{\"boundingBox\":[{\"x\":840,\"y\":190},{\"x\":923,\"y\":149},{\"x\":925,\"y\":157},{\"x\":842,\"y\":197}],\"text\":\"------\"},{\"boundingBox\":[{\"x\":887,\"y\":412},{\"x\":908,\"y\":404},{\"x\":910,\"y\":409},{\"x\":889,\"y\":418}],\"text\":\"--\"},{\"boundingBox\":[{\"x\":920,\"y\":398},{\"x\":924,\"y\":396},{\"x\":925,\"y\":401},{\"x\":922,\"y\":403}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":830,\"y\":436},{\"x\":838,\"y\":433},{\"x\":841,\"y\":440},{\"x\":833,\"y\":444}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":856,\"y\":425},{\"x\":861,\"y\":424},{\"x\":864,\"y\":431},{\"x\":858,\"y\":433}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":863,\"y\":422},{\"x\":868,\"y\":420},{\"x\":870,\"y\":426},{\"x\":864,\"y\":428}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":1448,\"y\":399},{\"x\":1455,\"y\":396},{\"x\":1459,\"y\":406},{\"x\":1453,\"y\":409}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":1392,\"y\":434},{\"x\":1413,\"y\":422},{\"x\":1416,\"y\":432},{\"x\":1395,\"y\":443}],\"text\":\"--\"},{\"boundingBox\":[{\"x\":1420,\"y\":419},{\"x\":1431,\"y\":414},{\"x\":1434,\"y\":424},{\"x\":1423,\"y\":429}],\"text\":\"--\"},{\"boundingBox\":[{\"x\":195,\"y\":781},{\"x\":320,\"y\":780},{\"x\":320,\"y\":832},{\"x\":195,\"y\":832}],\"text\":\"Input\"},{\"boundingBox\":[{\"x\":330,\"y\":780},{\"x\":498,\"y\":780},{\"x\":499,\"y\":831},{\"x\":330,\"y\":832}],\"text\":\"feature\"},{\"boundingBox\":[{\"x\":1005,\"y\":808},{\"x\":1327,\"y\":802},{\"x\":1326,\"y\":850},{\"x\":1006,\"y\":851}],\"text\":\"convolutional\"},{\"boundingBox\":[{\"x\":1106,\"y\":874},{\"x\":1231,\"y\":874},{\"x\":1231,\"y\":922},{\"x\":1106,\"y\":921}],\"text\":\"layer\"},{\"boundingBox\":[{\"x\":997,\"y\":733},{\"x\":1330,\"y\":734},{\"x\":1330,\"y\":787},{\"x\":997,\"y\":780}],\"text\":\"Multi-window\"},{\"boundingBox\":[{\"x\":1351,\"y\":734},{\"x\":1669,\"y\":743},{\"x\":1669,\"y\":798},{\"x\":1351,\"y\":787}],\"text\":\"Max-pooling\"},{\"boundingBox\":[{\"x\":1702,\"y\":744},{\"x\":1865,\"y\":752},{\"x\":1865,\"y\":807},{\"x\":1702,\"y\":800}],\"text\":\"Output\"},{\"boundingBox\":[{\"x\":1874,\"y\":752},{\"x\":1995,\"y\":759},{\"x\":1995,\"y\":813},{\"x\":1874,\"y\":807}],\"text\":\"layer\"},{\"boundingBox\":[{\"x\":1445,\"y\":808},{\"x\":1564,\"y\":808},{\"x\":1564,\"y\":855},{\"x\":1445,\"y\":854}],\"text\":\"layer\"}]}", "{\"language\":\"en\",\"text\":\"The existing Users Latent location- historical factor based location data model marketing resources Time series Features of User feature resources Training process preferences model Input Output CNN A new location-based Features of marketing resources new resources process Language CNN Results Recommending model User preferences\",\"lines\":[{\"boundingBox\":[{\"x\":314,\"y\":43},{\"x\":621,\"y\":46},{\"x\":620,\"y\":105},{\"x\":314,\"y\":100}],\"text\":\"The existing\"},{\"boundingBox\":[{\"x\":1160,\"y\":80},{\"x\":1305,\"y\":82},{\"x\":1305,\"y\":132},{\"x\":1162,\"y\":131}],\"text\":\"Users\"},{\"boundingBox\":[{\"x\":1617,\"y\":81},{\"x\":1779,\"y\":81},{\"x\":1779,\"y\":130},{\"x\":1616,\"y\":129}],\"text\":\"Latent\"},{\"boundingBox\":[{\"x\":357,\"y\":115},{\"x\":573,\"y\":117},{\"x\":572,\"y\":170},{\"x\":357,\"y\":168}],\"text\":\"location-\"},{\"boundingBox\":[{\"x\":1142,\"y\":141},{\"x\":1373,\"y\":142},{\"x\":1372,\"y\":194},{\"x\":1142,\"y\":194}],\"text\":\"historical\"},{\"boundingBox\":[{\"x\":1630,\"y\":139},{\"x\":1771,\"y\":142},{\"x\":1768,\"y\":191},{\"x\":1630,\"y\":191}],\"text\":\"factor\"},{\"boundingBox\":[{\"x\":394,\"y\":188},{\"x\":540,\"y\":187},{\"x\":539,\"y\":240},{\"x\":395,\"y\":242}],\"text\":\"based\"},{\"boundingBox\":[{\"x\":1098,\"y\":203},{\"x\":1420,\"y\":204},{\"x\":1420,\"y\":256},{\"x\":1097,\"y\":254}],\"text\":\"location data\"},{\"boundingBox\":[{\"x\":1620,\"y\":206},{\"x\":1777,\"y\":202},{\"x\":1778,\"y\":249},{\"x\":1621,\"y\":253}],\"text\":\"model\"},{\"boundingBox\":[{\"x\":339,\"y\":263},{\"x\":595,\"y\":263},{\"x\":595,\"y\":322},{\"x\":339,\"y\":321}],\"text\":\"marketing\"},{\"boundingBox\":[{\"x\":348,\"y\":344},{\"x\":581,\"y\":342},{\"x\":581,\"y\":385},{\"x\":348,\"y\":387}],\"text\":\"resources\"},{\"boundingBox\":[{\"x\":571,\"y\":387},{\"x\":857,\"y\":387},{\"x\":857,\"y\":441},{\"x\":571,\"y\":441}],\"text\":\"Time series\"},{\"boundingBox\":[{\"x\":1228,\"y\":390},{\"x\":1513,\"y\":388},{\"x\":1513,\"y\":440},{\"x\":1228,\"y\":441}],\"text\":\"Features of\"},{\"boundingBox\":[{\"x\":1675,\"y\":392},{\"x\":1792,\"y\":391},{\"x\":1790,\"y\":440},{\"x\":1673,\"y\":440}],\"text\":\"User\"},{\"boundingBox\":[{\"x\":631,\"y\":451},{\"x\":798,\"y\":451},{\"x\":799,\"y\":500},{\"x\":631,\"y\":500}],\"text\":\"feature\"},{\"boundingBox\":[{\"x\":1250,\"y\":470},{\"x\":1483,\"y\":469},{\"x\":1483,\"y\":511},{\"x\":1250,\"y\":512}],\"text\":\"resources\"},{\"boundingBox\":[{\"x\":52,\"y\":718},{\"x\":59,\"y\":304},{\"x\":116,\"y\":305},{\"x\":111,\"y\":719}],\"text\":\"Training process\"},{\"boundingBox\":[{\"x\":1587,\"y\":464},{\"x\":1877,\"y\":463},{\"x\":1878,\"y\":519},{\"x\":1587,\"y\":522}],\"text\":\"preferences\"},{\"boundingBox\":[{\"x\":636,\"y\":514},{\"x\":797,\"y\":511},{\"x\":798,\"y\":559},{\"x\":636,\"y\":561}],\"text\":\"model\"},{\"boundingBox\":[{\"x\":528,\"y\":880},{\"x\":659,\"y\":880},{\"x\":659,\"y\":936},{\"x\":530,\"y\":937}],\"text\":\"Input\"},{\"boundingBox\":[{\"x\":1396,\"y\":879},{\"x\":1573,\"y\":880},{\"x\":1573,\"y\":938},{\"x\":1396,\"y\":937}],\"text\":\"Output\"},{\"boundingBox\":[{\"x\":950,\"y\":1116},{\"x\":1084,\"y\":1116},{\"x\":1085,\"y\":1162},{\"x\":950,\"y\":1162}],\"text\":\"CNN\"},{\"boundingBox\":[{\"x\":208,\"y\":1277},{\"x\":738,\"y\":1276},{\"x\":738,\"y\":1330},{\"x\":208,\"y\":1331}],\"text\":\"A new location-based\"},{\"boundingBox\":[{\"x\":1351,\"y\":1271},{\"x\":1633,\"y\":1269},{\"x\":1634,\"y\":1316},{\"x\":1351,\"y\":1318}],\"text\":\"Features of\"},{\"boundingBox\":[{\"x\":221,\"y\":1353},{\"x\":715,\"y\":1354},{\"x\":715,\"y\":1407},{\"x\":221,\"y\":1406}],\"text\":\"marketing resources\"},{\"boundingBox\":[{\"x\":1436,\"y\":1346},{\"x\":1542,\"y\":1344},{\"x\":1542,\"y\":1380},{\"x\":1437,\"y\":1382}],\"text\":\"new\"},{\"boundingBox\":[{\"x\":1375,\"y\":1409},{\"x\":1606,\"y\":1408},{\"x\":1606,\"y\":1450},{\"x\":1375,\"y\":1450}],\"text\":\"resources\"},{\"boundingBox\":[{\"x\":82,\"y\":1544},{\"x\":80,\"y\":1350},{\"x\":131,\"y\":1350},{\"x\":133,\"y\":1543}],\"text\":\"process\"},{\"boundingBox\":[{\"x\":652,\"y\":1460},{\"x\":896,\"y\":1463},{\"x\":895,\"y\":1520},{\"x\":652,\"y\":1518}],\"text\":\"Language\"},{\"boundingBox\":[{\"x\":1034,\"y\":1497},{\"x\":1165,\"y\":1497},{\"x\":1164,\"y\":1546},{\"x\":1035,\"y\":1548}],\"text\":\"CNN\"},{\"boundingBox\":[{\"x\":1764,\"y\":1496},{\"x\":1953,\"y\":1492},{\"x\":1954,\"y\":1543},{\"x\":1765,\"y\":1547}],\"text\":\"Results\"},{\"boundingBox\":[{\"x\":3,\"y\":1742},{\"x\":8,\"y\":1348},{\"x\":66,\"y\":1349},{\"x\":58,\"y\":1744}],\"text\":\"Recommending\"},{\"boundingBox\":[{\"x\":694,\"y\":1534},{\"x\":849,\"y\":1530},{\"x\":850,\"y\":1583},{\"x\":694,\"y\":1585}],\"text\":\"model\"},{\"boundingBox\":[{\"x\":1431,\"y\":1621},{\"x\":1553,\"y\":1623},{\"x\":1552,\"y\":1673},{\"x\":1431,\"y\":1673}],\"text\":\"User\"},{\"boundingBox\":[{\"x\":1347,\"y\":1690},{\"x\":1636,\"y\":1689},{\"x\":1636,\"y\":1745},{\"x\":1348,\"y\":1747}],\"text\":\"preferences\"}],\"words\":[{\"boundingBox\":[{\"x\":315,\"y\":44},{\"x\":407,\"y\":44},{\"x\":409,\"y\":102},{\"x\":318,\"y\":100}],\"text\":\"The\"},{\"boundingBox\":[{\"x\":418,\"y\":44},{\"x\":621,\"y\":46},{\"x\":621,\"y\":106},{\"x\":420,\"y\":102}],\"text\":\"existing\"},{\"boundingBox\":[{\"x\":1160,\"y\":80},{\"x\":1303,\"y\":81},{\"x\":1302,\"y\":132},{\"x\":1160,\"y\":130}],\"text\":\"Users\"},{\"boundingBox\":[{\"x\":1617,\"y\":82},{\"x\":1778,\"y\":83},{\"x\":1776,\"y\":131},{\"x\":1617,\"y\":130}],\"text\":\"Latent\"},{\"boundingBox\":[{\"x\":357,\"y\":115},{\"x\":571,\"y\":120},{\"x\":571,\"y\":169},{\"x\":358,\"y\":170}],\"text\":\"location-\"},{\"boundingBox\":[{\"x\":1143,\"y\":142},{\"x\":1371,\"y\":144},{\"x\":1371,\"y\":195},{\"x\":1142,\"y\":195}],\"text\":\"historical\"},{\"boundingBox\":[{\"x\":1630,\"y\":139},{\"x\":1769,\"y\":140},{\"x\":1768,\"y\":192},{\"x\":1630,\"y\":190}],\"text\":\"factor\"},{\"boundingBox\":[{\"x\":394,\"y\":188},{\"x\":537,\"y\":187},{\"x\":538,\"y\":241},{\"x\":394,\"y\":242}],\"text\":\"based\"},{\"boundingBox\":[{\"x\":1098,\"y\":204},{\"x\":1299,\"y\":206},{\"x\":1298,\"y\":256},{\"x\":1098,\"y\":254}],\"text\":\"location\"},{\"boundingBox\":[{\"x\":1309,\"y\":206},{\"x\":1420,\"y\":206},{\"x\":1419,\"y\":254},{\"x\":1308,\"y\":256}],\"text\":\"data\"},{\"boundingBox\":[{\"x\":1621,\"y\":208},{\"x\":1775,\"y\":202},{\"x\":1775,\"y\":250},{\"x\":1621,\"y\":253}],\"text\":\"model\"},{\"boundingBox\":[{\"x\":339,\"y\":267},{\"x\":596,\"y\":263},{\"x\":594,\"y\":323},{\"x\":340,\"y\":317}],\"text\":\"marketing\"},{\"boundingBox\":[{\"x\":349,\"y\":345},{\"x\":582,\"y\":343},{\"x\":582,\"y\":384},{\"x\":350,\"y\":386}],\"text\":\"resources\"},{\"boundingBox\":[{\"x\":572,\"y\":388},{\"x\":694,\"y\":388},{\"x\":695,\"y\":442},{\"x\":575,\"y\":442}],\"text\":\"Time\"},{\"boundingBox\":[{\"x\":705,\"y\":388},{\"x\":856,\"y\":390},{\"x\":855,\"y\":440},{\"x\":706,\"y\":442}],\"text\":\"series\"},{\"boundingBox\":[{\"x\":1230,\"y\":392},{\"x\":1436,\"y\":390},{\"x\":1436,\"y\":441},{\"x\":1228,\"y\":441}],\"text\":\"Features\"},{\"boundingBox\":[{\"x\":1446,\"y\":390},{\"x\":1513,\"y\":389},{\"x\":1514,\"y\":441},{\"x\":1446,\"y\":441}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":1673,\"y\":391},{\"x\":1790,\"y\":391},{\"x\":1791,\"y\":439},{\"x\":1673,\"y\":440}],\"text\":\"User\"},{\"boundingBox\":[{\"x\":632,\"y\":452},{\"x\":797,\"y\":452},{\"x\":795,\"y\":500},{\"x\":632,\"y\":501}],\"text\":\"feature\"},{\"boundingBox\":[{\"x\":1251,\"y\":472},{\"x\":1483,\"y\":470},{\"x\":1483,\"y\":512},{\"x\":1250,\"y\":512}],\"text\":\"resources\"},{\"boundingBox\":[{\"x\":52,\"y\":714},{\"x\":61,\"y\":511},{\"x\":116,\"y\":512},{\"x\":112,\"y\":715}],\"text\":\"Training\"},{\"boundingBox\":[{\"x\":61,\"y\":499},{\"x\":62,\"y\":306},{\"x\":116,\"y\":308},{\"x\":116,\"y\":500}],\"text\":\"process\"},{\"boundingBox\":[{\"x\":1587,\"y\":465},{\"x\":1877,\"y\":465},{\"x\":1876,\"y\":519},{\"x\":1588,\"y\":523}],\"text\":\"preferences\"},{\"boundingBox\":[{\"x\":636,\"y\":517},{\"x\":796,\"y\":512},{\"x\":797,\"y\":560},{\"x\":637,\"y\":561}],\"text\":\"model\"},{\"boundingBox\":[{\"x\":528,\"y\":880},{\"x\":657,\"y\":880},{\"x\":657,\"y\":936},{\"x\":528,\"y\":937}],\"text\":\"Input\"},{\"boundingBox\":[{\"x\":1397,\"y\":880},{\"x\":1571,\"y\":882},{\"x\":1569,\"y\":939},{\"x\":1398,\"y\":938}],\"text\":\"Output\"},{\"boundingBox\":[{\"x\":950,\"y\":1116},{\"x\":1079,\"y\":1116},{\"x\":1079,\"y\":1162},{\"x\":950,\"y\":1162}],\"text\":\"CNN\"},{\"boundingBox\":[{\"x\":208,\"y\":1278},{\"x\":243,\"y\":1278},{\"x\":244,\"y\":1332},{\"x\":209,\"y\":1332}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":254,\"y\":1278},{\"x\":363,\"y\":1278},{\"x\":364,\"y\":1332},{\"x\":255,\"y\":1332}],\"text\":\"new\"},{\"boundingBox\":[{\"x\":374,\"y\":1278},{\"x\":737,\"y\":1277},{\"x\":737,\"y\":1332},{\"x\":374,\"y\":1332}],\"text\":\"location-based\"},{\"boundingBox\":[{\"x\":1353,\"y\":1271},{\"x\":1564,\"y\":1270},{\"x\":1562,\"y\":1317},{\"x\":1352,\"y\":1318}],\"text\":\"Features\"},{\"boundingBox\":[{\"x\":1573,\"y\":1270},{\"x\":1634,\"y\":1269},{\"x\":1632,\"y\":1317},{\"x\":1571,\"y\":1317}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":222,\"y\":1354},{\"x\":466,\"y\":1354},{\"x\":467,\"y\":1407},{\"x\":223,\"y\":1405}],\"text\":\"marketing\"},{\"boundingBox\":[{\"x\":476,\"y\":1354},{\"x\":714,\"y\":1357},{\"x\":714,\"y\":1405},{\"x\":477,\"y\":1407}],\"text\":\"resources\"},{\"boundingBox\":[{\"x\":1437,\"y\":1347},{\"x\":1534,\"y\":1345},{\"x\":1533,\"y\":1382},{\"x\":1438,\"y\":1382}],\"text\":\"new\"},{\"boundingBox\":[{\"x\":1376,\"y\":1411},{\"x\":1606,\"y\":1409},{\"x\":1605,\"y\":1451},{\"x\":1377,\"y\":1450}],\"text\":\"resources\"},{\"boundingBox\":[{\"x\":82,\"y\":1545},{\"x\":81,\"y\":1350},{\"x\":131,\"y\":1351},{\"x\":134,\"y\":1544}],\"text\":\"process\"},{\"boundingBox\":[{\"x\":652,\"y\":1461},{\"x\":895,\"y\":1465},{\"x\":894,\"y\":1518},{\"x\":652,\"y\":1516}],\"text\":\"Language\"},{\"boundingBox\":[{\"x\":1034,\"y\":1497},{\"x\":1163,\"y\":1497},{\"x\":1163,\"y\":1547},{\"x\":1034,\"y\":1548}],\"text\":\"CNN\"},{\"boundingBox\":[{\"x\":1765,\"y\":1497},{\"x\":1954,\"y\":1493},{\"x\":1954,\"y\":1545},{\"x\":1766,\"y\":1548}],\"text\":\"Results\"},{\"boundingBox\":[{\"x\":4,\"y\":1743},{\"x\":9,\"y\":1349},{\"x\":67,\"y\":1351},{\"x\":56,\"y\":1744}],\"text\":\"Recommending\"},{\"boundingBox\":[{\"x\":695,\"y\":1536},{\"x\":848,\"y\":1531},{\"x\":847,\"y\":1585},{\"x\":695,\"y\":1585}],\"text\":\"model\"},{\"boundingBox\":[{\"x\":1431,\"y\":1621},{\"x\":1552,\"y\":1622},{\"x\":1552,\"y\":1674},{\"x\":1431,\"y\":1673}],\"text\":\"User\"},{\"boundingBox\":[{\"x\":1348,\"y\":1691},{\"x\":1634,\"y\":1692},{\"x\":1631,\"y\":1744},{\"x\":1348,\"y\":1748}],\"text\":\"preferences\"}]}", "{\"language\":\"en\",\"text\":\"User behavior data Pageviews > No Retain data 30,000 Yes Purchase >3 Yes- Retain data 0 No Eliminate data\",\"lines\":[{\"boundingBox\":[{\"x\":74,\"y\":44},{\"x\":727,\"y\":44},{\"x\":727,\"y\":116},{\"x\":74,\"y\":116}],\"text\":\"User behavior data\"},{\"boundingBox\":[{\"x\":169,\"y\":328},{\"x\":620,\"y\":328},{\"x\":621,\"y\":400},{\"x\":169,\"y\":403}],\"text\":\"Pageviews >\"},{\"boundingBox\":[{\"x\":865,\"y\":378},{\"x\":980,\"y\":379},{\"x\":979,\"y\":444},{\"x\":865,\"y\":441}],\"text\":\"No\"},{\"boundingBox\":[{\"x\":1151,\"y\":377},{\"x\":1548,\"y\":376},{\"x\":1549,\"y\":447},{\"x\":1151,\"y\":448}],\"text\":\"Retain data\"},{\"boundingBox\":[{\"x\":280,\"y\":431},{\"x\":521,\"y\":429},{\"x\":522,\"y\":503},{\"x\":280,\"y\":504}],\"text\":\"30,000\"},{\"boundingBox\":[{\"x\":337,\"y\":632},{\"x\":469,\"y\":637},{\"x\":467,\"y\":699},{\"x\":334,\"y\":698}],\"text\":\"Yes\"},{\"boundingBox\":[{\"x\":180,\"y\":885},{\"x\":622,\"y\":884},{\"x\":623,\"y\":956},{\"x\":180,\"y\":958}],\"text\":\"Purchase >3\"},{\"boundingBox\":[{\"x\":850,\"y\":909},{\"x\":997,\"y\":917},{\"x\":994,\"y\":979},{\"x\":847,\"y\":977}],\"text\":\"Yes-\"},{\"boundingBox\":[{\"x\":1151,\"y\":911},{\"x\":1548,\"y\":910},{\"x\":1549,\"y\":980},{\"x\":1151,\"y\":982}],\"text\":\"Retain data\"},{\"boundingBox\":[{\"x\":373,\"y\":1000},{\"x\":419,\"y\":1002},{\"x\":419,\"y\":1051},{\"x\":372,\"y\":1048}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":348,\"y\":1144},{\"x\":455,\"y\":1149},{\"x\":451,\"y\":1213},{\"x\":344,\"y\":1207}],\"text\":\"No\"},{\"boundingBox\":[{\"x\":153,\"y\":1308},{\"x\":656,\"y\":1308},{\"x\":656,\"y\":1375},{\"x\":153,\"y\":1375}],\"text\":\"Eliminate data\"}],\"words\":[{\"boundingBox\":[{\"x\":75,\"y\":45},{\"x\":235,\"y\":44},{\"x\":235,\"y\":117},{\"x\":75,\"y\":117}],\"text\":\"User\"},{\"boundingBox\":[{\"x\":249,\"y\":44},{\"x\":555,\"y\":44},{\"x\":555,\"y\":117},{\"x\":249,\"y\":117}],\"text\":\"behavior\"},{\"boundingBox\":[{\"x\":569,\"y\":44},{\"x\":724,\"y\":45},{\"x\":724,\"y\":116},{\"x\":569,\"y\":117}],\"text\":\"data\"},{\"boundingBox\":[{\"x\":169,\"y\":329},{\"x\":532,\"y\":330},{\"x\":532,\"y\":398},{\"x\":170,\"y\":400}],\"text\":\"Pageviews\"},{\"boundingBox\":[{\"x\":546,\"y\":330},{\"x\":617,\"y\":331},{\"x\":616,\"y\":392},{\"x\":546,\"y\":397}],\"text\":\">\"},{\"boundingBox\":[{\"x\":865,\"y\":378},{\"x\":978,\"y\":379},{\"x\":977,\"y\":444},{\"x\":865,\"y\":442}],\"text\":\"No\"},{\"boundingBox\":[{\"x\":1152,\"y\":377},{\"x\":1376,\"y\":377},{\"x\":1376,\"y\":448},{\"x\":1152,\"y\":449}],\"text\":\"Retain\"},{\"boundingBox\":[{\"x\":1391,\"y\":377},{\"x\":1548,\"y\":376},{\"x\":1548,\"y\":449},{\"x\":1390,\"y\":448}],\"text\":\"data\"},{\"boundingBox\":[{\"x\":281,\"y\":431},{\"x\":520,\"y\":429},{\"x\":520,\"y\":504},{\"x\":281,\"y\":505}],\"text\":\"30,000\"},{\"boundingBox\":[{\"x\":344,\"y\":632},{\"x\":467,\"y\":635},{\"x\":465,\"y\":700},{\"x\":342,\"y\":698}],\"text\":\"Yes\"},{\"boundingBox\":[{\"x\":181,\"y\":888},{\"x\":482,\"y\":886},{\"x\":482,\"y\":958},{\"x\":181,\"y\":956}],\"text\":\"Purchase\"},{\"boundingBox\":[{\"x\":496,\"y\":886},{\"x\":622,\"y\":884},{\"x\":621,\"y\":957},{\"x\":495,\"y\":958}],\"text\":\">3\"},{\"boundingBox\":[{\"x\":854,\"y\":909},{\"x\":994,\"y\":914},{\"x\":992,\"y\":980},{\"x\":851,\"y\":977}],\"text\":\"Yes-\"},{\"boundingBox\":[{\"x\":1152,\"y\":912},{\"x\":1375,\"y\":910},{\"x\":1374,\"y\":982},{\"x\":1152,\"y\":983}],\"text\":\"Retain\"},{\"boundingBox\":[{\"x\":1389,\"y\":910},{\"x\":1546,\"y\":910},{\"x\":1545,\"y\":982},{\"x\":1389,\"y\":982}],\"text\":\"data\"},{\"boundingBox\":[{\"x\":376,\"y\":1000},{\"x\":419,\"y\":1002},{\"x\":416,\"y\":1051},{\"x\":374,\"y\":1049}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":347,\"y\":1144},{\"x\":454,\"y\":1149},{\"x\":450,\"y\":1213},{\"x\":344,\"y\":1207}],\"text\":\"No\"},{\"boundingBox\":[{\"x\":154,\"y\":1311},{\"x\":485,\"y\":1308},{\"x\":483,\"y\":1376},{\"x\":153,\"y\":1375}],\"text\":\"Eliminate\"},{\"boundingBox\":[{\"x\":498,\"y\":1308},{\"x\":654,\"y\":1309},{\"x\":653,\"y\":1376},{\"x\":496,\"y\":1376}],\"text\":\"data\"}]}", "{\"language\":\"en\",\"text\":\"Validation accuracy -Testing accuary Training accuracy 98 96 94 92 Accuracy rate/% 06 88 86 84 82 1 2 3 4 5 6 7 Number of iterations\",\"lines\":[{\"boundingBox\":[{\"x\":179,\"y\":0},{\"x\":492,\"y\":6},{\"x\":491,\"y\":36},{\"x\":179,\"y\":30}],\"text\":\"Validation accuracy\"},{\"boundingBox\":[{\"x\":495,\"y\":1},{\"x\":824,\"y\":5},{\"x\":824,\"y\":37},{\"x\":495,\"y\":33}],\"text\":\"-Testing accuary\"},{\"boundingBox\":[{\"x\":186,\"y\":56},{\"x\":458,\"y\":59},{\"x\":457,\"y\":91},{\"x\":186,\"y\":88}],\"text\":\"Training accuracy\"},{\"boundingBox\":[{\"x\":46,\"y\":122},{\"x\":80,\"y\":122},{\"x\":80,\"y\":149},{\"x\":46,\"y\":149}],\"text\":\"98\"},{\"boundingBox\":[{\"x\":46,\"y\":162},{\"x\":81,\"y\":162},{\"x\":81,\"y\":189},{\"x\":46,\"y\":189}],\"text\":\"96\"},{\"boundingBox\":[{\"x\":46,\"y\":204},{\"x\":80,\"y\":204},{\"x\":80,\"y\":230},{\"x\":45,\"y\":230}],\"text\":\"94\"},{\"boundingBox\":[{\"x\":47,\"y\":243},{\"x\":81,\"y\":245},{\"x\":80,\"y\":271},{\"x\":46,\"y\":269}],\"text\":\"92\"},{\"boundingBox\":[{\"x\":3,\"y\":426},{\"x\":3,\"y\":171},{\"x\":34,\"y\":171},{\"x\":35,\"y\":426}],\"text\":\"Accuracy rate/%\"},{\"boundingBox\":[{\"x\":76,\"y\":312},{\"x\":44,\"y\":312},{\"x\":45,\"y\":283},{\"x\":77,\"y\":284}],\"text\":\"06\"},{\"boundingBox\":[{\"x\":48,\"y\":323},{\"x\":80,\"y\":326},{\"x\":79,\"y\":353},{\"x\":46,\"y\":350}],\"text\":\"88\"},{\"boundingBox\":[{\"x\":47,\"y\":366},{\"x\":82,\"y\":368},{\"x\":80,\"y\":395},{\"x\":45,\"y\":392}],\"text\":\"86\"},{\"boundingBox\":[{\"x\":47,\"y\":409},{\"x\":80,\"y\":410},{\"x\":78,\"y\":434},{\"x\":45,\"y\":432}],\"text\":\"84\"},{\"boundingBox\":[{\"x\":48,\"y\":448},{\"x\":80,\"y\":447},{\"x\":81,\"y\":475},{\"x\":48,\"y\":475}],\"text\":\"82\"},{\"boundingBox\":[{\"x\":96,\"y\":496},{\"x\":113,\"y\":497},{\"x\":113,\"y\":514},{\"x\":96,\"y\":513}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":229,\"y\":493},{\"x\":248,\"y\":492},{\"x\":247,\"y\":515},{\"x\":229,\"y\":516}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":361,\"y\":495},{\"x\":378,\"y\":496},{\"x\":377,\"y\":515},{\"x\":359,\"y\":514}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":495,\"y\":492},{\"x\":515,\"y\":493},{\"x\":513,\"y\":515},{\"x\":494,\"y\":513}],\"text\":\"4\"},{\"boundingBox\":[{\"x\":626,\"y\":494},{\"x\":648,\"y\":493},{\"x\":647,\"y\":514},{\"x\":625,\"y\":515}],\"text\":\"5\"},{\"boundingBox\":[{\"x\":759,\"y\":495},{\"x\":779,\"y\":494},{\"x\":780,\"y\":511},{\"x\":759,\"y\":512}],\"text\":\"6\"},{\"boundingBox\":[{\"x\":893,\"y\":492},{\"x\":912,\"y\":494},{\"x\":911,\"y\":514},{\"x\":892,\"y\":512}],\"text\":\"7\"},{\"boundingBox\":[{\"x\":349,\"y\":538},{\"x\":664,\"y\":539},{\"x\":664,\"y\":569},{\"x\":349,\"y\":568}],\"text\":\"Number of iterations\"}],\"words\":[{\"boundingBox\":[{\"x\":191,\"y\":1},{\"x\":346,\"y\":4},{\"x\":346,\"y\":33},{\"x\":192,\"y\":31}],\"text\":\"Validation\"},{\"boundingBox\":[{\"x\":352,\"y\":4},{\"x\":493,\"y\":8},{\"x\":492,\"y\":36},{\"x\":352,\"y\":33}],\"text\":\"accuracy\"},{\"boundingBox\":[{\"x\":497,\"y\":1},{\"x\":697,\"y\":5},{\"x\":697,\"y\":36},{\"x\":498,\"y\":34}],\"text\":\"-Testing\"},{\"boundingBox\":[{\"x\":704,\"y\":5},{\"x\":824,\"y\":7},{\"x\":824,\"y\":37},{\"x\":704,\"y\":36}],\"text\":\"accuary\"},{\"boundingBox\":[{\"x\":189,\"y\":57},{\"x\":317,\"y\":57},{\"x\":317,\"y\":91},{\"x\":188,\"y\":89}],\"text\":\"Training\"},{\"boundingBox\":[{\"x\":323,\"y\":58},{\"x\":457,\"y\":62},{\"x\":458,\"y\":91},{\"x\":323,\"y\":91}],\"text\":\"accuracy\"},{\"boundingBox\":[{\"x\":46,\"y\":122},{\"x\":78,\"y\":122},{\"x\":78,\"y\":149},{\"x\":46,\"y\":149}],\"text\":\"98\"},{\"boundingBox\":[{\"x\":46,\"y\":162},{\"x\":80,\"y\":162},{\"x\":80,\"y\":189},{\"x\":46,\"y\":189}],\"text\":\"96\"},{\"boundingBox\":[{\"x\":45,\"y\":204},{\"x\":80,\"y\":204},{\"x\":80,\"y\":230},{\"x\":45,\"y\":230}],\"text\":\"94\"},{\"boundingBox\":[{\"x\":47,\"y\":243},{\"x\":80,\"y\":245},{\"x\":78,\"y\":271},{\"x\":46,\"y\":269}],\"text\":\"92\"},{\"boundingBox\":[{\"x\":4,\"y\":426},{\"x\":4,\"y\":283},{\"x\":35,\"y\":283},{\"x\":35,\"y\":426}],\"text\":\"Accuracy\"},{\"boundingBox\":[{\"x\":4,\"y\":277},{\"x\":4,\"y\":179},{\"x\":32,\"y\":179},{\"x\":35,\"y\":277}],\"text\":\"rate/%\"},{\"boundingBox\":[{\"x\":76,\"y\":312},{\"x\":44,\"y\":311},{\"x\":44,\"y\":283},{\"x\":76,\"y\":283}],\"text\":\"06\"},{\"boundingBox\":[{\"x\":50,\"y\":323},{\"x\":80,\"y\":326},{\"x\":78,\"y\":353},{\"x\":47,\"y\":350}],\"text\":\"88\"},{\"boundingBox\":[{\"x\":46,\"y\":366},{\"x\":82,\"y\":368},{\"x\":80,\"y\":395},{\"x\":45,\"y\":392}],\"text\":\"86\"},{\"boundingBox\":[{\"x\":48,\"y\":409},{\"x\":80,\"y\":410},{\"x\":78,\"y\":434},{\"x\":47,\"y\":432}],\"text\":\"84\"},{\"boundingBox\":[{\"x\":48,\"y\":447},{\"x\":79,\"y\":447},{\"x\":80,\"y\":475},{\"x\":48,\"y\":475}],\"text\":\"82\"},{\"boundingBox\":[{\"x\":99,\"y\":496},{\"x\":113,\"y\":497},{\"x\":112,\"y\":514},{\"x\":98,\"y\":513}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":229,\"y\":493},{\"x\":247,\"y\":492},{\"x\":248,\"y\":514},{\"x\":230,\"y\":515}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":361,\"y\":495},{\"x\":378,\"y\":496},{\"x\":377,\"y\":515},{\"x\":360,\"y\":514}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":501,\"y\":492},{\"x\":514,\"y\":493},{\"x\":512,\"y\":515},{\"x\":499,\"y\":514}],\"text\":\"4\"},{\"boundingBox\":[{\"x\":628,\"y\":494},{\"x\":648,\"y\":493},{\"x\":649,\"y\":513},{\"x\":629,\"y\":514}],\"text\":\"5\"},{\"boundingBox\":[{\"x\":766,\"y\":494},{\"x\":778,\"y\":494},{\"x\":779,\"y\":511},{\"x\":767,\"y\":512}],\"text\":\"6\"},{\"boundingBox\":[{\"x\":898,\"y\":492},{\"x\":913,\"y\":494},{\"x\":911,\"y\":514},{\"x\":896,\"y\":512}],\"text\":\"7\"},{\"boundingBox\":[{\"x\":351,\"y\":539},{\"x\":475,\"y\":539},{\"x\":474,\"y\":569},{\"x\":350,\"y\":569}],\"text\":\"Number\"},{\"boundingBox\":[{\"x\":481,\"y\":539},{\"x\":514,\"y\":539},{\"x\":514,\"y\":569},{\"x\":480,\"y\":569}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":520,\"y\":539},{\"x\":664,\"y\":540},{\"x\":664,\"y\":569},{\"x\":520,\"y\":569}],\"text\":\"iterations\"}]}", "{\"language\":\"en\",\"text\":\"Published online: 01 May 2019\",\"lines\":[{\"boundingBox\":[{\"x\":0,\"y\":16},{\"x\":894,\"y\":16},{\"x\":894,\"y\":72},{\"x\":0,\"y\":70}],\"text\":\"Published online: 01 May 2019\"}],\"words\":[{\"boundingBox\":[{\"x\":1,\"y\":18},{\"x\":282,\"y\":17},{\"x\":282,\"y\":71},{\"x\":0,\"y\":68}],\"text\":\"Published\"},{\"boundingBox\":[{\"x\":292,\"y\":17},{\"x\":499,\"y\":16},{\"x\":499,\"y\":73},{\"x\":292,\"y\":71}],\"text\":\"online:\"},{\"boundingBox\":[{\"x\":509,\"y\":16},{\"x\":587,\"y\":16},{\"x\":588,\"y\":73},{\"x\":509,\"y\":73}],\"text\":\"01\"},{\"boundingBox\":[{\"x\":597,\"y\":16},{\"x\":738,\"y\":16},{\"x\":739,\"y\":73},{\"x\":598,\"y\":73}],\"text\":\"May\"},{\"boundingBox\":[{\"x\":748,\"y\":16},{\"x\":892,\"y\":17},{\"x\":893,\"y\":73},{\"x\":749,\"y\":73}],\"text\":\"2019\"}]}" ] }, { "@search.score": 3.3773332, "content": "\nImproving prediction with enhanced \nDistributed Memory‑based Resilient Dataset \nFilter\nSandhya Narayanan1*, Philip Samuel2 and Mariamma Chacko3\n\nIntroduction\nAnalyzing and processing massive volumes of data in different applications like sensor \ndata, health care and e-Commerce require big data processing technologies. Extracting \nuseful information from the enormous size of unstructured data is a crucial thing. As the \namount of data becomes more extensive, sophisticated pre-processing techniques are \nrequired to analyze the data. In social networking sites and other online shopping sites, \na massive volume of online product reviews from a large size of customers are available \n[1]. The impact of online product reviews affects 90% of the current e-Commerce mar-\nket [2]. Customer reviews contribute the product sale to an extent and product life in the \nmarket depends on online product recommendations.\n\nOnline feedback is one of the communication methods which gives direct suggestions \nfrom the customers [3, 4]. Online reviews and ratings from customers are another infor-\nmation source about product quality [5, 6]. Customer reviews can help to decide on a new \nsuccessful product launch. Online shopping has several advantages over retail shopping. In \nretail shopping, the customers visit the shop and receive price information but less product \n\nAbstract \n\nLaunching new products in the consumer electronics market is challenging. Develop-\ning and marketing the same in limited time affect the sustainability of such companies. \nThis research work introduces a model that can predict the success of a product. A \nFeature Information Gain (FIG) measure is used for significant feature identification \nand Distributed Memory-based Resilient Dataset Filter (DMRDF) is used to eliminate \nduplicate reviews, which in turn improves the reliability of the product reviews. The \npre-processed dataset is used for prediction of product pre-launch in the market using \nclassifiers such as Logistic regression and Support vector machine. DMRDF method is \nfault-tolerant because of its resilience property and also reduces the dataset redun-\ndancy; hence, it increases the prediction accuracy of the model. The proposed model \nworks in a distributed environment to handle a massive volume of the dataset and \ntherefore, it is scalable. The output of this feature modelling and prediction allows the \nmanufacturer to optimize the design of his new product.\n\nKeywords: Distributed Memory-based, Resilient Distribution Dataset, Redundancy\n\nOpen Access\n\n© The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and \nthe source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material \nin this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material \nis not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the \npermitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creat iveco \nmmons .org/licen ses/by/4.0/.\n\nRESEARCH\n\nNarayanan et al. J Big Data (2020) 7:13 \nhttps://doi.org/10.1186/s40537‑020‑00292‑y\n\n*Correspondence: \nnairsands@gmail.com \n1 Information Technology, \nSchool of Engineering, \nCochin University of Science \n& Technology, Kochi 682022, \nIndia\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-020-00292-y&domain=pdf\n\n\nPage 2 of 15Narayanan et al. J Big Data (2020) 7:13 \n\ninformation from shop owners. On the other hand, online shopping sites give product \nreviews and previous customer feedbacks without extra cost and effort for the customers \n[7–10].\n\nInvesting in poor quality products potentially affects an industry’s brand loyalty and this \nstrategy should be changed by the eCommerce firms [5, 11]. Consumer product success \ndepends on different criteria, such as the quality of the product and marketing strategies. \nThe users should provide their valuable and accurate reviews about the products [12]. Cus-\ntomers bother to give reviews about products, whether they liked it or not. If the users \nprovide reviews, then other retailers can create some duplicated reviews [13, 14]. In online \nmarketing, the volume and value of product reviews are examined [15, 16]. The number \nof the product reviews on the shopping sites, blogs and forums has increased awareness \namong the users. This large volume of the reviews leads to the need for significant data \nprocessing methods [17, 18]. The value is the rating on the products. The ratio of positive to \nnegative reviews about the product leads to the quality of the product [19, 20].\n\nFeature selection is a crucial phase in data pre-processing [21]. Selecting features from \nan un-structured massive volume of data reduce the model complexity and improves the \nprediction accuracy. Different feature selection methods existing are the filter, wrapper and \nembedded. The wrapper feature selection method evaluates the usefulness of the feature \nand it depends on the performance of the classifier [22]. The filter method calculates the \nrelevance of the features and analyzes data in a univariate manner. The embedded process \nis similar to the wrapper method. Embedded and wrapper methods are more expensive \ncompared to the filter method. The state-of-art methods in customer review analysis gener-\nally discuss on categorizing positive and negative reviews using different natural language \nprocessing techniques and spam reviews recognition [23]. Feature selection of customer \nreviews increases prediction accuracy, thereby improves the model performance.\n\nAn enhanced method, which is a combination of filter and wrapper method is proposed \nin this work, which focuses on product pre-launch prediction with enhanced distributive \nfeature selection method. Since many redundant reviews are available on the web in large \nvolumes, a big data processing model has been implemented to filter out duplicated and \nunreliable data from customer reviews in-order to increase prediction accuracy. A scalable \nbig data processing model has been applied to predict the success or failure of a new prod-\nuct. The realization of the model has been done by Distributed Memory-based Resilient \nDataset Filter with prediction classifiers.\n\nThis paper is organized as follows. “Related work” section discusses related work. “Meth-\nodology” section contains the proposed methodology with System design, Resilient Distrib-\nuted Dataset and Prediction using classifiers. “Results and discussions” section summarizes \nresults and discussion. The conclusion of the paper is shown in “Conclusion and future \nwork” section.\n\nRelated work\nMakridakis et al. [24] illustrate that machine learning methods are alternative methods \nfor statistical analysis of multiple forecasting field. Author claims that statistical methods \nare more accurate than machine learning [25] methods. The reason for less accuracy is \nthe unknown values of data i.e., improper knowledge and pre-processing of data.\n\n\n\nPage 3 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nDifferent works have been implemented using the Matrix factorization (MF) [14] \nmethod with collaborative filtering [26]. Hao et al. [15] focused on a work based on the \nfactorization of the user rating matrix into two vectors, i.e., user latent and item latent \nwith low dimensionality. The sum of squared distance can be minimized by training a \nmodel that can find a solution using Stochastic Gradient Decent [27] or by least squares \n[28]. Salakhutdinov et al. [29] proposed a method that can be scaled linearly by probabil-\nity related matrix factorization on a big volume of datasets and then comparing it with \nthe single value decomposition method. This matrix factorization outperforms other \nprobability factorization methods like Bayesian-based probabilistic analysis [29] and \nstandard probability-based matrix factorization methods. A conventional approach, like \ntraditional collaborative Filtering [13, 30] method depends on customers and items. The \nuser item matrix factorization technique has been used for implementation purpose. \nIn the recommender system, there is a limitation in the sparsity problem and cold start \nproblem. In addition to the user item matrix factorization method, various analyses and \napproaches have been implemented to solve these recommendation issues.\n\nWietsma et al. [31] proposed a recommender system that gives information about the \nmobile decision aid and filtering function. This has been implemented with a study of \n29 features of student user behavior. The result shows the correlation among the user \nreviews and product reviews from different websites. Jianguo Chen et al. [32] proposed \na recommendation system for the treatment and diagnosis of the diseases. For cluster \nanalysis of disease symptoms, a density-peaked method is adopted. A rule-based apriori \nalgorithm is used for the diagnosis of disease and treatment. Asha et al. [33] proposed \nthe Gini-index feature method using movie review dataset. The sentimental analysis \nof the reviews are performed and opinion extraction of the sentences are done. Gini-\nindex impurity measure improves the accuracy of the polarity prediction by sentimental \nanalysis using Support vector machine [34, 35]. Depending on the frequency of occur-\nrence of a word in the document, the term frequency is calculated and opinion words \nare extracted using the Gini-index method. In this method, high term frequency words \nare not included, as it decreases the precision. The disadvantage of this method is that \nfor the huge volume of data, the prediction accuracy decreases.\n\nLuo et al. [36] proposed a method based on historical data to analyze the quality of \nservice for automatic service selection. Liu et al. [37] proposed a system in a mobile envi-\nronment for movie rating and review summarization. The authors used Latent Semantic \nAnalysis (LSA-based) method for product feature identification and feature-based sum-\nmarization. Statistical methods [38] have been used for identifying opinion words. The \ndisadvantage of this method is that LSA-based method cannot be represented efficiently; \nhence, it is difficult to index based on individual dimensions. This reduces the prediction \naccuracy in large datasets.\n\nLack of appropriate computing models for handling huge volume and redundancy in \ncustomer review datasets is a major challenge. Another major challenge handled in the \nproposed work is the existence of a pre-launch product in the industry based on the \nproduct features, which can be predicted based on the customer feedback in the form \nof reviews and ratings of the existing products. This prediction helps to optimize the \ndesign of the product to improve its quality with the required product features. Many \nof the relational database management systems are handling structured data, which is \n\n\n\nPage 4 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nnot scalable for big data that handles a large volume of unstructured data. This proposed \nmodel solves the problem of redundancy in a huge volume of the dataset for better pre-\ndiction accuracy.\n\nMethodology\nA pre-launch product prediction using different classifiers has been analysed by huge \ncustomer review and rating dataset. The product prediction is done through the phases \nconsisting of data collection phase, feature selection and duplicate data removal, build-\ning prediction classifier, training as well as testing.\n\nFigure 1 describes the various stages in system design of the model. The input dataset \nconsists of multivariate data which includes categorical, real and text data. Input dataset \nis fed for data pre-processing. Data pre-processing consists of feature selection, redun-\ndancy elimination and data integration which is done using Feature Information Gain \nand Distributed Memory-based Resilient Dataset Filter approach. The cleaned dataset \nis trained using classification algorithms. The classifiers considered for training are Sup-\nport Vector Machine (SVM) and Logistic Regression (LR). Further the dataset is tested \nfor pre-launch prediction using LR and SVM.\n\nData collection phase\n\nThis methodology can be applied for different products. Several datasets like Ama-\nzon and flip cart customer reviews are available as public datasets [39–41]. The data-\nset of customer reviews and ratings of seven brands of mobile phones for a period of \n24 months are considered in this work. The mobile phones product reviews are chosen \nbecause of two reasons. New mobile phones are launched into the market industry day \nby day which is one of the unavoidable items in everyone’s life. Market sustainability for \nthe mobile phones is very low.\n\nTable  1 shows a sample set of product reviews in which input dataset consists of \nuser features and product features. User features consists of Author, ReviewID and \nTitle depending on the user. Product feature consists of Product categories, Overall \nratings and Review Content. Since mobile phone is taken as the product, the catego-\nrization is done according to the features such as Battery life, price, camera, RAM, \n\nData collection \n\nCategorical\n\nText\n\nReal\n\nData Pre-\nprocessing\n\nFeature \nIdentification\n\nRedundancy\nRemoval\n\nData \nIntegration\n\nTraining \nDataset Using \nclassification \nalgorithms\n\nSupport \nVector \n\nLogistic \nRegression\n\nTesting Dataset \nUsing \n\nclassification \nalgorithms\n\nLogistic \nRegression\n\nSupport \nVector \n\nFig. 1 Product prelaunch prediction System Design\n\n\n\n\n\nPage 5 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nprocessor, weight etc. Some features are given a priority weightage depending on the \nproduct and user requirements. Input dataset with JSON file format is taken.\n\nDataset pre‑processing\n\nIn data pre-processing, feature selection plays a major role. In the product review \ndataset of a mobile phone, a large number of features exist. Identifying a feature from \ncustomer reviews is important for this model to improve the prediction accuracy. \nEnhanced Feature Information Gain measure has been implemented to identify sig-\nnificant feature.\n\nFeatures are identified based on the content of the product reviews, ratings of the \nproduct reviews and opinion identification of the reviews. Ratings of the product \nreviews can be further categorized based on a rating scale of 5 (1—Bad, 2—Average, \n3—Good, 4—very good, 5—Excellent). For opinion identification of the product, the \npolarity of extracted opinions for each review is classified using Senti-WordNet [42].\n\nFeature Information Gain measures the amount of information of a feature \nretrieved from a particular review. Impurity which is the measure of reliability of fea-\ntures in the input dataset should be reduced to get significant features. To measure \nfeature impurity, the best information of a feature obtained from each review is calcu-\nlated as follows\n\n• Let Pi be the probability of any feature instance \n(\n\nf\n)\n\n of k feature set F =\n{\n\nf1, f2, . . . fk\n}\n\n \nbelonging to ith customer review Ri , where i varies from 1 to N.\n\n• Let N denotes the total number of customer reviews.\n• Let OR denotes the polarity of extracted opinions of the Review.\n• Let SR denotes product rating scale of review (R).\n\nTable 1 Sample set of Product Reviews\n\n\n\nPage 6 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nThe information of a feature with respect to review rating and opinion is denoted by \nIf\n\nExpected information gain of the feature denoted as Ef\n\nReview Feature Impurity R(I) is calculated as\n\nThen Feature Information Gain (�G) to find out significant features are calculated \nas\n\nFeatures are selected based on the �G value and those with an Information gain \ngreater than 0.5 is selected as a significant feature. Table 2 shows the significant fea-\nture from customer reviews and ratings.\n\nNext step is to eliminate the redundant reviews and to replace null values of an \nactive customer from the customer review dataset using an enhanced big data pro-\ncessing approach. Reviews with significant features obtained from feature identifica-\ntion are considered for further processing.\n\n(1)If = log2\n\n(\n\n1\n\nP(R = F)\n\n)\n\n∗ OR ∗ SR.\n\n(2)Ef =\n\nN\n∑\n\ni=1\n\n−Pi(R = F).\n∥\n\n∥If\n∥\n\n∥\n\n1\n.\n\n(3)R(I) = −\n\nN\n∑\n\ni=1\n\nPi.log2Ef .\n\n(4)�G = R(I)−\n\nN\n∑\n\ni=1\n\n[(\n\nOR\n\nN\n∗ Ef\n\n)\n\n−\n\n(\n\nSR\n\nN\n∗ Ef\n\n)]\n\n.\n\nTable 2 Significant Features from Customer Reviews and Ratings\n\nNo Customer reviewed features No Customer reviewed features\n\n1 Author 17 RAM\n\n2 Title 18 Sim type\n\n3 ReviewID 19 Product category\n\n4 Content 20 Thickness\n\n5 Product brand 21 Weight of mobile phone\n\n6 Ratings 22 Height\n\n7 Battery life 23 Product type\n\n8 Price 24 Product rating\n\n9 Feature information gain 25 Front camera\n\n10 Review type 26 Back camera\n\n11 Product display 27 Opinion of review\n\n12 Processor 28 Multi-band\n\n13 Operating system 29 Network support\n\n14 Water proof 30 Quick charging\n\n15 Rear camera 31 Finger sensor\n\n16 Applications inbuilt 32 Internal storage\n\n\n\nPage 7 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nResilient Distributed Dataset\n\nResilient Distributed Dataset (RDD) [43] is a big data processing approach, which allows \nto store cache chunks of data on memory and persevere it as per the requirements. \nThe in-memory data caching is supported by RDD. Variety of jobs at a point of time \nis another challenge which is handled by RDD. This method deals with chunks of data \nduring processing and analysis. RDD can also be used for machine learning supported \nsystems as well as in big data processing and analysis, which happens to be an almost \npervasive requirement in the industry.\n\nIn the proposed method the main actions of RDD are:\n\n• Reduce (β): Combine all the elements of the dataset using the function β.\n• First (): This function will return the first element\n• takeOrdered(n): RDD is returned with first ‘n’ elements.\n• saveAsSequenceFile(path): the elements in the dataset to be written to the local file \n\nsystem with given path.\n\nThe main Transformations of RDD are:\n\n• map(β): Elements from the input file is mapped and new dataset is returned through \nfunction β.\n\n• filter(β): New dataset is returned if the function β returns true.\n• groupBykey(): When called a dataset of (key, value) pairs, this function returns a \n\ndataset of (key, value) pairs.\n• ReduceBykey(β): A (key, value) pair dataset is returned, where the values of each key \n\nare combined using the given reduce function β.\n\nIn the proposed work an enhanced Distributed Memory-based Resilience Dataset \nFilter (DMRDF) is applied. DMRDF method have long Lineage and it is recomputed \nthemselves using prior information, thus it achieves fault-tolerance. DMRDF has been \nimplemented to remove the redundancy in the dataset for product pre-launch predic-\ntion. This enhanced method is simple and fast.\n\n• Let the list of n customers represented as C = {c1, c2, c3 . . . , cn}\n\n• Let the list of N reviews be represented as R = {r1, r2, r3 . . . , rN }\n\n• Let x significant features are identified from feature set (F ) represented as Fx ⊂ F\n\n• An active customer consists of significant feature having information Gain value \ndenoted by �G\n\nIn the DMRDF method, a product is chosen and its customer reviews are found out. \nEliminate customers with similar reviews on the selected product and also reviews \nwith insignificant features. Calculate the memory-based Resilient Dataset Filter score \nbetween each of the customer reviews with significant features.\n\nLet us consider a set C of ‘n’ number of customers, the set R of ‘N’ number of reviews and \na set of significant features ′F ′\n\nx are considered. The corresponding vectors are represented \nas KC , KR and KFx . Then KRi is represented using a row vector and KFj is represented using \nthe column vector. Each entry KCm denote the number of times the mth review arrives in \n\n\n\nPage 8 of 15Narayanan et al. J Big Data (2020) 7:13 \n\ncustomers. The similarities between ith review of mth customer is found out using L1 norm \nof KRi and KCm . The Distributed Memory-based resilient filter score δ is calculated using the \nEq. (5).\n\nThe δ score is calculated for each customer review whereas the score lies between [0,1]. \nThe significant features are found out using Eq. 4. For customer reviews without significant \nfeatures, �G value will be zero. The reviews with δ score value 0 are found to be insignificant \nwithout any significant feature or opinion and hence those reviews are eliminated and not \nconsidered for further processing in the work. More than one Distributed Memory-based \nresilient filter score value is identified then the second occurrence of the review is consid-\nered as duplicate.\n\nPrediction classifiers\n\nLogistic regression and Support Vector Machine classifiers are the supervised machine \nlearning approaches used in the proposed work for product pre-launch prediction.\n\nLogistic regression (LR)\n\nWe have implemented proposed model using logistic regression analysis for prediction. \nThis model predicts the failure or success of a new product in the market by analysing \nselected product features from customer reviews. A case study has been conducted using \nthe dataset of customer reviews of mobile phones. Success or failure is the predictor vari-\nable used for training and testing the dataset. For training the model 75% of the dataset is \nused and for testing the model, remaining 25% is used.\n\n• Let p be the prediction variable value, assigning 0 for failure and 1 for success.\n• p0 is the constant value.\n• b is the logarithmic base value.\n\nThen the logit function is,\n\nThen the Logistic regression value γ is shown in Eq. (7),\n\n(5)δ =\n\nN\nn\n�\n\ni = 1\n\nm = 1\n\n\n\n\n\n�\n\nKRi ∗\n\n�\n\n�x\nj=1 KFj\n\n��\n\n∗ KCm\n\nKRi · KCm\n\n\n\n ∗ |�G|\n\n(6)\nL0 = b\n\np0+p\nx\n∑\n\ni=1\n\nfi\n\n(7.1)γ =\nL0\n\n(\n\nbp0+p\n∑x\n\ni=1 fi\n)\n\n+ 1\n\n(7.2)=\n1\n\n1+ b\n−\n\n(\n\nb\np0+p\n\n∑x\ni=1\n\nfi\n)\n\n\n\nPage 9 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nThe probability value of γ lies between [0,1]. In this work, if this value is greater than 0.5 \nthe pre-launch prediction of the product is considered as success and for values less than \n0.5, it is considered as failure.\n\nSupport Vector Machine (SVM)\n\nSVM is the supervised machine learning method, used to learn from set of data to get new \nskills and knowledge. This classification method can learn from data features relationships \n( zi ) and its class \n\n(\n\nyi\n)\n\n that can be applied to predict the success or failure class the product \nbelongs to.\n\n• For a set T of t training feature vectors, zi ∈ RD, where i = 1 to t.\n• Let yi ∈ {+1,−1} , where +1 belongs to product success class and -1 belongs to product \n\nfailure class.\n• The data separation occurs in the real numbers denoted as X in the D dimensional \n\ninput space.\n• Let w be the hyper plane normal vector element, where w ∈ XD.\n\nThe hyper plane is placed in such a way that distance between the nearest vectors of the \ntwo classes to the hyperplane should be maximum. Thus, the decision hyper plane is calcu-\nlated as,\n\nThe conditions for training dataset d ∈ X , is calculated as\n\nTo maximize the margin the value of w should be minimized.\nThe products in the positive one class (+1) are considered as successful products, [from \n\nEq. (9)] and those in the negative one class (−1) [from Eq. (10)] are in failure class.\n\nExperimental setup\n\nThe proposed system was implemented using Apache Spark 2.2.1 framework. Spark pro-\ngramming for python using PySpark version 2.1.2, which is the Spark python API has been \nused for the application development. An Ubuntu running Apache web server using Web \nServer Gateway Interface is used. Amazon Web Services is used to run some components \nof the software system large servers (nodes), having two Intel Xeon E5-2699V4 2.2 G Hz \nprocessors (VCPUs) with 4 cores and 16 GB of RAM on different Spark cluster configura-\ntions. According to the scalability requirements the software components can be config-\nured and can run on separate servers.\n\n(8)α(w) =\n2\n\n�w�\n\n(9)wtzi + d ≥ 1, where yi = +1.\n\n(10)wtzi + d ≤ −1, whereyi = yi − 1.\n\n\n\nPage 10 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nResults and discussions\nTo evaluate our prediction system several case studies have been conducted. Support \nVector Machine and Logistic regression classifiers are employed to perform the predic-\ntion. Most significant customer review features are used to analyse the system perfor-\nmance. The prediction accuracy evaluation is taken as one of the system design factors. \nThe system response time is another major concern for big data processing system. In \nthe customer review feature identification, we propose feature information gain and \nDMRDF approach to identify significant features and to eliminate redundant customer \nreviews from the input dataset.\n\nFigure  2 illustrates significant features required for the mobile phone sustainability. \nCustomer reviews and ratings of 7 brands of mobile phones are identified and evalu-\nated with DMRDF using SVM and LR. The graph shows the significant features identi-\nfied by the model against the percentage of customers whose reviews are analysed. 88% \nof the customers identified internal storage as a significant feature. Product price has \nbeen identified by 79% of customers as significant feature. With this evaluation customer \nrequirements for a product can be analysed in a better manner, thus can optimize the \ndesign of the product for better product quality and for product sustainability in the \nindustry.\n\nFigure 3 shows the comparison of the processing time taken by the proposed model \nwith different dataset size against that of the state of art techniques. DMRDF method \ntakes less time for completion of the application compared to other gini-index and latent \nsemantic analysis methods. Hence the proposed model is fast and scalable. It provides a \nhigh-speed processing performance with large datasets. This shows the DMRDF applica-\nbility in big data analytics, whereas gini-index and LSA-based methods processing time \nis larger for large volume of dataset. From the Fig. 3 it can be seen that with 9 GB dataset \ntime taken for prediction using LSA-based model, Gini-index model and DMRDF model \nis 342 s, 495 s and 156 s respectively. With 18 GB dataset time taken for prediction using \nLSA-based model, Gini-index model and DMRDF model 740 s, 910 s and 256 s respec-\ntively. Gini-index and LSA-based methods time taken for 18 GB dataset is twice that of \n9 GB dataset. But for DMRDF model time taken for 18 GB dataset is 1.6 times that of \n\n79%\n\n15%\n\n45%\n35%\n\n22%\n\n40%\n\n22%\n\n39%\n\n88%\n\n53%\n\n21%\n\n61%\n\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n\n100%\n\nPe\nrc\n\nen\nta\n\nge\n o\n\nf C\nus\n\nto\nm\n\ner\ns\n\nIden�fied Significant Features\nFig. 2 Identified Significant Features from Customer reviews and Ratings\n\n\n\nPage 11 of 15Narayanan et al. J Big Data (2020) 7:13 \n\n9 GB dataset and also it is 3 times lesser than Gini-index method. DMRDF model has \nmore advantage compared to the other state of art techniques in the case of application \nexecution and performance.\n\nThe reliability of the methods considered for the pre-launch prediction depends on \nprecision [44], recall and prediction accuracy measurement. Table 5 shows a comparison \nof precision, recall and accuracy measures of DMRDF, Gini-index and LSA-based meth-\nods with Support Vector Machine and Logistic Regression classifiers using customer \nreviews dataset over a period of 24 months. The results shown in Table 3 are best proved \nusing DMRDF with Support Vector Machine classification with prediction accuracy of \n95.4%. The DMRDF outperforms LSA-based and Gini-index methods in P@R, R@R and \nPA measures. Using proposed method, true positive (TP), false positive (FP), true nega-\ntive (TN) and false negative (FN) are found out. The prediction accuracy (PA), precision \n(P@R) and recall (R@R) are computed using Eqs. (10), (11), and (12) respectively.\n\n(10)PA =\nTP + TN\n\nTP + TN + FP + FN\n\n0\n\n100\n\n200\n\n300\n\n400\n\n500\n\n600\n\n700\n\n800\n\n900\n\n1000\n\n1GB 5GB 9GB 13GB 18GB\n\nGini-index\n\nDMRDF\n\nLSA-based\n\nTi\nm\n\ne \nTa\n\nke\nn \n\nin\n se\n\nc\n\nDataset size\nFig. 3 Dataset Size versus Processing Time Graph\n\nTable 3 Performance comparison of the proposed model with state of art techniques\n\nClassifier Support vector machine\n\nMethod used P@R (precision) PA % \n(prediction \naccuracy)\n\nDMRDF 0.941 0.92 95.4\n\nLSA-based 0.894 0.79 87.5\n\nGini-index 0.66 0.567 83.2\n\nClassifier Logistic regression\n\nMethod used P@R R@R % PA %\n\nDMRDF 0.915 0.849 93.5\n\nLSA-based 0.839 0.753 83\n\nGini-index 0.62 0.52 79.8\n\n\n\nPage 12 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nUsing DMRDF with SVM classifier and LR classifier, the prediction accuracy varia-\ntions are less compared to LSA-based and Gini-index methods. Hence DMRDF out-\nperforms the other two methods for customer review feature prediction.\n\nFurthermore Fig.  4, shows the DMRDF, LSA-based and Gini-index approaches as \napplied to the customer reviews and ratings datasets for 3, 6, 12, 18 and 24 months. \nIn DMRDF many features may appear in different customer review aspects, hence \nperformance evaluation will not consider duplicate customer reviews. In Gini- index, \nfeatures are extracted based on the polarity of the reviews and for large dataset P@R \nand R@R are less. The results show that DMRDF method outperforms the other two \nmethods in big data analysis. Gini-index approach does not perform well in customer \nreview feature prediction.\n\nConclusion and future work\nTechnological development in this era brings new challenges in artificial intelligence \nlike prediction, which is the next frontier for innovation and productivity. This work \nproposes the implementation of a scalable and reliable big data processing model \n\n(11)P@R =\nTP\n\nTP + FP\n\n(12)R@R =\nTP\n\nTP + FN\n\na SVM b SVM \n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nP@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nR@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\nc Logistic Regression d Logistic Regression\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nP@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nR@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\nFig. 4 Precision and Recall of DMRDF, LSA-based and Gini-index methods using SVM and LR classifiers\n\n\n\nPage 13 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nwhich identify significant features and eliminates redundant data using Feature Infor-\nmation Gain and Distributed Memory-based Resilient Dataset Filter method with \nLogistic Regression and Support Vector Machine prediction classifiers. A compari-\nson of the analysis has been conducted with state of art techniques like Gini-index \nand LSA-based approaches. The prediction accuracy, precision and recall of DMRDF \nmethod outperforms the other methods. Results show that the prediction accuracy \nof the proposed method increases by 10% using significant feature identification and \nelimination of redundancy from dataset compared to state of art techniques. Large \nfeature dimensionality reduces the prediction accuracy of the LSA-based method \nwhere as number of significant features plays an important role in prediction model-\nling. Results show that proposed DMRDF model is scalable and with huge volume of \ndataset model performance is good as well as time taken for processing the applica-\ntion is less compared to state of art techniques.\n\nResilience property of DMRDF method have long lineage, hence this can achieve \nfault-tolerance. DMRDF model is fast because of the in-memory computation \nmethod. Proposed design can be extended to other product feature identification big \ndata processing domains. As a future work, the model may be developed to make real \ntime streaming predictions through a unified API that searches customer comments, \nratings and surveys from different reliable online websites concurrently to obtain syn-\nthesis of sentiments with an information fusion approach. Since the statistical prop-\nerties of customer reviews and ratings vary over time, the performance of machine \nlearning algorithms can also come down. To cope w", "metadata_storage_path": "aHR0cHM6Ly9zdG9yYWdlYWNjb3VudGxpZ2lyay5ibG9iLmNvcmUud2luZG93cy5uZXQvcGFwZXIvczQwNTM3LTAyMC0wMDI5Mi15LnBkZg2", "metadata_author": "Sandhya Narayanan ", "metadata_title": "Improving prediction with enhanced Distributed Memory-based Resilient Dataset Filter", "keyphrases": [ "Distributed Memory‑based Resilient Dataset Filter", "Creative Commons Attribution 4.0 International License", "Distributed Memory-based Resilient Dataset Filter", "other third party material", "big data processing technologies", "A Feature Information Gain", "other online shopping sites", "Resilient Distribution Dataset", "social networking sites", "Creative Commons licence", "sophisticated pre-processing techniques", "significant feature identification", "Support vector machine", "Redundancy Open Access", "J Big Data", "successful product launch", "online product recommendations", "consumer electronics market", "online product reviews", "distributed environment", "Online reviews", "pre-processed dataset", "feature modelling", "Online feedback", "retail shopping", "useful information", "price information", "author information", "Customer reviews", "duplicate reviews", "product sale", "product life", "product quality", "less product", "product pre-launch", "Sandhya Narayanan1", "Philip Samuel2", "Mariamma Chacko3", "massive volumes", "different applications", "sensor data", "health care", "enormous size", "unstructured data", "crucial thing", "large size", "communication methods", "direct suggestions", "several advantages", "limited time", "research work", "FIG) measure", "Logistic regression", "resilience property", "appropriate credit", "original author", "credit line", "statutory regulation", "copyright holder", "creat iveco", "RESEARCH Narayanan", "Cochin University", "Full list", "new product", "1 Information Technology", "intended use", "permitted use", "mation source", "DMRDF method", "The Author", "doi.org", "prediction accuracy", "Introduction", "Extracting", "amount", "extensive", "customers", "impact", "extent", "ratings", "Abstract", "sustainability", "companies", "turn", "reliability", "classifiers", "output", "manufacturer", "design", "Keywords", "article", "sharing", "adaptation", "reproduction", "medium", "link", "changes", "images", "permission", "Correspondence", "nairsands", "School", "Engineering", "Science", "Kochi", "India", "creativecommons", "licenses", "crossmark", "crossref", "different natural language processing techniques", "significant data processing methods", "big data processing model", "Different feature selection methods", "wrapper feature selection method", "new prod- uct", "multiple forecasting field", "previous customer feedbacks", "online shopping sites", "structured massive volume", "customer review analysis", "spam reviews recognition", "many redundant reviews", "machine learning methods", "Consumer product success", "product pre-launch prediction", "future work” section", "user rating matrix", "poor quality products", "different criteria", "Different works", "wrapper methods", "art methods", "alternative methods", "statistical methods", "model complexity", "statistical analysis", "filter method", "customer reviews", "enhanced method", "unreliable data", "model performance", "shop owners", "other hand", "extra cost", "brand loyalty", "eCommerce firms", "other retailers", "large volume", "crucial phase", "univariate manner", "System design", "less accuracy", "unknown values", "improper knowledge", "Matrix factorization", "collaborative filtering", "two vectors", "low dimensionality", "accurate reviews", "duplicated reviews", "negative reviews", "Related work", "product reviews", "odology” section", "data pre-processing", "marketing strategies", "embedded process", "prediction classifiers", "Page", "15Narayanan", "information", "effort", "industry", "strategy", "users", "valuable", "number", "blogs", "forums", "awareness", "need", "ratio", "positive", "features", "usefulness", "relevance", "state", "gener", "ally", "combination", "distributive", "web", "order", "scalable", "failure", "realization", "paper", "methodology", "Results", "discussions", "conclusion", "Makridakis", "Author", "reason", "MF", "Hao", "item", "user item matrix factorization technique", "standard probability-based matrix factorization methods", "user item matrix factorization method", "ity related matrix factorization", "Gini- index impurity measure", "relational database management systems", "single value decomposition method", "high term frequency words", "probability factorization methods", "student user behavior", "Stochastic Gradient Decent", "mobile decision aid", "rule-based apriori algorithm", "mobile envi- ronment", "appropriate computing models", "traditional collaborative Filtering", "product feature identification", "Bayesian-based probabilistic analysis", "cold start problem", "automatic service selection", "Latent Semantic Analysis", "Gini-index feature method", "movie review dataset", "customer review datasets", "pre-launch product prediction", "Statistical methods", "user reviews", "opinion words", "Gini-index method", "filtering function", "movie rating", "review summarization", "customer feedback", "density-peaked method", "LSA-based) method", "LSA-based method", "squared distance", "big volume", "conventional approach", "implementation purpose", "sparsity problem", "various analyses", "recommendation issues", "different websites", "Jianguo Chen", "opinion extraction", "individual dimensions", "large datasets", "major challenge", "existing products", "different classifiers", "rating dataset", "polarity prediction", "product features", "recommender system", "recommendation system", "huge volume", "historical data", "big data", "disease symptoms", "sentimental analysis", "diction accuracy", "29 features", "solution", "squares", "Salakhutdinov", "other", "items", "limitation", "addition", "approaches", "Wietsma", "study", "result", "correlation", "treatment", "diagnosis", "diseases", "cluster", "Asha", "sentences", "rence", "document", "precision", "disadvantage", "Luo", "quality", "Liu", "authors", "Lack", "redundancy", "work", "existence", "Methodology", "phases", "Distributed Memory-based Resilient Dataset Filter approach", "Identification Redundancy Removal Data Integration Training", "classification algorithms Support Vector Logistic", "Product prelaunch prediction System Design", "Data collection Categorical Text Real", "Enhanced Feature Information Gain measure", "flip cart customer reviews", "ith customer review Ri", "Regression Support Vector", "duplicate data removal", "classification algorithms Logistic", "mobile phones product reviews", "port Vector Machine", "data collection phase", "JSON file format", "New mobile phones", "product rating scale", "categorical, real", "Regression Testing Dataset", "Dataset pre‑processing", "product review dataset", "text data", "Logistic Regression", "opinion identification", "data pre", "best information", "prediction classifier", "multivariate data", "pre-launch prediction", "Product feature", "input dataset", "Product categories", "feature selection", "processing Feature", "nificant feature", "feature instance", "k feature", "various stages", "dancy elimination", "different products", "Several datasets", "public datasets", "data- set", "seven brands", "two reasons", "unavoidable items", "sample set", "catego- rization", "priority weightage", "major role", "large number", "fea- tures", "total number", "particular review", "user requirements", "feature impurity", "market industry", "Battery life", "Review Content", "user features", "Figure", "model", "SVM", "LR", "zon", "period", "24 months", "day", "everyone", "Table", "ReviewID", "Title", "price", "camera", "RAM", "Fig.", "processor", "Average", "polarity", "opinions", "Senti-WordNet", "probability", "fk", "SR", "Distributed Memory-based Resilience Dataset Filter", "15 Rear camera 31 Finger sensor", "big data processing approach", "launch predic- tion", "Resilient Distributed Dataset", "local file system", "3 ReviewID 19 Product category", "Impurity R(I", "value) pair dataset", "memory data caching", "information Gain value", "Feature Information Gain", "customer review dataset", "Table 2 Significant Features", "Ef Review Feature", "13 Operating system", "input file", "P(R", "25 Front camera", "new dataset", "prior information", "9 Feature information", "Next step", "OR N", "Sim type", "4 Content 20 Thickness", "mobile phone", "7 Battery life", "10 Review type", "29 Network support", "14 Water proof", "Quick charging", "32 Internal storage", "machine learning", "pervasive requirement", "main actions", "first element", "main Transformations", "long Lineage", "n customers", "feature set", "active customer", "5 Product brand", "Product type", "11 Product display", "redundant reviews", "N reviews", "value) pairs", "�G value", "null values", "Pi.log2Ef", "SR N", "cache chunks", "24 Product rating", "reduce function", "SR.", "respect", "opinion", "No", "1 Author", "17 RAM", "2 Title", "Weight", "8 Price", "12 Processor", "Multi-band", "16 Applications", "RDD", "requirements", "Variety", "jobs", "point", "time", "challenge", "analysis", "systems", "elements", "saveAsSequenceFile", "path", "map", "groupBykey", "ReduceBykey", "fault-tolerance", "list", "Fx", "∑", "β", "Distributed Memory-based resilient filter score", "hyper plane normal vector element", "memory-based Resilient Dataset Filter score", "D dimensional input space", "resilient filter score value", "Support Vector Machine classifiers", "t training feature vectors", "one Distributed Memory-based", "decision hyper plane", "positive one class", "logarithmic base value", "logistic regression analysis", "machine learning method", "prediction variable value", "Logistic regression value", "data features relationships", "product failure class", "product success class", "δ score value", "row vector", "column vector", "Prediction classifiers", "significant feature", "corresponding vectors", "nearest vectors", "learning approaches", "classification method", "training dataset", "constant value", "probability value", "mth customer", "L1 norm", "second occurrence", "case study", "mobile phones", "logit function", "+ b", "new skills", "data separation", "real numbers", "two classes", "mth review", "ith review", "customer review", "similar reviews", "successful products", "N’ number", "KC", "KR", "KFx", "KFj", "entry", "similarities", "The", "Eq.", "processing", "More", "market", "p0", "L0", "values", "knowledge", "RD", "XD", "way", "distance", "hyperplane", "conditions", "margin", "γ", "different Spark cluster configura- tions", "Most significant customer review features", "two Intel Xeon E", "2699V4 2.2 G Hz processors", "Web Server Gateway Interface", "customer review feature identification", "big data processing system", "software system large servers", "LSA-based methods processing time", "big data analytics", "Apache web server", "Amazon Web Services", "Apache Spark 2.2.1 framework", "Logistic regression classifiers", "system perfor- mance", "different dataset size", "feature information gain", "negative one class", "semantic analysis methods", "high-speed processing performance", "system response time", "Spark python API", "several case studies", "system design factors", "mobile phone sustainability", "prediction accuracy measurement", "redundant customer reviews", "prediction accuracy evaluation", "DMRDF model time", "proposed system", "separate servers", "prediction system", "failure class", "software components", "LSA-based model", "less time", "product sustainability", "Experimental setup", "PySpark version", "Vector Machine", "predic- tion", "major concern", "internal storage", "art techniques", "DMRDF approach", "9 GB dataset", "18 GB dataset", "DMRDF model 740", "other gini-index", "Product price", "scalability requirements", "other state", "Gini-index model", "application development", "16 GB", "gramming", "Ubuntu", "nodes", "VCPUs", "4 cores", "wtzi", "Support", "7 brands", "LR.", "graph", "percentage", "manner", "comparison", "completion", "latent", "342 s", "495 s", "156 s", "910 s", "advantage", "execution", "recall", "Distributed Memory-based Resilient Dataset Filter method", "reliable big data processing model", "Support Vector Machine prediction classifiers", "Support Vector Machine classification", "Classifier Support vector machine", "different customer review aspects", "customer review feature prediction", "Processing Time Graph", "big data analysis", "Months LSA-based DMRDF Gini-index", "LSA-based meth- ods", "Logistic Regression classifiers", "other two methods", "duplicate customer reviews", "Classifier Logistic regression", "LR classifiers", "redundant data", "feature dimensionality", "other methods", "Gini-index methods", "Dataset size", "Gini-index approaches", "significant features", "LSA-based approaches", "accuracy measures", "P@R", "R@R", "false negative", "1GB 5GB", "SVM classifier", "ratings datasets", "performance evaluation", "Gini- index", "Technological development", "new challenges", "artificial intelligence", "next frontier", "mation Gain", "The DMRDF", "large dataset", "many features", "Performance comparison", "future work", "PA measures", "results", "TP", "FP", "TN", "FN", "Eqs", "18GB", "tions", "Conclusion", "era", "innovation", "productivity", "implementation", "elimination", "12", "7", "other product feature identification", "different reliable online websites", "prediction model- ling", "data processing domains", "information fusion approach", "statistical prop- erties", "memory computation method", "time streaming predictions", "dataset model performance", "important role", "DMRDF model", "applica- tion", "Resilience property", "long lineage", "Proposed design", "unified API", "customer comments", "learning algorithms", "real", "surveys", "thesis", "sentiments", "machine" ], "merged_content": "\nImproving prediction with enhanced \nDistributed Memory‑based Resilient Dataset \nFilter\nSandhya Narayanan1*, Philip Samuel2 and Mariamma Chacko3\n\nIntroduction\nAnalyzing and processing massive volumes of data in different applications like sensor \ndata, health care and e-Commerce require big data processing technologies. Extracting \nuseful information from the enormous size of unstructured data is a crucial thing. As the \namount of data becomes more extensive, sophisticated pre-processing techniques are \nrequired to analyze the data. In social networking sites and other online shopping sites, \na massive volume of online product reviews from a large size of customers are available \n[1]. The impact of online product reviews affects 90% of the current e-Commerce mar-\nket [2]. Customer reviews contribute the product sale to an extent and product life in the \nmarket depends on online product recommendations.\n\nOnline feedback is one of the communication methods which gives direct suggestions \nfrom the customers [3, 4]. Online reviews and ratings from customers are another infor-\nmation source about product quality [5, 6]. Customer reviews can help to decide on a new \nsuccessful product launch. Online shopping has several advantages over retail shopping. In \nretail shopping, the customers visit the shop and receive price information but less product \n\nAbstract \n\nLaunching new products in the consumer electronics market is challenging. Develop-\ning and marketing the same in limited time affect the sustainability of such companies. \nThis research work introduces a model that can predict the success of a product. A \nFeature Information Gain (FIG) measure is used for significant feature identification \nand Distributed Memory-based Resilient Dataset Filter (DMRDF) is used to eliminate \nduplicate reviews, which in turn improves the reliability of the product reviews. The \npre-processed dataset is used for prediction of product pre-launch in the market using \nclassifiers such as Logistic regression and Support vector machine. DMRDF method is \nfault-tolerant because of its resilience property and also reduces the dataset redun-\ndancy; hence, it increases the prediction accuracy of the model. The proposed model \nworks in a distributed environment to handle a massive volume of the dataset and \ntherefore, it is scalable. The output of this feature modelling and prediction allows the \nmanufacturer to optimize the design of his new product.\n\nKeywords: Distributed Memory-based, Resilient Distribution Dataset, Redundancy\n\nOpen Access\n\n© The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and \nthe source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material \nin this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material \nis not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the \npermitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creat iveco \nmmons .org/licen ses/by/4.0/.\n\nRESEARCH\n\nNarayanan et al. J Big Data (2020) 7:13 \nhttps://doi.org/10.1186/s40537‑020‑00292‑y\n\n*Correspondence: \nnairsands@gmail.com \n1 Information Technology, \nSchool of Engineering, \nCochin University of Science \n& Technology, Kochi 682022, \nIndia\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-020-00292-y&domain=pdf\n\n\nPage 2 of 15Narayanan et al. J Big Data (2020) 7:13 \n\ninformation from shop owners. On the other hand, online shopping sites give product \nreviews and previous customer feedbacks without extra cost and effort for the customers \n[7–10].\n\nInvesting in poor quality products potentially affects an industry’s brand loyalty and this \nstrategy should be changed by the eCommerce firms [5, 11]. Consumer product success \ndepends on different criteria, such as the quality of the product and marketing strategies. \nThe users should provide their valuable and accurate reviews about the products [12]. Cus-\ntomers bother to give reviews about products, whether they liked it or not. If the users \nprovide reviews, then other retailers can create some duplicated reviews [13, 14]. In online \nmarketing, the volume and value of product reviews are examined [15, 16]. The number \nof the product reviews on the shopping sites, blogs and forums has increased awareness \namong the users. This large volume of the reviews leads to the need for significant data \nprocessing methods [17, 18]. The value is the rating on the products. The ratio of positive to \nnegative reviews about the product leads to the quality of the product [19, 20].\n\nFeature selection is a crucial phase in data pre-processing [21]. Selecting features from \nan un-structured massive volume of data reduce the model complexity and improves the \nprediction accuracy. Different feature selection methods existing are the filter, wrapper and \nembedded. The wrapper feature selection method evaluates the usefulness of the feature \nand it depends on the performance of the classifier [22]. The filter method calculates the \nrelevance of the features and analyzes data in a univariate manner. The embedded process \nis similar to the wrapper method. Embedded and wrapper methods are more expensive \ncompared to the filter method. The state-of-art methods in customer review analysis gener-\nally discuss on categorizing positive and negative reviews using different natural language \nprocessing techniques and spam reviews recognition [23]. Feature selection of customer \nreviews increases prediction accuracy, thereby improves the model performance.\n\nAn enhanced method, which is a combination of filter and wrapper method is proposed \nin this work, which focuses on product pre-launch prediction with enhanced distributive \nfeature selection method. Since many redundant reviews are available on the web in large \nvolumes, a big data processing model has been implemented to filter out duplicated and \nunreliable data from customer reviews in-order to increase prediction accuracy. A scalable \nbig data processing model has been applied to predict the success or failure of a new prod-\nuct. The realization of the model has been done by Distributed Memory-based Resilient \nDataset Filter with prediction classifiers.\n\nThis paper is organized as follows. “Related work” section discusses related work. “Meth-\nodology” section contains the proposed methodology with System design, Resilient Distrib-\nuted Dataset and Prediction using classifiers. “Results and discussions” section summarizes \nresults and discussion. The conclusion of the paper is shown in “Conclusion and future \nwork” section.\n\nRelated work\nMakridakis et al. [24] illustrate that machine learning methods are alternative methods \nfor statistical analysis of multiple forecasting field. Author claims that statistical methods \nare more accurate than machine learning [25] methods. The reason for less accuracy is \nthe unknown values of data i.e., improper knowledge and pre-processing of data.\n\n\n\nPage 3 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nDifferent works have been implemented using the Matrix factorization (MF) [14] \nmethod with collaborative filtering [26]. Hao et al. [15] focused on a work based on the \nfactorization of the user rating matrix into two vectors, i.e., user latent and item latent \nwith low dimensionality. The sum of squared distance can be minimized by training a \nmodel that can find a solution using Stochastic Gradient Decent [27] or by least squares \n[28]. Salakhutdinov et al. [29] proposed a method that can be scaled linearly by probabil-\nity related matrix factorization on a big volume of datasets and then comparing it with \nthe single value decomposition method. This matrix factorization outperforms other \nprobability factorization methods like Bayesian-based probabilistic analysis [29] and \nstandard probability-based matrix factorization methods. A conventional approach, like \ntraditional collaborative Filtering [13, 30] method depends on customers and items. The \nuser item matrix factorization technique has been used for implementation purpose. \nIn the recommender system, there is a limitation in the sparsity problem and cold start \nproblem. In addition to the user item matrix factorization method, various analyses and \napproaches have been implemented to solve these recommendation issues.\n\nWietsma et al. [31] proposed a recommender system that gives information about the \nmobile decision aid and filtering function. This has been implemented with a study of \n29 features of student user behavior. The result shows the correlation among the user \nreviews and product reviews from different websites. Jianguo Chen et al. [32] proposed \na recommendation system for the treatment and diagnosis of the diseases. For cluster \nanalysis of disease symptoms, a density-peaked method is adopted. A rule-based apriori \nalgorithm is used for the diagnosis of disease and treatment. Asha et al. [33] proposed \nthe Gini-index feature method using movie review dataset. The sentimental analysis \nof the reviews are performed and opinion extraction of the sentences are done. Gini-\nindex impurity measure improves the accuracy of the polarity prediction by sentimental \nanalysis using Support vector machine [34, 35]. Depending on the frequency of occur-\nrence of a word in the document, the term frequency is calculated and opinion words \nare extracted using the Gini-index method. In this method, high term frequency words \nare not included, as it decreases the precision. The disadvantage of this method is that \nfor the huge volume of data, the prediction accuracy decreases.\n\nLuo et al. [36] proposed a method based on historical data to analyze the quality of \nservice for automatic service selection. Liu et al. [37] proposed a system in a mobile envi-\nronment for movie rating and review summarization. The authors used Latent Semantic \nAnalysis (LSA-based) method for product feature identification and feature-based sum-\nmarization. Statistical methods [38] have been used for identifying opinion words. The \ndisadvantage of this method is that LSA-based method cannot be represented efficiently; \nhence, it is difficult to index based on individual dimensions. This reduces the prediction \naccuracy in large datasets.\n\nLack of appropriate computing models for handling huge volume and redundancy in \ncustomer review datasets is a major challenge. Another major challenge handled in the \nproposed work is the existence of a pre-launch product in the industry based on the \nproduct features, which can be predicted based on the customer feedback in the form \nof reviews and ratings of the existing products. This prediction helps to optimize the \ndesign of the product to improve its quality with the required product features. Many \nof the relational database management systems are handling structured data, which is \n\n\n\nPage 4 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nnot scalable for big data that handles a large volume of unstructured data. This proposed \nmodel solves the problem of redundancy in a huge volume of the dataset for better pre-\ndiction accuracy.\n\nMethodology\nA pre-launch product prediction using different classifiers has been analysed by huge \ncustomer review and rating dataset. The product prediction is done through the phases \nconsisting of data collection phase, feature selection and duplicate data removal, build-\ning prediction classifier, training as well as testing.\n\nFigure 1 describes the various stages in system design of the model. The input dataset \nconsists of multivariate data which includes categorical, real and text data. Input dataset \nis fed for data pre-processing. Data pre-processing consists of feature selection, redun-\ndancy elimination and data integration which is done using Feature Information Gain \nand Distributed Memory-based Resilient Dataset Filter approach. The cleaned dataset \nis trained using classification algorithms. The classifiers considered for training are Sup-\nport Vector Machine (SVM) and Logistic Regression (LR). Further the dataset is tested \nfor pre-launch prediction using LR and SVM.\n\nData collection phase\n\nThis methodology can be applied for different products. Several datasets like Ama-\nzon and flip cart customer reviews are available as public datasets [39–41]. The data-\nset of customer reviews and ratings of seven brands of mobile phones for a period of \n24 months are considered in this work. The mobile phones product reviews are chosen \nbecause of two reasons. New mobile phones are launched into the market industry day \nby day which is one of the unavoidable items in everyone’s life. Market sustainability for \nthe mobile phones is very low.\n\nTable  1 shows a sample set of product reviews in which input dataset consists of \nuser features and product features. User features consists of Author, ReviewID and \nTitle depending on the user. Product feature consists of Product categories, Overall \nratings and Review Content. Since mobile phone is taken as the product, the catego-\nrization is done according to the features such as Battery life, price, camera, RAM, \n\nData collection \n\nCategorical\n\nText\n\nReal\n\nData Pre-\nprocessing\n\nFeature \nIdentification\n\nRedundancy\nRemoval\n\nData \nIntegration\n\nTraining \nDataset Using \nclassification \nalgorithms\n\nSupport \nVector \n\nLogistic \nRegression\n\nTesting Dataset \nUsing \n\nclassification \nalgorithms\n\nLogistic \nRegression\n\nSupport \nVector \n\nFig. 1 Product prelaunch prediction System Design\n\n \n\n\n\nPage 5 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nprocessor, weight etc. Some features are given a priority weightage depending on the \nproduct and user requirements. Input dataset with JSON file format is taken.\n\nDataset pre‑processing\n\nIn data pre-processing, feature selection plays a major role. In the product review \ndataset of a mobile phone, a large number of features exist. Identifying a feature from \ncustomer reviews is important for this model to improve the prediction accuracy. \nEnhanced Feature Information Gain measure has been implemented to identify sig-\nnificant feature.\n\nFeatures are identified based on the content of the product reviews, ratings of the \nproduct reviews and opinion identification of the reviews. Ratings of the product \nreviews can be further categorized based on a rating scale of 5 (1—Bad, 2—Average, \n3—Good, 4—very good, 5—Excellent). For opinion identification of the product, the \npolarity of extracted opinions for each review is classified using Senti-WordNet [42].\n\nFeature Information Gain measures the amount of information of a feature \nretrieved from a particular review. Impurity which is the measure of reliability of fea-\ntures in the input dataset should be reduced to get significant features. To measure \nfeature impurity, the best information of a feature obtained from each review is calcu-\nlated as follows\n\n• Let Pi be the probability of any feature instance \n(\n\nf\n)\n\n of k feature set F =\n{\n\nf1, f2, . . . fk\n}\n\n \nbelonging to ith customer review Ri , where i varies from 1 to N.\n\n• Let N denotes the total number of customer reviews.\n• Let OR denotes the polarity of extracted opinions of the Review.\n• Let SR denotes product rating scale of review (R).\n\nTable 1 Sample set of Product Reviews\n\n\n\nPage 6 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nThe information of a feature with respect to review rating and opinion is denoted by \nIf\n\nExpected information gain of the feature denoted as Ef\n\nReview Feature Impurity R(I) is calculated as\n\nThen Feature Information Gain (�G) to find out significant features are calculated \nas\n\nFeatures are selected based on the �G value and those with an Information gain \ngreater than 0.5 is selected as a significant feature. Table 2 shows the significant fea-\nture from customer reviews and ratings.\n\nNext step is to eliminate the redundant reviews and to replace null values of an \nactive customer from the customer review dataset using an enhanced big data pro-\ncessing approach. Reviews with significant features obtained from feature identifica-\ntion are considered for further processing.\n\n(1)If = log2\n\n(\n\n1\n\nP(R = F)\n\n)\n\n∗ OR ∗ SR.\n\n(2)Ef =\n\nN\n∑\n\ni=1\n\n−Pi(R = F).\n∥\n\n∥If\n∥\n\n∥\n\n1\n.\n\n(3)R(I) = −\n\nN\n∑\n\ni=1\n\nPi.log2Ef .\n\n(4)�G = R(I)−\n\nN\n∑\n\ni=1\n\n[(\n\nOR\n\nN\n∗ Ef\n\n)\n\n−\n\n(\n\nSR\n\nN\n∗ Ef\n\n)]\n\n.\n\nTable 2 Significant Features from Customer Reviews and Ratings\n\nNo Customer reviewed features No Customer reviewed features\n\n1 Author 17 RAM\n\n2 Title 18 Sim type\n\n3 ReviewID 19 Product category\n\n4 Content 20 Thickness\n\n5 Product brand 21 Weight of mobile phone\n\n6 Ratings 22 Height\n\n7 Battery life 23 Product type\n\n8 Price 24 Product rating\n\n9 Feature information gain 25 Front camera\n\n10 Review type 26 Back camera\n\n11 Product display 27 Opinion of review\n\n12 Processor 28 Multi-band\n\n13 Operating system 29 Network support\n\n14 Water proof 30 Quick charging\n\n15 Rear camera 31 Finger sensor\n\n16 Applications inbuilt 32 Internal storage\n\n\n\nPage 7 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nResilient Distributed Dataset\n\nResilient Distributed Dataset (RDD) [43] is a big data processing approach, which allows \nto store cache chunks of data on memory and persevere it as per the requirements. \nThe in-memory data caching is supported by RDD. Variety of jobs at a point of time \nis another challenge which is handled by RDD. This method deals with chunks of data \nduring processing and analysis. RDD can also be used for machine learning supported \nsystems as well as in big data processing and analysis, which happens to be an almost \npervasive requirement in the industry.\n\nIn the proposed method the main actions of RDD are:\n\n• Reduce (β): Combine all the elements of the dataset using the function β.\n• First (): This function will return the first element\n• takeOrdered(n): RDD is returned with first ‘n’ elements.\n• saveAsSequenceFile(path): the elements in the dataset to be written to the local file \n\nsystem with given path.\n\nThe main Transformations of RDD are:\n\n• map(β): Elements from the input file is mapped and new dataset is returned through \nfunction β.\n\n• filter(β): New dataset is returned if the function β returns true.\n• groupBykey(): When called a dataset of (key, value) pairs, this function returns a \n\ndataset of (key, value) pairs.\n• ReduceBykey(β): A (key, value) pair dataset is returned, where the values of each key \n\nare combined using the given reduce function β.\n\nIn the proposed work an enhanced Distributed Memory-based Resilience Dataset \nFilter (DMRDF) is applied. DMRDF method have long Lineage and it is recomputed \nthemselves using prior information, thus it achieves fault-tolerance. DMRDF has been \nimplemented to remove the redundancy in the dataset for product pre-launch predic-\ntion. This enhanced method is simple and fast.\n\n• Let the list of n customers represented as C = {c1, c2, c3 . . . , cn}\n\n• Let the list of N reviews be represented as R = {r1, r2, r3 . . . , rN }\n\n• Let x significant features are identified from feature set (F ) represented as Fx ⊂ F\n\n• An active customer consists of significant feature having information Gain value \ndenoted by �G\n\nIn the DMRDF method, a product is chosen and its customer reviews are found out. \nEliminate customers with similar reviews on the selected product and also reviews \nwith insignificant features. Calculate the memory-based Resilient Dataset Filter score \nbetween each of the customer reviews with significant features.\n\nLet us consider a set C of ‘n’ number of customers, the set R of ‘N’ number of reviews and \na set of significant features ′F ′\n\nx are considered. The corresponding vectors are represented \nas KC , KR and KFx . Then KRi is represented using a row vector and KFj is represented using \nthe column vector. Each entry KCm denote the number of times the mth review arrives in \n\n\n\nPage 8 of 15Narayanan et al. J Big Data (2020) 7:13 \n\ncustomers. The similarities between ith review of mth customer is found out using L1 norm \nof KRi and KCm . The Distributed Memory-based resilient filter score δ is calculated using the \nEq. (5).\n\nThe δ score is calculated for each customer review whereas the score lies between [0,1]. \nThe significant features are found out using Eq. 4. For customer reviews without significant \nfeatures, �G value will be zero. The reviews with δ score value 0 are found to be insignificant \nwithout any significant feature or opinion and hence those reviews are eliminated and not \nconsidered for further processing in the work. More than one Distributed Memory-based \nresilient filter score value is identified then the second occurrence of the review is consid-\nered as duplicate.\n\nPrediction classifiers\n\nLogistic regression and Support Vector Machine classifiers are the supervised machine \nlearning approaches used in the proposed work for product pre-launch prediction.\n\nLogistic regression (LR)\n\nWe have implemented proposed model using logistic regression analysis for prediction. \nThis model predicts the failure or success of a new product in the market by analysing \nselected product features from customer reviews. A case study has been conducted using \nthe dataset of customer reviews of mobile phones. Success or failure is the predictor vari-\nable used for training and testing the dataset. For training the model 75% of the dataset is \nused and for testing the model, remaining 25% is used.\n\n• Let p be the prediction variable value, assigning 0 for failure and 1 for success.\n• p0 is the constant value.\n• b is the logarithmic base value.\n\nThen the logit function is,\n\nThen the Logistic regression value γ is shown in Eq. (7),\n\n(5)δ =\n\nN\nn\n�\n\ni = 1\n\nm = 1\n\n\n\n\n\n�\n\nKRi ∗\n\n�\n\n�x\nj=1 KFj\n\n��\n\n∗ KCm\n\nKRi · KCm\n\n\n\n ∗ |�G|\n\n(6)\nL0 = b\n\np0+p\nx\n∑\n\ni=1\n\nfi\n\n(7.1)γ =\nL0\n\n(\n\nbp0+p\n∑x\n\ni=1 fi\n)\n\n+ 1\n\n(7.2)=\n1\n\n1+ b\n−\n\n(\n\nb\np0+p\n\n∑x\ni=1\n\nfi\n)\n\n\n\nPage 9 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nThe probability value of γ lies between [0,1]. In this work, if this value is greater than 0.5 \nthe pre-launch prediction of the product is considered as success and for values less than \n0.5, it is considered as failure.\n\nSupport Vector Machine (SVM)\n\nSVM is the supervised machine learning method, used to learn from set of data to get new \nskills and knowledge. This classification method can learn from data features relationships \n( zi ) and its class \n\n(\n\nyi\n)\n\n that can be applied to predict the success or failure class the product \nbelongs to.\n\n• For a set T of t training feature vectors, zi ∈ RD, where i = 1 to t.\n• Let yi ∈ {+1,−1} , where +1 belongs to product success class and -1 belongs to product \n\nfailure class.\n• The data separation occurs in the real numbers denoted as X in the D dimensional \n\ninput space.\n• Let w be the hyper plane normal vector element, where w ∈ XD.\n\nThe hyper plane is placed in such a way that distance between the nearest vectors of the \ntwo classes to the hyperplane should be maximum. Thus, the decision hyper plane is calcu-\nlated as,\n\nThe conditions for training dataset d ∈ X , is calculated as\n\nTo maximize the margin the value of w should be minimized.\nThe products in the positive one class (+1) are considered as successful products, [from \n\nEq. (9)] and those in the negative one class (−1) [from Eq. (10)] are in failure class.\n\nExperimental setup\n\nThe proposed system was implemented using Apache Spark 2.2.1 framework. Spark pro-\ngramming for python using PySpark version 2.1.2, which is the Spark python API has been \nused for the application development. An Ubuntu running Apache web server using Web \nServer Gateway Interface is used. Amazon Web Services is used to run some components \nof the software system large servers (nodes), having two Intel Xeon E5-2699V4 2.2 G Hz \nprocessors (VCPUs) with 4 cores and 16 GB of RAM on different Spark cluster configura-\ntions. According to the scalability requirements the software components can be config-\nured and can run on separate servers.\n\n(8)α(w) =\n2\n\n�w�\n\n(9)wtzi + d ≥ 1, where yi = +1.\n\n(10)wtzi + d ≤ −1, whereyi = yi − 1.\n\n\n\nPage 10 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nResults and discussions\nTo evaluate our prediction system several case studies have been conducted. Support \nVector Machine and Logistic regression classifiers are employed to perform the predic-\ntion. Most significant customer review features are used to analyse the system perfor-\nmance. The prediction accuracy evaluation is taken as one of the system design factors. \nThe system response time is another major concern for big data processing system. In \nthe customer review feature identification, we propose feature information gain and \nDMRDF approach to identify significant features and to eliminate redundant customer \nreviews from the input dataset.\n\nFigure  2 illustrates significant features required for the mobile phone sustainability. \nCustomer reviews and ratings of 7 brands of mobile phones are identified and evalu-\nated with DMRDF using SVM and LR. The graph shows the significant features identi-\nfied by the model against the percentage of customers whose reviews are analysed. 88% \nof the customers identified internal storage as a significant feature. Product price has \nbeen identified by 79% of customers as significant feature. With this evaluation customer \nrequirements for a product can be analysed in a better manner, thus can optimize the \ndesign of the product for better product quality and for product sustainability in the \nindustry.\n\nFigure 3 shows the comparison of the processing time taken by the proposed model \nwith different dataset size against that of the state of art techniques. DMRDF method \ntakes less time for completion of the application compared to other gini-index and latent \nsemantic analysis methods. Hence the proposed model is fast and scalable. It provides a \nhigh-speed processing performance with large datasets. This shows the DMRDF applica-\nbility in big data analytics, whereas gini-index and LSA-based methods processing time \nis larger for large volume of dataset. From the Fig. 3 it can be seen that with 9 GB dataset \ntime taken for prediction using LSA-based model, Gini-index model and DMRDF model \nis 342 s, 495 s and 156 s respectively. With 18 GB dataset time taken for prediction using \nLSA-based model, Gini-index model and DMRDF model 740 s, 910 s and 256 s respec-\ntively. Gini-index and LSA-based methods time taken for 18 GB dataset is twice that of \n9 GB dataset. But for DMRDF model time taken for 18 GB dataset is 1.6 times that of \n\n79%\n\n15%\n\n45%\n35%\n\n22%\n\n40%\n\n22%\n\n39%\n\n88%\n\n53%\n\n21%\n\n61%\n\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n\n100%\n\nPe\nrc\n\nen\nta\n\nge\n o\n\nf C\nus\n\nto\nm\n\ner\ns\n\nIden�fied Significant Features\nFig. 2 Identified Significant Features from Customer reviews and Ratings\n\n\n\nPage 11 of 15Narayanan et al. J Big Data (2020) 7:13 \n\n9 GB dataset and also it is 3 times lesser than Gini-index method. DMRDF model has \nmore advantage compared to the other state of art techniques in the case of application \nexecution and performance.\n\nThe reliability of the methods considered for the pre-launch prediction depends on \nprecision [44], recall and prediction accuracy measurement. Table 5 shows a comparison \nof precision, recall and accuracy measures of DMRDF, Gini-index and LSA-based meth-\nods with Support Vector Machine and Logistic Regression classifiers using customer \nreviews dataset over a period of 24 months. The results shown in Table 3 are best proved \nusing DMRDF with Support Vector Machine classification with prediction accuracy of \n95.4%. The DMRDF outperforms LSA-based and Gini-index methods in P@R, R@R and \nPA measures. Using proposed method, true positive (TP), false positive (FP), true nega-\ntive (TN) and false negative (FN) are found out. The prediction accuracy (PA), precision \n(P@R) and recall (R@R) are computed using Eqs. (10), (11), and (12) respectively.\n\n(10)PA =\nTP + TN\n\nTP + TN + FP + FN\n\n0\n\n100\n\n200\n\n300\n\n400\n\n500\n\n600\n\n700\n\n800\n\n900\n\n1000\n\n1GB 5GB 9GB 13GB 18GB\n\nGini-index\n\nDMRDF\n\nLSA-based\n\nTi\nm\n\ne \nTa\n\nke\nn \n\nin\n se\n\nc\n\nDataset size\nFig. 3 Dataset Size versus Processing Time Graph\n\nTable 3 Performance comparison of the proposed model with state of art techniques\n\nClassifier Support vector machine\n\nMethod used P@R (precision) PA % \n(prediction \naccuracy)\n\nDMRDF 0.941 0.92 95.4\n\nLSA-based 0.894 0.79 87.5\n\nGini-index 0.66 0.567 83.2\n\nClassifier Logistic regression\n\nMethod used P@R R@R % PA %\n\nDMRDF 0.915 0.849 93.5\n\nLSA-based 0.839 0.753 83\n\nGini-index 0.62 0.52 79.8\n\n\n\nPage 12 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nUsing DMRDF with SVM classifier and LR classifier, the prediction accuracy varia-\ntions are less compared to LSA-based and Gini-index methods. Hence DMRDF out-\nperforms the other two methods for customer review feature prediction.\n\nFurthermore Fig.  4, shows the DMRDF, LSA-based and Gini-index approaches as \napplied to the customer reviews and ratings datasets for 3, 6, 12, 18 and 24 months. \nIn DMRDF many features may appear in different customer review aspects, hence \nperformance evaluation will not consider duplicate customer reviews. In Gini- index, \nfeatures are extracted based on the polarity of the reviews and for large dataset P@R \nand R@R are less. The results show that DMRDF method outperforms the other two \nmethods in big data analysis. Gini-index approach does not perform well in customer \nreview feature prediction.\n\nConclusion and future work\nTechnological development in this era brings new challenges in artificial intelligence \nlike prediction, which is the next frontier for innovation and productivity. This work \nproposes the implementation of a scalable and reliable big data processing model \n\n(11)P@R =\nTP\n\nTP + FP\n\n(12)R@R =\nTP\n\nTP + FN\n\na SVM b SVM \n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nP@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nR@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\nc Logistic Regression d Logistic Regression\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nP@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nR@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\nFig. 4 Precision and Recall of DMRDF, LSA-based and Gini-index methods using SVM and LR classifiers\n\n\n\nPage 13 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nwhich identify significant features and eliminates redundant data using Feature Infor-\nmation Gain and Distributed Memory-based Resilient Dataset Filter method with \nLogistic Regression and Support Vector Machine prediction classifiers. A compari-\nson of the analysis has been conducted with state of art techniques like Gini-index \nand LSA-based approaches. The prediction accuracy, precision and recall of DMRDF \nmethod outperforms the other methods. Results show that the prediction accuracy \nof the proposed method increases by 10% using significant feature identification and \nelimination of redundancy from dataset compared to state of art techniques. Large \nfeature dimensionality reduces the prediction accuracy of the LSA-based method \nwhere as number of significant features plays an important role in prediction model-\nling. Results show that proposed DMRDF model is scalable and with huge volume of \ndataset model performance is good as well as time taken for processing the applica-\ntion is less compared to state of art techniques.\n\nResilience property of DMRDF method have long lineage, hence this can achieve \nfault-tolerance. DMRDF model is fast because of the in-memory computation \nmethod. Proposed design can be extended to other product feature identification big \ndata processing domains. As a future work, the model may be developed to make real \ntime streaming predictions through a unified API that searches customer comments, \nratings and surveys from different reliable online websites concurrently to obtain syn-\nthesis of sentiments with an information fusion approach. Since the statistical prop-\nerties of customer reviews and ratings vary over time, the performance of machine \nlearning algorithms can also come down. To cope w", "text": [ "", "Published online: 28 February 2020" ], "layoutText": [ "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}", "{\"language\":\"en\",\"text\":\"Published online: 28 February 2020\",\"lines\":[{\"boundingBox\":[{\"x\":0,\"y\":16},{\"x\":1020,\"y\":16},{\"x\":1020,\"y\":74},{\"x\":0,\"y\":71}],\"text\":\"Published online: 28 February 2020\"}],\"words\":[{\"boundingBox\":[{\"x\":1,\"y\":17},{\"x\":281,\"y\":17},{\"x\":281,\"y\":70},{\"x\":0,\"y\":69}],\"text\":\"Published\"},{\"boundingBox\":[{\"x\":291,\"y\":17},{\"x\":497,\"y\":17},{\"x\":498,\"y\":72},{\"x\":291,\"y\":70}],\"text\":\"online:\"},{\"boundingBox\":[{\"x\":507,\"y\":17},{\"x\":585,\"y\":17},{\"x\":586,\"y\":72},{\"x\":508,\"y\":72}],\"text\":\"28\"},{\"boundingBox\":[{\"x\":595,\"y\":17},{\"x\":862,\"y\":17},{\"x\":864,\"y\":74},{\"x\":596,\"y\":72}],\"text\":\"February\"},{\"boundingBox\":[{\"x\":872,\"y\":17},{\"x\":1017,\"y\":17},{\"x\":1019,\"y\":75},{\"x\":874,\"y\":74}],\"text\":\"2020\"}]}" ] }, { "@search.score": 2.5412152, "content": "\nRESEARCH Open Access\n\nA classification method for social\ninformation of sellers on social network\nHaoliang Cui1, Shuai Shao2* , Shaozhang Niu1, Chengjie Shi3 and Lingyu Zhou1\n\n* Correspondence: shaoshuaib@163.\ncom\n2China Information Technology\nSecurity Evaluation Center, Beijing\n100085, China\nFull list of author information is\navailable at the end of the article\n\nAbstract\n\nSocial e-commerce has been a hot topic in recent years, with the number of users\nincreasing year by year and the transaction money exploding. Unlike traditional e-\ncommerce, the main activities of social e-commerce are on social network apps. To\nclassify sellers by the merchandise, this article designs and implements a social\nnetwork seller classification scheme. We develop an app, which runs on the mobile\nphones of the sellers and provides the operating environment and automated\nassistance capabilities of social network applications. The app can collect social\ninformation published by the sellers during the assistance process, uploads to the\nserver to perform model training on the data. We collect 38,970 sellers’ information,\nextract the text information in the picture with the help of OCR, and establish a\ndeep learning model based on BERT to classify the merchandise of sellers. In the\nfinal experiment, we achieve an accuracy of more than 90%, which shows that the\nmodel can accurately classify sellers on a social network.\n\nKeywords: User model, Machine learning, Social e-commerce\n\n1 Introduction\nWith the continuous improvement of social network and mobile payment technology,\n\none kind of commodity trading based on social relations called social e-commerce is in\n\nrapid development. According to the 2019 China social e-commerce industry develop-\n\nment report released by the Internet society of China, the number of employees of so-\n\ncial e-commerce in China is expected to reach 48.01 million in 2019, up by 58.3\n\npercent year on year, and the market size is expected to reach 2060.58 billion yuan, up\n\nby 63.2% year on year. Social e-commerce has become a large scale, and the high\n\ngrowth cannot be ignored. Different from e-commerce platforms such as Taobao, so-\n\ncial e-commerce is at the end of online retail. It carries out trading activities through\n\nsocial software and uses social interaction, user generated content and other means to\n\nassist the purchase and sale of goods. At the same time, sellers on social network use\n\ndifferent social software without uniform registration, have no systematic classification\n\nof products for sale, and there are no standardized terms for product description.\n\nThese bring great difficulty to the accurate classification of user portrait. This paper\n\nproposes a method based on the NLP classification model, which can realize accurate\n\n© The Author(s). 2021 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the\noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or\nother third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit\nline to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by\nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a\ncopy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\n\nEURASIP Journal on Image\nand Video Processing\n\nCui et al. EURASIP Journal on Image and Video Processing (2021) 2021:4 \nhttps://doi.org/10.1186/s13640-020-00545-z\n\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13640-020-00545-z&domain=pdf\nhttp://orcid.org/0000-0001-9638-0201\nmailto:shaoshuaib@163.com\nmailto:shaoshuaib@163.com\nhttp://creativecommons.org/licenses/by/4.0/\n\n\nbusiness classification of social e-commerce based on social information of social e-\n\ncommerce. This method analyzes 38,970 sellers on social networks and establishes a\n\ndeep learning model based on BERT to accurately classify the merchandise of sellers.\n\nIn addition, we introduced the OCR algorithm to extract the text information in the\n\npicture and superimposed it on the social content data, which effectively improved the\n\nclassification accuracy. The final experiment shows that the measured accuracy is more\n\nthan 90%.\n\n2 Related work\n2.1 Natural language processing\n\nIn order to analyze e-commerce business classification based on social data of sellers on a\n\nsocial network, the text needs to be analyzed based on the NLP correlation algorithm.\n\nThe rapid development of NLP at the present stage is due to the neural network language\n\nmodel (NNLM) Bengio et al. [1] proposed in 2003. Researchers have been trying to realize\n\nthe end-to-end classification recognition by using a neural network as a classifier in the\n\ntext classification research based on word embedding. Kim first introduces the convolu-\n\ntional neural network (CNN) into the study of text classification. The network structure is\n\na dropout full connection layer and a softmax layer connected after one convolution layer\n\n[2]. Although this algorithm achieves good results in various benchmark tests, it cannot\n\nobtain long-distance text dependency due to the limitation of network structure. There-\n\nfore, Tencent AI Lab proposed DPCNN, which further enhanced the extraction capacity\n\nof long-distance text dependency by deepening CNN [3].\n\nSocial content data includes multimedia text data and picture data. With the help of\n\nOCR, we extract the text in the picture and convert the picture data into text data. Text\n\nis a kind of sequential data, and the classification of it by recurrent neural network\n\n(RNN) has been the focus of long-term research in academia [4]. As a variation of\n\nRNN, long short-term memory (LSTM) adds control units such as forgetting gate, in-\n\nput gate, and output gate on the original basis, which solves the problem of gradient\n\nexplosion and gradient disappearance in the long sequence training of RNN and pro-\n\nmotes the use of RNN [5]. By introducing the sharing information mechanism, Liu\n\net al. further improved the accuracy of the RNN algorithm in the text multi-\n\nclassification task and achieved good results in four benchmark text classifications [6].\n\nHowever, Word vectors cannot be constructed in Word embedding to solve the\n\nproblem of polysemy. Even though different semantic environments are considered\n\nduring training, the result of training is still one word corresponding to one row vector.\n\nConsidering the widespread phenomenon of polysemy, Peters et al. propose embed-\n\ndings from language model (ELMO) to address the impact of polysemy on natural lan-\n\nguage modeling [7]. ELMO uses a feature-based form of pre-training. First, two-way\n\nLSTM is used to pre-train the corpus, and then word embedding resulting from train-\n\ning is adjusted by double-layer two-way LSTM when processing downstream tasks to\n\nadd more grammatical and semantic information according to the context words.\n\nThe ability of ELMO to extract features is limited for choosing LSTM as the feature\n\nextractor instead of Transformer [8], and ELMO’s bidirectional splicing method is also\n\nweak in feature fusion. Therefore, Devlin et al. propose the BERT model, taking Trans-\n\nformer as a feature extractor to pre-train large-scale text corpus [9].\n\nCui et al. EURASIP Journal on Image and Video Processing (2021) 2021:4 Page 2 of 12\n\n\n\n2.2 User analysis of social networks\n\nUser analysis is an important part of social network analysis. Most existing studies use\n\nuser-generated content or social links between users to simulate users. Wu et al. mod-\n\neled users on the content curation social network (CCSN) in the unified framework by\n\nmining user-generated content and social links [10]. They proposed a potential Bayes-\n\nian model, multilevel LDA (MLLDA), that could represent users of potential interest\n\nfound in social links formed by text descriptions contributed by users and information\n\nsharing. In 2017, Wu et al. proposed a latent model [11], trying to explain how the so-\n\ncial network structure and users’ historical preferences change over time affect each\n\nuser’s future behavior and predict each user’s consumption preferences and social con-\n\nnections in the near future. Malli et al. proposed a new online social network user pro-\n\nfile rating model [12], which solved the problem of large and complicated user data. In\n\nterms of data analysis platform, Chen et al. [13] developed a big data platform for the\n\nstudy of the garlic industry chain. Garlic planting management, price control, and pre-\n\ndiction were realized through data collection, storage, and pretreatment. Ning et al.\n\n[14] designed a ga-bp hybrid algorithm based on the fuzzy theory and constructed an\n\nair quality evaluation model by combining the knowledge of BP neural network, genetic\n\nalgorithm, and fuzzy theory. Yin et al. [15] studied two methods of extracting supervis-\n\nory relations and applied them to the field of English news. One is the combination of\n\nsupport vector machine and principal component analysis, and the other is the combin-\n\nation of support vector machine and CNN, which can extract high-quality feature vec-\n\ntors from sentences of support vector machine. In the social apps, the data we obtain is\n\nmostly image data, so we introduced the OCR technology to identify text information\n\nin images.\n\n3 Data collection\nIn order to analyze the behavior patterns of social e-commerce, we developed an auxil-\n\niary tool for social e-commerce. In this tool, sellers on a social network are provided\n\nwith the independent running environment of social software and the automatic auxil-\n\niary ability, and the information acquisition module of the auxiliary process is used to\n\ncollect the social information published by sellers on a social network, which is\n\nuploaded to the background server for model training. We provided this tool to nearly\n\n10,000 sellers on a social network who participated in the experiment to obtain their\n\nsocial information in their e-commerce activities.\n\n3.1 Overall structure\n\nThe whole data collection scheme is mainly composed of two parts: intelligent space\n\napp and background server. The overall architecture is shown in Fig. 1. Intelligent\n\nspace app is deployed in the mobile phones of sellers on a social network and imple-\n\nmented based on the application layer of the Android platform, providing sellers on a\n\nsocial network with a secure container for the independent operation of social software.\n\nThe app contains the automatic assistant module, which provides the automatic assist-\n\nant capability of various business processes for seller, and collects the social informa-\n\ntion in the auxiliary process through the information grasping module. The collected\n\ninformation is cached and uploaded locally through the information collection service.\n\nCui et al. EURASIP Journal on Image and Video Processing (2021) 2021:4 Page 3 of 12\n\n\n\nThe background server is responsible for receiving the collected data uploaded by the\n\nintelligent space, preprocessing the data first, and then classifying the social e-\n\ncommerce through the data based on the machine learning classification model, and fi-\n\nnally storing the classification results.\n\n3.1.1 Security container\n\nThe security container is designed to allow social software to run independently with-\n\nout modifying the OS or gaining root privileges. The basic principle of its realization is\n\nto create an independent container process; load APK file of social software dynamic-\n\nally; monitor and intercept process communication interface such as Binder IPC\n\nthrough Libc hook, Java reflection, dynamic proxy, and other technical means; and col-\n\nlect social information through an automatic assistant module. The main part of the\n\ncontainer is composed of an application layer module and a service layer module.\n\nThe application layer module is responsible for the process startup and execution of\n\nsocial software, and its main functions include three parts.\n\n3.1.1.1 Interactive interception The application layer module intercepts the inter-\n\naction between the application process and the underlying system in the container and\n\nmodifies the calling logic. By hook or dynamic proxy of system library API and Binder\n\ncommunication interface, the application layer module blocks all interfaces that interact\n\nwith the system during the execution of social software and controls the process\n\nboundary of interaction between social applications and system services.\n\n3.1.1.2 Social information collection The loading of the automatic auxiliary module\n\nby social software is realized when initializing the process of social application.\n\nThe application layer module injects the corresponding plugins in the automatic\n\nassistant module into the social application process. The automatic assistance mod-\n\nule provides a number of e-commerce auxiliary functions for sellers on a social\n\nnetwork, including customer acquisition, social customer relationship management\n\nLinux Kernel\n\nBinder Mode\n\nIntelligent Space\n\nService Layer Mode\nAMS Proxy PMS Proxy\n\nApplication Layer Mode\nSocial App\n\nInteractive \ninterception\n\nautomatic \nassistance \nmodule\n\nInformation\nCollection \n\nBinder IPC\n\nBinder IPC\n\nBinder \nIPC Backgroud\n\n Server\n\nInternet\n\nFig. 1 The overall architecture diagram of the data acquisition scheme\n\nCui et al. EURASIP Journal on Image and Video Processing (2021) 2021:4 Page 4 of 12\n\n\n\n(SCRM), group management, sales assistance, and daily affairs. Sellers on social\n\nnetworks publish social information with commercial attributes through auxiliary\n\nfunctions, then the automatic auxiliary module will automatically collect the social\n\ninformation and send it to the information collection service for processing.\n\n3.1.1.3 Local processing of social information When the information collection ser-\n\nvice receives the social information collected by the automatic auxiliary module, the\n\ndata will be compressed and encrypted in the local cache. The service then uploads the\n\ncollected data to the background server periodically through the timer, and HTTPS is\n\nused to ensure data transmission security.\n\nThe main function of the service layer module is to take over the call logic modified\n\nby the application layer module by simulating the system service modify the parameters\n\nin the communication process and finally call the real system service. The service layer\n\nmodule exists in the container as an independent process. It focuses on the simulation\n\nof activity manager service (AMS) and package manager service (PMS) and realizes the\n\nsupport of system services in the process of launching and running social software.\n\n3.1.2 Background server\n\nThe background server mainly realizes the machine learning model processing of the\n\ncollected social data, including the functions of data preprocessing, data training, classi-\n\nfication, and result storage. The core processing logic will be described in chapter 5.\n\n3.2 Key processes\n\nThere are four key processes in the process of social information collection and pro-\n\ncessing. They are social software process initialization, social software process\n\nIntelligent \nSpace App \nlaunched\n\nProcess Boundaries\n\nUser Process\n\nSocial software process \ninitialization\n\nlaunching social \nsoftware\n\ninject automatic \nauxiliary\n\nSocial software process \nexecution\n\nRun the plug-in\n\nCollect social \ninformation\n\nProcess Boundaries\n\nUser Process\n\nLocal processing of \nsocial information\n\nBatch upload \nprocessed social \n\ninformation\n\nEncrypt, compress \nand store social \n\ninformation\n\nInternet\n\nInformation collecting \nservice process\n\nBackground processing \nof social information\n\nReceiving social \ninformation\n\nPreprocessing social \ninformation\n\nThe Server\n\nMachine learning \ncategorizes social \n\ninformation\n\nStore the \nclassification results\n\nFig. 2 Key flow chart of data acquisition scheme\n\nCui et al. EURASIP Journal on Image and Video Processing (2021) 2021:4 Page 5 of 12\n\n\n\nexecution, local processing of social information, and background processing of social\n\ninformation. The complete process is shown in Fig. 2.\n\n3.2.1 Social software process initialization\n\nWhen launching social software, the intelligent space will first intercept the callback\n\nfunction of the life cycle of all its components, then realize the process loading of the\n\nautomatic auxiliary module during the process initialization.\n\n3.2.2 Social software process execution\n\nThe process execution is completed by the application layer module and service layer\n\nmodule together. Sellers on a social network use automatic auxiliary modules to\n\ncomplete business activities, trigger information capture module to collect social infor-\n\nmation, and send it to the information collection service for subsequent processing.\n\n3.2.3 Local processing of social information\n\nThe local processing of social information is mainly completed by the information col-\n\nlection service. In order to ensure the safe storage and transmission of the collected so-\n\ncial information, the information collection service first adopts the encryption and\n\ncompression method to realize the local security cache and then adopts the HTTPS se-\n\ncure communication and transmission protocol to upload the data.\n\n3.2.4 Background processing of social information\n\nThe background processing of social information is completed by the background ser-\n\nver. The server first receives the social information uploaded by the intelligent space,\n\nnext decrypts and decompresses the social information, cleans the plaintext data, uses\n\nthird-party OCR technology to identify text information in images, and adds it to the\n\nuser’s social information after simple data processing. Then, the classification of sellers\n\non a social network is realized through the data based on machine learning modeling.\n\nFinally, the classification results are stored in the target database.\n\n4 Methods\nTo classify the business attributes of social e-commerce based on the information of\n\nsellers on a social network, traditional feature matching scheme and classification clus-\n\ntering scheme based on machine learning can be used to establish the model. In this\n\nchapter, we introduce the scheme based on term frequency-inverse document fre-\n\nquency (TF-IDF) clustering and the classification scheme based on BERT.\n\n4.1 Feature classification and TF-IDF clustering\n\n4.1.1 Feature classification\n\nWe randomly select 5000 sellers on a social network from the data collected by the\n\nbackground server and extracted the text data of their social information for analysis.\n\nEach social e-commerce user contains an average of 50 social text data. Based on the\n\ncontent, we manually classify social e-commerce into 11 categories. With the help of e-\n\ncommerce platforms like JD.COM, 50–100 keywords are sorted out for each category,\n\nand these keywords are screened and expanded according to the language habits of\n\nCui et al. EURASIP Journal on Image and Video Processing (2021) 2021:4 Page 6 of 12\n\n\n\nsellers on a social network. On this basis, we collect all the social information of each\n\nsocial network seller, cut and remove word segmentation, and match the results with\n\nthe keywords of the selected 11 categories. The number of keywords that are matched\n\nis counted as the matching degree. According to the situation of different classification,\n\nthe threshold of matching degree is determined by manual screening of some results,\n\nand then all social e-commerce is classified according to the threshold. After\n\noptimization and verification, the accuracy of the classical feature matching scheme fi-\n\nnally reached 40%. However, due to the simplicity of the rules of the feature matching\n\nscheme, the small optimization space, the high misjudgment rate of the scheme, and\n\nthe large human intervention in the basic word segmentation process, it is difficult to\n\ncover various situations of social e-commerce due to the limitation of these basic key-\n\nwords, thus making it insensitive to the dynamic changes of new hot words of social e-\n\ncommerce.\n\n4.1.2 TF-IDF clustering\n\nTo achieve the goal of accurate classification of social e-commerce, we designed a\n\nscheme based on TF-IDF clustering. Term frequency-inverse document frequency (TF-\n\nIDF) is a commonly used weighted technique for information retrieval and text mining\n\nto evaluate the importance of a single word to a document in a set of documents or a\n\ncorpus. In this scheme, the social information of each social e-commerce user is\n\nmapped as one file set of TF-IDF, and all texts of all sellers on a social network are\n\nmapped as the whole corpus. The words with the highest frequency used by each social\n\ne-commerce user are the most representative words in this document and become key-\n\nwords. Category labels can be generated to calculate the probability that a document\n\nbelongs to a certain category using the naive Bayes algorithm formula. The advantages\n\nof TF-IDF clustering to achieve the classification of sellers on the social network in-\n\nclude the following: (1) clear mapping; (2) emphasize the weight of keywords and lower\n\nthe weight of non-keywords; (3) compared with other machine learning algorithms, the\n\ncharacteristic dimension of the model is greatly reduced to avoid the dimension disas-\n\nter; and (4) while improving the efficiency of classification calculation, ensure that the\n\nclassification effect has a good accuracy and recall rate. The architecture of the entire\n\nsolution is shown in Fig. 3.\n\nIn the text preprocessing stage, the first thing to do is to format the social informa-\n\ntion, mainly including deleting the space, deleting the newline character, merging the\n\nsocial e-commerce text, and so on, and finally getting the text to be processed for word\n\nsegmentation. In this scheme, we choose Jieba’s simplified mode for word segmenta-\n\ntion, then filter out the noise by filtering the stop words (e.g., yes, ah, etc.).\n\nIn the stage of establishing the vector space model, the first step is to load the train-\n\ning set and take the pre-processed social information of each social e-commerce user\n\nas a document. The next step is to generate a dictionary, by adding every word that ap-\n\npears in the training set to it, using the complete dictionary to calculate the TF-IDF\n\nvalue of each document. In this scheme, CountVectorizer and TfidfTransformer in Py-\n\nthon’s Scikit-Learn library are used. CountVectorizer is used to convert words in the\n\ntext into word frequency matrix, TfidfTransformer is used to count the TF-IDF value\n\nof each word in each document, and the top20 words in each document are taken as\n\nCui et al. EURASIP Journal on Image and Video Processing (2021) 2021:4 Page 7 of 12\n\n\n\nkeywords of sellers on a social network. After this step, the keywords with a large TF-\n\nIDF value in each document are the most representative words in the document, which\n\nbecome the keyword set of the social e-commerce user. Finally, the naive Bayes method\n\nis used to generate the category label, and the document vectors belonging to the same\n\ncategory in the TF-IDF matrix are added to form a matrix of m*n, where m represents\n\nthe number of categories and n represents the number of documents. The weight of\n\neach word is divided by the total weight of all words of the class, to calculate the prob-\n\nability that a document belongs to a certain class.\n\nIn the model optimization stage, we optimize the whole scheme model by adjusting\n\nthe stop word set, adjusting parameters (including CountVectorizer, TfidfTransformer\n\nclass construction parameters), and adjusting the category label generation method.\n\nThe main idea of TFIDF is if a word or phrase appears in an article with a high fre-\n\nquency of TF, and rarely appears in other articles, it is considered that the word or\n\nphrase has a good classification ability and is suitable for classification. TFIDF is actu-\n\nally: TF * IDF, TF is term frequency and IDF is inverse document frequency.\n\nIn a given document, word frequency refers to the frequency of a given word in the\n\ndocument. This number is a normalization of the number of words to prevent it from\n\nbeing biased towards long documents. For the word ti in a particular document, its im-\n\nportance can be expressed as:\n\ntf i; j ¼\nj D j\n\nj j : ti∈d j\n� � j\n\namong them:\n\n|D|: The total number of files in the corpus\n\n∣{j : ti ∈ dj}∣: The number of documents containing the term ti (i.e., the number of\n\ndocuments in ni, j ≠ 0). If the term is not in the corpus, it will cause the dividend to be\n\nzero, so it is generally used 1 + ∣ {j : ti ∈ dj}∣.\n\nand then:\n\n Social e-\ncommerce data\n\nData preparation\n\nFormat processing\n\nFilter stop words\n\nText preprocessing\n\nGenerate directory\n\nBuild the vector space and \nTF-IDF\n\nGenerate category tags\n\nBayesian classifier\n\nText articiple\n\nLoad training set \n\nBuild tf matrix \n\nbuild vector\n\nbuild matrix \n\nConditional probability \nmatrix\n\nModel optimization\n\nFig. 3 TF-IDF scheme framework\n\nCui et al. EURASIP Journal on Image and Video Processing (2021) 2021:4 Page 8 of 12\n\n\n\ntfidf i; j ¼ tf i; j � idf i\n\nA high word frequency in a particular document and a low document frequency of\n\nthe word in the entire document collection can produce a high-weight TF-IDF. There-\n\nfore, TF-IDF tends to filter out common words and keep important words.\n\n4.2 Classification scheme based on BERT\n\n4.2.1 Data label\n\nWe manually classify and mark the data of sellers on a social network according to the\n\ncharacteristics of the products. Classified labels include 38,970 items and 17 categories\n\nof data, including 3c, dress, food, car, house, beauty, makeup, training, jewelry, promo-\n\ntion, medicine and health, phone charge recharge, finance, card category, cigarettes, es-\n\nsays, and others. The pre-processing phase removes emojis, numbers, and spaces from\n\nthe text through Unicode encoding.\n\n4.2.2 Classification scheme\n\nIn the BERT model, Transformer, as an encoder-decoder model based on attention\n\nmechanism, solves the problem that RNN cannot deal with long-distance dependence\n\nand the model cannot be parallel, improving the performance of the model without re-\n\nducing the accuracy. At the same time, BERT introduced the shading language model\n\n(MLM, masked language model) and context prediction method, further enhance the\n\ntwo-way training of the ability of feature extraction and text. MLM uses Transformer\n\nencoders and bilateral contexts to predict random masked tokens to pre-train two-way\n\ntransformers. This makes BERT different from the GPT model, which can only conduct\n\none-way training and can better extract context information through feature fusion.\n\nAnaphase prediction is more embodied in QA and NLI. Therefore, we choose the\n\nBERT model based on the bidirectional coding technology of pre-training and attention\n\nmechanism to classify sellers on a social network.\n\nWe chose the official Chinese pre-training model of Google as the pre-training model\n\nof the experiment: BERT-Base which is Chinese simplified and traditional, 12-layer,\n\n768-hidden, 12-head, 110M parameters [16]. This pre-training model is obtained by\n\nGoogle’s unsupervised pre-training on a large-scale Chinese corpus. On this basis, we\n\nwill carry out fine-tuning to realize the classification model of sellers on a social net-\n\nwork. When dividing the data set, we divided 38,970 pieces of data into training set\n\nand verification set according to the ratio of 6:4, that is, 23,382 pieces of training set\n\nand 15,588 pieces of verification set.\n\n5 Results and discussion\n5.1 TF-IDF clustering scheme\n\nThe computer used in the experiment is configured with AMD Ryzen R5-4600H CPU,\n\n16G memory, and windows10 64bit operating system. First, the default construction\n\nparameters are used, and the average accuracy of each classification is 45.7%. Next, the\n\nparameters are adjusted through a genetic algorithm, and 100 rounds of genetic algo-\n\nrithm optimization are performed, then the average accuracy reached the highest value\n\nof 52.5%. In the process of genetic algorithm, statistical estimation of algorithm time is\n\nCui et al. EURASIP Journal on Image and Video Processing (2021) 2021:4 Page 9 of 12\n\n\n\nalso carried out. On average, on this data set, the running time of each round of the\n\nTF-IDF model is about 28 s.\n\nExperiments show that the accuracy of the TF-IDF clustering scheme has been\n\nimproved after optimization, and it has a certain reference value for the classifica-\n\ntion of sellers on a social network, but there is still a big gap from the accurate\n\nclassification. We found three reasons after analyzing the experimental results. (1)\n\nCompared to the feature matching scheme, the TF-IDF-based model is improved\n\nto some extent. However, the input of the model is still the result of direct word\n\nsegmentation, and more information is lost in the word segmentation process, such\n\nas the semantic information of previous and later texts and the repetition fre-\n\nquency of corpus, which are relatively important in the process of natural language\n\nprocessing. (2) The classification problem of sellers on a social network is compli-\n\ncated. This model does not analyze the correlation between words and is essen-\n\ntially an upgraded version of word frequency statistics, which makes it difficult to\n\nimprove the accuracy after reaching a certain value. (3) For the optimization of the\n\nmodel, only the parameters of the intermediate function are adjusted, and the\n\nmethod is not upgraded. Therefore, the machine learning scheme based on TF-IDF\n\nclustering cannot solve the problem of accurate classification of sellers on a social\n\nnetwork. In the next chapter, we will introduce a scheme based on deep learning\n\nto achieve the goal of classifying sellers on a social network.\n\n5.2 Classification scheme based on BERT\n\nText classification fine-tuning is to serialize the preprocessed text information\n\ntoken and input BERT, and select the final hidden state of the first token [CLS] as\n\na sentence vector to output to the full connection layer, and then output the prob-\n\nability of obtaining various labels corresponding to the text through the softmax\n\nlayer. The experimental schematic diagram is shown in Figs. 4 and 5. The max-\n\nimum length of the sequence (ma_seq_length) is set to 256 according to the actual\n\ntext length of the social information data set of the sellers on a social network and\n\nFig. 4 Text message token serialization\n\nFig. 5 Text classification BERT fine-tuning model structure diagram\n\nCui et al. EURASIP Journal on Image and Video Processing (2021) 2021:4 Page 10 of 12\n\n\n\n\n\nthe batch_size and learning rate adopt the official recommended values of 32 and\n\n2e−5. In addition, we also adjust the super parameter num_train_epochs and in-\n\ncrease the number of training epochs (num_train_epochs) from 3 to 9 to improve\n\nthe recognition rate of the model (Table 1). The results are shown in Table 2.\n\nWe select an additional 9500 text data of sellers on social networks and test the\n\nmodel after the same preprocessing. The accuracy rate is 90.5%, which is lower than\n\nthat of the verification set (96.2%). The reason may be that the data of the test set con-\n\ntains a large number of commodity terms not included in the corpus and training set,\n\nand the text description of these commodities is too colloquial. Sellers on a social net-\n\nwork often use colloquial words in the industry to replace the standard product names\n\nwhen releasing product information, such as “Bobo” instead of “Botox,” which to some\n\nextent limits the accuracy of text-based classification in the social e-commerce market\n\nscene.\n\n6 Conclusion\nThe classification model proposes in this paper achieves an accuracy of 90.5% in the\n\ntest data. However, there are still some problems such as non-standard description text.\n\nA corpus with a high correlation with a social e-commerce environment will be estab-\n\nlished in order to further improve the accuracy of social e-commerce classification. At\n\nthe same time, we will use the knowledge distillation technology to compress and refine\n\nthe existing model, so as to improve the model recognition rate while simplifying the\n\nmodel and improving the operational performance [16]. In addition, in view of the high\n\nlabor cost and time cost of large-scale data marking, the next step will be trying to\n\nmake full use of semi", "metadata_storage_path": "aHR0cHM6Ly9zdG9yYWdlYWNjb3VudGxpZ2lyay5ibG9iLmNvcmUud2luZG93cy5uZXQvcGFwZXIvczEzNjQwLTAyMC0wMDU0NS16LnBkZg2", "metadata_author": "Haoliang Cui", "metadata_title": "A classification method for social information of sellers on social network", "keyphrases": [ "2China Information Technology Security Evaluation Center", "Creative Commons Attribution 4.0 International License", "social network seller classification scheme", "other third party material", "2019 China social e-commerce industry", "mobile payment technology", "Creative Commons licence", "automated assistance capabilities", "social network apps", "social network applications", "original author(s", "RESEARCH Open Access", "different social software", "NLP classification model", "deep learning model", "Video Processing Cui", "social network use", "social information", "author information", "other means", "2021 Open Access", "systematic classification", "text information", "Haoliang Cui", "mobile phones", "assistance process", "Machine learning", "social relations", "social interaction", "The Author", "classification method", "accurate classification", "model training", "Shuai Shao2", "Shaozhang Niu", "Chengjie Shi3", "Lingyu Zhou1", "Full list", "hot topic", "recent years", "transaction money", "main activities", "operating environment", "final experiment", "continuous improvement", "one kind", "commodity trading", "rapid development", "ment report", "Internet society", "market size", "large scale", "high growth", "online retail", "trading activities", "same time", "uniform registration", "standardized terms", "product description", "great difficulty", "appropriate credit", "credit line", "intended use", "statutory regulation", "permitted use", "copyright holder", "EURASIP Journal", "User model", "38,970 sellers’ information", "user portrait", "doi.org", "orcid.org", "commerce platforms", "Correspondence", "shaoshuaib", "Beijing", "article", "Abstract", "number", "users", "traditional", "merchandise", "server", "data", "picture", "help", "OCR", "BERT", "accuracy", "Keywords", "1 Introduction", "employees", "percent", "billion", "Taobao", "content", "purchase", "sale", "goods", "products", "paper", "sharing", "adaptation", "distribution", "reproduction", "medium", "source", "link", "changes", "images", "permission", "creativecommons", "licenses", "crossmark", "dropout full connection layer", "four benchmark text classifications", "content curation social network", "neural network language model", "various benchmark tests", "Tencent AI Lab", "long short-term memory", "Most existing studies", "tional neural network", "recurrent neural network", "one convolution layer", "one row vector", "Natural language processing", "different semantic environments", "end classification recognition", "multi- classification task", "sharing information mechanism", "long-distance text dependency", "bidirectional splicing method", "social content data", "e-commerce business classification", "social network analysis", "multimedia text data", "long sequence training", "large-scale text corpus", "text classification research", "NLP correlation algorithm", "double-layer two-way LSTM", "softmax layer", "user-generated content", "social data", "network structure", "semantic information", "social networks", "long-term research", "Video Processing", "social links", "one word", "sequential data", "2.2 User analysis", "BERT model", "Related work", "present stage", "good results", "extraction capacity", "control units", "original basis", "widespread phenomenon", "guage modeling", "feature-based form", "train- ing", "downstream tasks", "context words", "feature extractor", "feature fusion", "important part", "unified framework", "picture data", "word embedding", "Word vectors", "classification accuracy", "OCR algorithm", "gradient disappearance", "output gate", "RNN algorithm", "38,970 sellers", "addition", "order", "NNLM", "Bengio", "Researchers", "classifier", "Kim", "CNN", "study", "limitation", "fore", "kind", "focus", "academia", "variation", "problem", "explosion", "Liu", "al.", "polysemy", "Peters", "dings", "ELMO", "impact", "grammatical", "ability", "features", "Transformer", "Devlin", "Cui", "Image", "Page", "Wu", "CCSN", "new online social network user", "air quality evaluation model", "machine learning classification model", "support vector machine", "mining user-generated content", "garlic industry chain", "Garlic planting management", "principal component analysis", "automatic assistant module", "various business processes", "independent running environment", "BP neural network", "ga-bp hybrid algorithm", "process communication interface", "file rating model", "social con- nections", "social informa- tion", "information acquisition module", "information grasping module", "cial network structure", "information collection service", "data analysis platform", "big data platform", "complicated user data", "data collection scheme", "independent container process", "users’ historical preferences", "Intelligent space app", "classification results", "ian model", "latent model", "Android platform", "independent operation", "consumption preferences", "genetic algorithm", "3 Data collection", "auxiliary process", "Overall structure", "APK file", "social apps", "social e-commerce", "social software", "information sharing", "secure container", "Security container", "multilevel LDA", "potential interest", "text descriptions", "future behavior", "near future", "price control", "fuzzy theory", "two methods", "ory relations", "English news", "combin- ation", "OCR technology", "behavior patterns", "iary ability", "background server", "two parts", "overall architecture", "application layer", "ant capability", "root privileges", "basic principle", "Binder IPC", "image data", "commerce activities", "iary tool", "MLLDA", "time", "Malli", "large", "terms", "Chen", "diction", "storage", "pretreatment", "knowledge", "Yin", "field", "combination", "tors", "sentences", "sellers", "experiment", "Fig.", "OS", "realization", "load", "ally", "intercept", "1.1", "interception automatic assistance module Information Collection Binder IPC Binder IPC Binder", "social customer relationship management Linux Kernel Binder Mode", "AMS Proxy PMS Proxy Application Layer Mode Social App Interactive", "Intelligent Space Service Layer Mode", "social information Process Boundaries User Process", "machine learning model processing", "Binder communication interface", "IPC Backgroud Server", "data acquisition scheme Cui", "social software process initialization", "Interactive interception", "application layer module", "Social software process execution", "Social information collection", "service layer module", "automatic auxiliary module", "Space App", "social application process", "other technical means", "overall architecture diagram", "activity manager service", "package manager service", "sales assistance", "data transmission security", "dynamic proxy", "system library API", "customer acquisition", "four key processes", "core processing logic", "group management", "communication process", "social applications", "social network", "3.2 Key processes", "process startup", "independent process", "system service", "calling logic", "Local processing", "call logic", "data preprocessing", "data training", "auxiliary functions", "Java reflection", "main part", "three parts", "inter- action", "underlying system", "corresponding plugins", "daily affairs", "commercial attributes", "local cache", "main function", "result storage", "Batch upload", "Libc hook", "container", "interfaces", "boundary", "interaction", "loading", "Internet", "SCRM", "timer", "HTTPS", "parameters", "real", "simulation", "support", "fication", "chapter", "Encrypt", "1.2", "Machine learning categorizes social information", "traditional feature matching scheme", "3.2.1 Social software process initialization", "machine learning modeling", "Key flow chart", "automatic auxiliary modules", "third-party OCR technology", "local security cache", "complete business activities", "information capture module", "social information Preprocessing", "social network seller", "simple data processing", "50 social text data", "social e-commerce user", "service process", "complete process", "service layer", "matching degree", "tering scheme", "process loading", "4.1 Feature classification", "4.1.1 Feature classification", "business attributes", "classification scheme", "local processing", "plaintext data", "Background processing", "subsequent processing", "intelligent space", "callback function", "life cycle", "safe storage", "compression method", "cure communication", "target database", "TF-IDF) clustering", "TF-IDF clustering", "JD.COM", "language habits", "word segmentation", "manual screening", "classification clus", "different classification", "The Server", "transmission protocol", "components", "Sellers", "encryption", "next", "4 Methods", "quency", "analysis", "average", "11 categories", "50–100 keywords", "category", "basis", "situation", "threshold", "3.2.2", "other machine learning algorithms", "naive Bayes algorithm formula", "classical feature matching scheme", "basic word segmentation process", "Term frequency-inverse document frequency", "naive Bayes method", "large human intervention", "high misjudgment rate", "basic key- words", "word segmenta- tion", "one file set", "new hot words", "small optimization space", "vector space model", "text preprocessing stage", "word frequency matrix", "model optimization stage", "social e-commerce text", "highest frequency", "recall rate", "single word", "various situations", "dynamic changes", "weighted technique", "information retrieval", "text mining", "clear mapping", "characteristic dimension", "entire solution", "first thing", "newline character", "simplified mode", "Scikit-Learn library", "keyword set", "m*n", "scheme model", "representative words", "stop words", "top20 words", "Category labels", "classification calculation", "classification effect", "same category", "4.1.2 TF-IDF clustering", "TF-IDF matrix", "first step", "next step", "good accuracy", "complete dictionary", "TF-IDF value", "document vectors", "total weight", "verification", "simplicity", "rules", "goal", "importance", "documents", "corpus", "texts", "probability", "advantages", "keywords", "lower", "efficiency", "architecture", "Jieba", "noise", "pears", "training", "CountVectorizer", "TfidfTransformer", "thon", "categories", "Conditional probability matrix Model optimization", "category label generation method", "3 TF-IDF scheme framework Cui", "official Chinese pre-training model", "phone charge recharge", "random masked tokens", "bidirectional coding technology", "vector build matrix", "context prediction method", "shading language model", "masked language model", "large-scale Chinese corpus", "entire document collection", "class construction parameters", "inverse document frequency", "low document frequency", "Load training set", "good classification ability", "high word frequency", "category tags", "4.2 Classification scheme", "card category", "4.2.2 Classification scheme", "Data label", "tf matrix", "vector space", "context information", "Anaphase prediction", "encoder-decoder model", "GPT model", "classification model", "particular document", "main idea", "other articles", "Format processing", "Bayesian classifier", "high-weight TF-IDF", "Classified labels", "promo- tion", "pre-processing phase", "Unicode encoding", "attention mechanism", "long-distance dependence", "feature extraction", "bilateral contexts", "two-way transformers", "traditional, 12-layer", "110M parameters", "data set", "two-way training", "one-way training", "term frequency", "commerce data", "Data preparation", "stop word", "Text preprocessing", "Text articiple", "common words", "important words", "long documents", "j � idf", "total number", "phrase", "normalization", "portance", "D|", "files", "dj", "dividend", "Filter", "directory", "characteristics", "38,970 items", "17 categories", "3c", "dress", "food", "house", "beauty", "makeup", "jewelry", "medicine", "health", "finance", "cigarettes", "others", "emojis", "numbers", "spaces", "RNN", "performance", "MLM", "encoders", "QA", "NLI", "Google", "BERT-Base", "hidden", "fine-tuning", "38,970 pieces", "4.2.1", "Text classification BERT fine-tuning model structure diagram Cui", "AMD Ryzen R5-4600H CPU", "windows10 64bit operating system", "Text message token serialization", "discussion 5.1 TF-IDF clustering scheme", "Text classification fine-tuning", "experimental schematic diagram", "direct word segmentation", "word frequency statistics", "final hidden state", "Tokels Tok1 Tok10", "official recommended values", "feature matching scheme", "text information token", "full connection layer", "additional 9500 text data", "natural language processing", "machine learning scheme", "word segmentation process", "social information data", "5.2 Classification scheme", "text length", "text description", "input BERT", "first token", "TF-IDF model", "classification problem", "deep learning", "learning rate", "TF-IDF-based model", "training set", "16G memory", "default construction", "genetic algo", "statistical estimation", "classifica- tion", "big gap", "three reasons", "later texts", "upgraded version", "intermediate function", "next chapter", "sentence vector", "various labels", "imum length", "super parameter", "training epochs", "recognition rate", "same preprocessing", "commodity terms", "experimental results", "verification set", "highest value", "reference value", "average accuracy", "accuracy rate", "running time", "large number", "rithm optimization", "algorithm", "5 Results", "ratio", "23,382 pieces", "15,588 pieces", "computer", "100 rounds", "28 s", "Experiments", "extent", "previous", "correlation", "words", "method", "preprocessed", "Figs.", "sequence", "actual", "batch_size", "train_epochs", "Table", "test", "commodities", "social net- work", "social e-commerce market", "standard description text", "social e-commerce environment", "knowledge distillation technology", "standard product names", "large-scale data marking", "social e-commerce classification", "model recognition rate", "product information", "test data", "text-based classification", "colloquial words", "existing model", "operational performance", "labor cost", "time cost", "full use", "high correlation", "industry", "Bobo", "Botox", "scene", "6 Conclusion", "problems", "view", "semi" ], "merged_content": "\nRESEARCH Open Access\n\nA classification method for social\ninformation of sellers on social network\nHaoliang Cui1, Shuai Shao2* , Shaozhang Niu1, Chengjie Shi3 and Lingyu Zhou1\n\n* Correspondence: shaoshuaib@163.\ncom\n2China Information Technology\nSecurity Evaluation Center, Beijing\n100085, China\nFull list of author information is\navailable at the end of the article\n\nAbstract\n\nSocial e-commerce has been a hot topic in recent years, with the number of users\nincreasing year by year and the transaction money exploding. Unlike traditional e-\ncommerce, the main activities of social e-commerce are on social network apps. To\nclassify sellers by the merchandise, this article designs and implements a social\nnetwork seller classification scheme. We develop an app, which runs on the mobile\nphones of the sellers and provides the operating environment and automated\nassistance capabilities of social network applications. The app can collect social\ninformation published by the sellers during the assistance process, uploads to the\nserver to perform model training on the data. We collect 38,970 sellers’ information,\nextract the text information in the picture with the help of OCR, and establish a\ndeep learning model based on BERT to classify the merchandise of sellers. In the\nfinal experiment, we achieve an accuracy of more than 90%, which shows that the\nmodel can accurately classify sellers on a social network.\n\nKeywords: User model, Machine learning, Social e-commerce\n\n1 Introduction\nWith the continuous improvement of social network and mobile payment technology,\n\none kind of commodity trading based on social relations called social e-commerce is in\n\nrapid development. According to the 2019 China social e-commerce industry develop-\n\nment report released by the Internet society of China, the number of employees of so-\n\ncial e-commerce in China is expected to reach 48.01 million in 2019, up by 58.3\n\npercent year on year, and the market size is expected to reach 2060.58 billion yuan, up\n\nby 63.2% year on year. Social e-commerce has become a large scale, and the high\n\ngrowth cannot be ignored. Different from e-commerce platforms such as Taobao, so-\n\ncial e-commerce is at the end of online retail. It carries out trading activities through\n\nsocial software and uses social interaction, user generated content and other means to\n\nassist the purchase and sale of goods. At the same time, sellers on social network use\n\ndifferent social software without uniform registration, have no systematic classification\n\nof products for sale, and there are no standardized terms for product description.\n\nThese bring great difficulty to the accurate classification of user portrait. This paper\n\nproposes a method based on the NLP classification model, which can realize accurate\n\n© The Author(s). 2021 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the\noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or\nother third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit\nline to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by\nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a\ncopy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\n\nEURASIP Journal on Image\nand Video Processing\n\nCui et al. EURASIP Journal on Image and Video Processing (2021) 2021:4 \nhttps://doi.org/10.1186/s13640-020-00545-z\n\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13640-020-00545-z&domain=pdf\nhttp://orcid.org/0000-0001-9638-0201\nmailto:shaoshuaib@163.com\nmailto:shaoshuaib@163.com\nhttp://creativecommons.org/licenses/by/4.0/\n\n\nbusiness classification of social e-commerce based on social information of social e-\n\ncommerce. This method analyzes 38,970 sellers on social networks and establishes a\n\ndeep learning model based on BERT to accurately classify the merchandise of sellers.\n\nIn addition, we introduced the OCR algorithm to extract the text information in the\n\npicture and superimposed it on the social content data, which effectively improved the\n\nclassification accuracy. The final experiment shows that the measured accuracy is more\n\nthan 90%.\n\n2 Related work\n2.1 Natural language processing\n\nIn order to analyze e-commerce business classification based on social data of sellers on a\n\nsocial network, the text needs to be analyzed based on the NLP correlation algorithm.\n\nThe rapid development of NLP at the present stage is due to the neural network language\n\nmodel (NNLM) Bengio et al. [1] proposed in 2003. Researchers have been trying to realize\n\nthe end-to-end classification recognition by using a neural network as a classifier in the\n\ntext classification research based on word embedding. Kim first introduces the convolu-\n\ntional neural network (CNN) into the study of text classification. The network structure is\n\na dropout full connection layer and a softmax layer connected after one convolution layer\n\n[2]. Although this algorithm achieves good results in various benchmark tests, it cannot\n\nobtain long-distance text dependency due to the limitation of network structure. There-\n\nfore, Tencent AI Lab proposed DPCNN, which further enhanced the extraction capacity\n\nof long-distance text dependency by deepening CNN [3].\n\nSocial content data includes multimedia text data and picture data. With the help of\n\nOCR, we extract the text in the picture and convert the picture data into text data. Text\n\nis a kind of sequential data, and the classification of it by recurrent neural network\n\n(RNN) has been the focus of long-term research in academia [4]. As a variation of\n\nRNN, long short-term memory (LSTM) adds control units such as forgetting gate, in-\n\nput gate, and output gate on the original basis, which solves the problem of gradient\n\nexplosion and gradient disappearance in the long sequence training of RNN and pro-\n\nmotes the use of RNN [5]. By introducing the sharing information mechanism, Liu\n\net al. further improved the accuracy of the RNN algorithm in the text multi-\n\nclassification task and achieved good results in four benchmark text classifications [6].\n\nHowever, Word vectors cannot be constructed in Word embedding to solve the\n\nproblem of polysemy. Even though different semantic environments are considered\n\nduring training, the result of training is still one word corresponding to one row vector.\n\nConsidering the widespread phenomenon of polysemy, Peters et al. propose embed-\n\ndings from language model (ELMO) to address the impact of polysemy on natural lan-\n\nguage modeling [7]. ELMO uses a feature-based form of pre-training. First, two-way\n\nLSTM is used to pre-train the corpus, and then word embedding resulting from train-\n\ning is adjusted by double-layer two-way LSTM when processing downstream tasks to\n\nadd more grammatical and semantic information according to the context words.\n\nThe ability of ELMO to extract features is limited for choosing LSTM as the feature\n\nextractor instead of Transformer [8], and ELMO’s bidirectional splicing method is also\n\nweak in feature fusion. Therefore, Devlin et al. propose the BERT model, taking Trans-\n\nformer as a feature extractor to pre-train large-scale text corpus [9].\n\nCui et al. EURASIP Journal on Image and Video Processing (2021) 2021:4 Page 2 of 12\n\n\n\n2.2 User analysis of social networks\n\nUser analysis is an important part of social network analysis. Most existing studies use\n\nuser-generated content or social links between users to simulate users. Wu et al. mod-\n\neled users on the content curation social network (CCSN) in the unified framework by\n\nmining user-generated content and social links [10]. They proposed a potential Bayes-\n\nian model, multilevel LDA (MLLDA), that could represent users of potential interest\n\nfound in social links formed by text descriptions contributed by users and information\n\nsharing. In 2017, Wu et al. proposed a latent model [11], trying to explain how the so-\n\ncial network structure and users’ historical preferences change over time affect each\n\nuser’s future behavior and predict each user’s consumption preferences and social con-\n\nnections in the near future. Malli et al. proposed a new online social network user pro-\n\nfile rating model [12], which solved the problem of large and complicated user data. In\n\nterms of data analysis platform, Chen et al. [13] developed a big data platform for the\n\nstudy of the garlic industry chain. Garlic planting management, price control, and pre-\n\ndiction were realized through data collection, storage, and pretreatment. Ning et al.\n\n[14] designed a ga-bp hybrid algorithm based on the fuzzy theory and constructed an\n\nair quality evaluation model by combining the knowledge of BP neural network, genetic\n\nalgorithm, and fuzzy theory. Yin et al. [15] studied two methods of extracting supervis-\n\nory relations and applied them to the field of English news. One is the combination of\n\nsupport vector machine and principal component analysis, and the other is the combin-\n\nation of support vector machine and CNN, which can extract high-quality feature vec-\n\ntors from sentences of support vector machine. In the social apps, the data we obtain is\n\nmostly image data, so we introduced the OCR technology to identify text information\n\nin images.\n\n3 Data collection\nIn order to analyze the behavior patterns of social e-commerce, we developed an auxil-\n\niary tool for social e-commerce. In this tool, sellers on a social network are provided\n\nwith the independent running environment of social software and the automatic auxil-\n\niary ability, and the information acquisition module of the auxiliary process is used to\n\ncollect the social information published by sellers on a social network, which is\n\nuploaded to the background server for model training. We provided this tool to nearly\n\n10,000 sellers on a social network who participated in the experiment to obtain their\n\nsocial information in their e-commerce activities.\n\n3.1 Overall structure\n\nThe whole data collection scheme is mainly composed of two parts: intelligent space\n\napp and background server. The overall architecture is shown in Fig. 1. Intelligent\n\nspace app is deployed in the mobile phones of sellers on a social network and imple-\n\nmented based on the application layer of the Android platform, providing sellers on a\n\nsocial network with a secure container for the independent operation of social software.\n\nThe app contains the automatic assistant module, which provides the automatic assist-\n\nant capability of various business processes for seller, and collects the social informa-\n\ntion in the auxiliary process through the information grasping module. The collected\n\ninformation is cached and uploaded locally through the information collection service.\n\nCui et al. EURASIP Journal on Image and Video Processing (2021) 2021:4 Page 3 of 12\n\n\n\nThe background server is responsible for receiving the collected data uploaded by the\n\nintelligent space, preprocessing the data first, and then classifying the social e-\n\ncommerce through the data based on the machine learning classification model, and fi-\n\nnally storing the classification results.\n\n3.1.1 Security container\n\nThe security container is designed to allow social software to run independently with-\n\nout modifying the OS or gaining root privileges. The basic principle of its realization is\n\nto create an independent container process; load APK file of social software dynamic-\n\nally; monitor and intercept process communication interface such as Binder IPC\n\nthrough Libc hook, Java reflection, dynamic proxy, and other technical means; and col-\n\nlect social information through an automatic assistant module. The main part of the\n\ncontainer is composed of an application layer module and a service layer module.\n\nThe application layer module is responsible for the process startup and execution of\n\nsocial software, and its main functions include three parts.\n\n3.1.1.1 Interactive interception The application layer module intercepts the inter-\n\naction between the application process and the underlying system in the container and\n\nmodifies the calling logic. By hook or dynamic proxy of system library API and Binder\n\ncommunication interface, the application layer module blocks all interfaces that interact\n\nwith the system during the execution of social software and controls the process\n\nboundary of interaction between social applications and system services.\n\n3.1.1.2 Social information collection The loading of the automatic auxiliary module\n\nby social software is realized when initializing the process of social application.\n\nThe application layer module injects the corresponding plugins in the automatic\n\nassistant module into the social application process. The automatic assistance mod-\n\nule provides a number of e-commerce auxiliary functions for sellers on a social\n\nnetwork, including customer acquisition, social customer relationship management\n\nLinux Kernel\n\nBinder Mode\n\nIntelligent Space\n\nService Layer Mode\nAMS Proxy PMS Proxy\n\nApplication Layer Mode\nSocial App\n\nInteractive \ninterception\n\nautomatic \nassistance \nmodule\n\nInformation\nCollection \n\nBinder IPC\n\nBinder IPC\n\nBinder \nIPC Backgroud\n\n Server\n\nInternet\n\nFig. 1 The overall architecture diagram of the data acquisition scheme\n\nCui et al. EURASIP Journal on Image and Video Processing (2021) 2021:4 Page 4 of 12\n\n\n\n(SCRM), group management, sales assistance, and daily affairs. Sellers on social\n\nnetworks publish social information with commercial attributes through auxiliary\n\nfunctions, then the automatic auxiliary module will automatically collect the social\n\ninformation and send it to the information collection service for processing.\n\n3.1.1.3 Local processing of social information When the information collection ser-\n\nvice receives the social information collected by the automatic auxiliary module, the\n\ndata will be compressed and encrypted in the local cache. The service then uploads the\n\ncollected data to the background server periodically through the timer, and HTTPS is\n\nused to ensure data transmission security.\n\nThe main function of the service layer module is to take over the call logic modified\n\nby the application layer module by simulating the system service modify the parameters\n\nin the communication process and finally call the real system service. The service layer\n\nmodule exists in the container as an independent process. It focuses on the simulation\n\nof activity manager service (AMS) and package manager service (PMS) and realizes the\n\nsupport of system services in the process of launching and running social software.\n\n3.1.2 Background server\n\nThe background server mainly realizes the machine learning model processing of the\n\ncollected social data, including the functions of data preprocessing, data training, classi-\n\nfication, and result storage. The core processing logic will be described in chapter 5.\n\n3.2 Key processes\n\nThere are four key processes in the process of social information collection and pro-\n\ncessing. They are social software process initialization, social software process\n\nIntelligent \nSpace App \nlaunched\n\nProcess Boundaries\n\nUser Process\n\nSocial software process \ninitialization\n\nlaunching social \nsoftware\n\ninject automatic \nauxiliary\n\nSocial software process \nexecution\n\nRun the plug-in\n\nCollect social \ninformation\n\nProcess Boundaries\n\nUser Process\n\nLocal processing of \nsocial information\n\nBatch upload \nprocessed social \n\ninformation\n\nEncrypt, compress \nand store social \n\ninformation\n\nInternet\n\nInformation collecting \nservice process\n\nBackground processing \nof social information\n\nReceiving social \ninformation\n\nPreprocessing social \ninformation\n\nThe Server\n\nMachine learning \ncategorizes social \n\ninformation\n\nStore the \nclassification results\n\nFig. 2 Key flow chart of data acquisition scheme\n\nCui et al. EURASIP Journal on Image and Video Processing (2021) 2021:4 Page 5 of 12\n\n\n\nexecution, local processing of social information, and background processing of social\n\ninformation. The complete process is shown in Fig. 2.\n\n3.2.1 Social software process initialization\n\nWhen launching social software, the intelligent space will first intercept the callback\n\nfunction of the life cycle of all its components, then realize the process loading of the\n\nautomatic auxiliary module during the process initialization.\n\n3.2.2 Social software process execution\n\nThe process execution is completed by the application layer module and service layer\n\nmodule together. Sellers on a social network use automatic auxiliary modules to\n\ncomplete business activities, trigger information capture module to collect social infor-\n\nmation, and send it to the information collection service for subsequent processing.\n\n3.2.3 Local processing of social information\n\nThe local processing of social information is mainly completed by the information col-\n\nlection service. In order to ensure the safe storage and transmission of the collected so-\n\ncial information, the information collection service first adopts the encryption and\n\ncompression method to realize the local security cache and then adopts the HTTPS se-\n\ncure communication and transmission protocol to upload the data.\n\n3.2.4 Background processing of social information\n\nThe background processing of social information is completed by the background ser-\n\nver. The server first receives the social information uploaded by the intelligent space,\n\nnext decrypts and decompresses the social information, cleans the plaintext data, uses\n\nthird-party OCR technology to identify text information in images, and adds it to the\n\nuser’s social information after simple data processing. Then, the classification of sellers\n\non a social network is realized through the data based on machine learning modeling.\n\nFinally, the classification results are stored in the target database.\n\n4 Methods\nTo classify the business attributes of social e-commerce based on the information of\n\nsellers on a social network, traditional feature matching scheme and classification clus-\n\ntering scheme based on machine learning can be used to establish the model. In this\n\nchapter, we introduce the scheme based on term frequency-inverse document fre-\n\nquency (TF-IDF) clustering and the classification scheme based on BERT.\n\n4.1 Feature classification and TF-IDF clustering\n\n4.1.1 Feature classification\n\nWe randomly select 5000 sellers on a social network from the data collected by the\n\nbackground server and extracted the text data of their social information for analysis.\n\nEach social e-commerce user contains an average of 50 social text data. Based on the\n\ncontent, we manually classify social e-commerce into 11 categories. With the help of e-\n\ncommerce platforms like JD.COM, 50–100 keywords are sorted out for each category,\n\nand these keywords are screened and expanded according to the language habits of\n\nCui et al. EURASIP Journal on Image and Video Processing (2021) 2021:4 Page 6 of 12\n\n\n\nsellers on a social network. On this basis, we collect all the social information of each\n\nsocial network seller, cut and remove word segmentation, and match the results with\n\nthe keywords of the selected 11 categories. The number of keywords that are matched\n\nis counted as the matching degree. According to the situation of different classification,\n\nthe threshold of matching degree is determined by manual screening of some results,\n\nand then all social e-commerce is classified according to the threshold. After\n\noptimization and verification, the accuracy of the classical feature matching scheme fi-\n\nnally reached 40%. However, due to the simplicity of the rules of the feature matching\n\nscheme, the small optimization space, the high misjudgment rate of the scheme, and\n\nthe large human intervention in the basic word segmentation process, it is difficult to\n\ncover various situations of social e-commerce due to the limitation of these basic key-\n\nwords, thus making it insensitive to the dynamic changes of new hot words of social e-\n\ncommerce.\n\n4.1.2 TF-IDF clustering\n\nTo achieve the goal of accurate classification of social e-commerce, we designed a\n\nscheme based on TF-IDF clustering. Term frequency-inverse document frequency (TF-\n\nIDF) is a commonly used weighted technique for information retrieval and text mining\n\nto evaluate the importance of a single word to a document in a set of documents or a\n\ncorpus. In this scheme, the social information of each social e-commerce user is\n\nmapped as one file set of TF-IDF, and all texts of all sellers on a social network are\n\nmapped as the whole corpus. The words with the highest frequency used by each social\n\ne-commerce user are the most representative words in this document and become key-\n\nwords. Category labels can be generated to calculate the probability that a document\n\nbelongs to a certain category using the naive Bayes algorithm formula. The advantages\n\nof TF-IDF clustering to achieve the classification of sellers on the social network in-\n\nclude the following: (1) clear mapping; (2) emphasize the weight of keywords and lower\n\nthe weight of non-keywords; (3) compared with other machine learning algorithms, the\n\ncharacteristic dimension of the model is greatly reduced to avoid the dimension disas-\n\nter; and (4) while improving the efficiency of classification calculation, ensure that the\n\nclassification effect has a good accuracy and recall rate. The architecture of the entire\n\nsolution is shown in Fig. 3.\n\nIn the text preprocessing stage, the first thing to do is to format the social informa-\n\ntion, mainly including deleting the space, deleting the newline character, merging the\n\nsocial e-commerce text, and so on, and finally getting the text to be processed for word\n\nsegmentation. In this scheme, we choose Jieba’s simplified mode for word segmenta-\n\ntion, then filter out the noise by filtering the stop words (e.g., yes, ah, etc.).\n\nIn the stage of establishing the vector space model, the first step is to load the train-\n\ning set and take the pre-processed social information of each social e-commerce user\n\nas a document. The next step is to generate a dictionary, by adding every word that ap-\n\npears in the training set to it, using the complete dictionary to calculate the TF-IDF\n\nvalue of each document. In this scheme, CountVectorizer and TfidfTransformer in Py-\n\nthon’s Scikit-Learn library are used. CountVectorizer is used to convert words in the\n\ntext into word frequency matrix, TfidfTransformer is used to count the TF-IDF value\n\nof each word in each document, and the top20 words in each document are taken as\n\nCui et al. EURASIP Journal on Image and Video Processing (2021) 2021:4 Page 7 of 12\n\n\n\nkeywords of sellers on a social network. After this step, the keywords with a large TF-\n\nIDF value in each document are the most representative words in the document, which\n\nbecome the keyword set of the social e-commerce user. Finally, the naive Bayes method\n\nis used to generate the category label, and the document vectors belonging to the same\n\ncategory in the TF-IDF matrix are added to form a matrix of m*n, where m represents\n\nthe number of categories and n represents the number of documents. The weight of\n\neach word is divided by the total weight of all words of the class, to calculate the prob-\n\nability that a document belongs to a certain class.\n\nIn the model optimization stage, we optimize the whole scheme model by adjusting\n\nthe stop word set, adjusting parameters (including CountVectorizer, TfidfTransformer\n\nclass construction parameters), and adjusting the category label generation method.\n\nThe main idea of TFIDF is if a word or phrase appears in an article with a high fre-\n\nquency of TF, and rarely appears in other articles, it is considered that the word or\n\nphrase has a good classification ability and is suitable for classification. TFIDF is actu-\n\nally: TF * IDF, TF is term frequency and IDF is inverse document frequency.\n\nIn a given document, word frequency refers to the frequency of a given word in the\n\ndocument. This number is a normalization of the number of words to prevent it from\n\nbeing biased towards long documents. For the word ti in a particular document, its im-\n\nportance can be expressed as:\n\ntf i; j ¼\nj D j\n\nj j : ti∈d j\n� � j\n\namong them:\n\n|D|: The total number of files in the corpus\n\n∣{j : ti ∈ dj}∣: The number of documents containing the term ti (i.e., the number of\n\ndocuments in ni, j ≠ 0). If the term is not in the corpus, it will cause the dividend to be\n\nzero, so it is generally used 1 + ∣ {j : ti ∈ dj}∣.\n\nand then:\n\n Social e-\ncommerce data\n\nData preparation\n\nFormat processing\n\nFilter stop words\n\nText preprocessing\n\nGenerate directory\n\nBuild the vector space and \nTF-IDF\n\nGenerate category tags\n\nBayesian classifier\n\nText articiple\n\nLoad training set \n\nBuild tf matrix \n\nbuild vector\n\nbuild matrix \n\nConditional probability \nmatrix\n\nModel optimization\n\nFig. 3 TF-IDF scheme framework\n\nCui et al. EURASIP Journal on Image and Video Processing (2021) 2021:4 Page 8 of 12\n\n\n\ntfidf i; j ¼ tf i; j � idf i\n\nA high word frequency in a particular document and a low document frequency of\n\nthe word in the entire document collection can produce a high-weight TF-IDF. There-\n\nfore, TF-IDF tends to filter out common words and keep important words.\n\n4.2 Classification scheme based on BERT\n\n4.2.1 Data label\n\nWe manually classify and mark the data of sellers on a social network according to the\n\ncharacteristics of the products. Classified labels include 38,970 items and 17 categories\n\nof data, including 3c, dress, food, car, house, beauty, makeup, training, jewelry, promo-\n\ntion, medicine and health, phone charge recharge, finance, card category, cigarettes, es-\n\nsays, and others. The pre-processing phase removes emojis, numbers, and spaces from\n\nthe text through Unicode encoding.\n\n4.2.2 Classification scheme\n\nIn the BERT model, Transformer, as an encoder-decoder model based on attention\n\nmechanism, solves the problem that RNN cannot deal with long-distance dependence\n\nand the model cannot be parallel, improving the performance of the model without re-\n\nducing the accuracy. At the same time, BERT introduced the shading language model\n\n(MLM, masked language model) and context prediction method, further enhance the\n\ntwo-way training of the ability of feature extraction and text. MLM uses Transformer\n\nencoders and bilateral contexts to predict random masked tokens to pre-train two-way\n\ntransformers. This makes BERT different from the GPT model, which can only conduct\n\none-way training and can better extract context information through feature fusion.\n\nAnaphase prediction is more embodied in QA and NLI. Therefore, we choose the\n\nBERT model based on the bidirectional coding technology of pre-training and attention\n\nmechanism to classify sellers on a social network.\n\nWe chose the official Chinese pre-training model of Google as the pre-training model\n\nof the experiment: BERT-Base which is Chinese simplified and traditional, 12-layer,\n\n768-hidden, 12-head, 110M parameters [16]. This pre-training model is obtained by\n\nGoogle’s unsupervised pre-training on a large-scale Chinese corpus. On this basis, we\n\nwill carry out fine-tuning to realize the classification model of sellers on a social net-\n\nwork. When dividing the data set, we divided 38,970 pieces of data into training set\n\nand verification set according to the ratio of 6:4, that is, 23,382 pieces of training set\n\nand 15,588 pieces of verification set.\n\n5 Results and discussion\n5.1 TF-IDF clustering scheme\n\nThe computer used in the experiment is configured with AMD Ryzen R5-4600H CPU,\n\n16G memory, and windows10 64bit operating system. First, the default construction\n\nparameters are used, and the average accuracy of each classification is 45.7%. Next, the\n\nparameters are adjusted through a genetic algorithm, and 100 rounds of genetic algo-\n\nrithm optimization are performed, then the average accuracy reached the highest value\n\nof 52.5%. In the process of genetic algorithm, statistical estimation of algorithm time is\n\nCui et al. EURASIP Journal on Image and Video Processing (2021) 2021:4 Page 9 of 12\n\n\n\nalso carried out. On average, on this data set, the running time of each round of the\n\nTF-IDF model is about 28 s.\n\nExperiments show that the accuracy of the TF-IDF clustering scheme has been\n\nimproved after optimization, and it has a certain reference value for the classifica-\n\ntion of sellers on a social network, but there is still a big gap from the accurate\n\nclassification. We found three reasons after analyzing the experimental results. (1)\n\nCompared to the feature matching scheme, the TF-IDF-based model is improved\n\nto some extent. However, the input of the model is still the result of direct word\n\nsegmentation, and more information is lost in the word segmentation process, such\n\nas the semantic information of previous and later texts and the repetition fre-\n\nquency of corpus, which are relatively important in the process of natural language\n\nprocessing. (2) The classification problem of sellers on a social network is compli-\n\ncated. This model does not analyze the correlation between words and is essen-\n\ntially an upgraded version of word frequency statistics, which makes it difficult to\n\nimprove the accuracy after reaching a certain value. (3) For the optimization of the\n\nmodel, only the parameters of the intermediate function are adjusted, and the\n\nmethod is not upgraded. Therefore, the machine learning scheme based on TF-IDF\n\nclustering cannot solve the problem of accurate classification of sellers on a social\n\nnetwork. In the next chapter, we will introduce a scheme based on deep learning\n\nto achieve the goal of classifying sellers on a social network.\n\n5.2 Classification scheme based on BERT\n\nText classification fine-tuning is to serialize the preprocessed text information\n\ntoken and input BERT, and select the final hidden state of the first token [CLS] as\n\na sentence vector to output to the full connection layer, and then output the prob-\n\nability of obtaining various labels corresponding to the text through the softmax\n\nlayer. The experimental schematic diagram is shown in Figs. 4 and 5. The max-\n\nimum length of the sequence (ma_seq_length) is set to 256 according to the actual\n\ntext length of the social information data set of the sellers on a social network and\n\nFig. 4 Text message token serialization\n\nFig. 5 Text classification BERT fine-tuning model structure diagram\n\nCui et al. EURASIP Journal on Image and Video Processing (2021) 2021:4 Page 10 of 12\n\n Tokels Tok1 Tok10 \n\n\n\nthe batch_size and learning rate adopt the official recommended values of 32 and\n\n2e−5. In addition, we also adjust the super parameter num_train_epochs and in-\n\ncrease the number of training epochs (num_train_epochs) from 3 to 9 to improve\n\nthe recognition rate of the model (Table 1). The results are shown in Table 2.\n\nWe select an additional 9500 text data of sellers on social networks and test the\n\nmodel after the same preprocessing. The accuracy rate is 90.5%, which is lower than\n\nthat of the verification set (96.2%). The reason may be that the data of the test set con-\n\ntains a large number of commodity terms not included in the corpus and training set,\n\nand the text description of these commodities is too colloquial. Sellers on a social net-\n\nwork often use colloquial words in the industry to replace the standard product names\n\nwhen releasing product information, such as “Bobo” instead of “Botox,” which to some\n\nextent limits the accuracy of text-based classification in the social e-commerce market\n\nscene.\n\n6 Conclusion\nThe classification model proposes in this paper achieves an accuracy of 90.5% in the\n\ntest data. However, there are still some problems such as non-standard description text.\n\nA corpus with a high correlation with a social e-commerce environment will be estab-\n\nlished in order to further improve the accuracy of social e-commerce classification. At\n\nthe same time, we will use the knowledge distillation technology to compress and refine\n\nthe existing model, so as to improve the model recognition rate while simplifying the\n\nmodel and improving the operational performance [16]. In addition, in view of the high\n\nlabor cost and time cost of large-scale data marking, the next step will be trying to\n\nmake full use of semi", "text": [ "Tokels Tok1 Tok10", "Published online: 14 January 2021" ], "layoutText": [ "{\"language\":\"en\",\"text\":\"Tokels Tok1 Tok10\",\"lines\":[{\"boundingBox\":[{\"x\":48,\"y\":114},{\"x\":170,\"y\":113},{\"x\":170,\"y\":141},{\"x\":48,\"y\":141}],\"text\":\"Tokels\"},{\"boundingBox\":[{\"x\":280,\"y\":113},{\"x\":371,\"y\":113},{\"x\":371,\"y\":141},{\"x\":280,\"y\":141}],\"text\":\"Tok1\"},{\"boundingBox\":[{\"x\":616,\"y\":113},{\"x\":732,\"y\":113},{\"x\":732,\"y\":141},{\"x\":616,\"y\":141}],\"text\":\"Tok10\"}],\"words\":[{\"boundingBox\":[{\"x\":53,\"y\":114},{\"x\":169,\"y\":115},{\"x\":168,\"y\":142},{\"x\":52,\"y\":142}],\"text\":\"Tokels\"},{\"boundingBox\":[{\"x\":285,\"y\":114},{\"x\":371,\"y\":113},{\"x\":371,\"y\":142},{\"x\":285,\"y\":142}],\"text\":\"Tok1\"},{\"boundingBox\":[{\"x\":619,\"y\":114},{\"x\":733,\"y\":114},{\"x\":733,\"y\":142},{\"x\":619,\"y\":142}],\"text\":\"Tok10\"}]}", "{\"language\":\"en\",\"text\":\"Published online: 14 January 2021\",\"lines\":[{\"boundingBox\":[{\"x\":0,\"y\":16},{\"x\":982,\"y\":17},{\"x\":982,\"y\":72},{\"x\":0,\"y\":70}],\"text\":\"Published online: 14 January 2021\"}],\"words\":[{\"boundingBox\":[{\"x\":1,\"y\":17},{\"x\":283,\"y\":17},{\"x\":283,\"y\":70},{\"x\":0,\"y\":69}],\"text\":\"Published\"},{\"boundingBox\":[{\"x\":293,\"y\":17},{\"x\":504,\"y\":17},{\"x\":505,\"y\":72},{\"x\":293,\"y\":70}],\"text\":\"online:\"},{\"boundingBox\":[{\"x\":514,\"y\":17},{\"x\":588,\"y\":17},{\"x\":589,\"y\":72},{\"x\":515,\"y\":72}],\"text\":\"14\"},{\"boundingBox\":[{\"x\":598,\"y\":17},{\"x\":827,\"y\":17},{\"x\":828,\"y\":73},{\"x\":599,\"y\":72}],\"text\":\"January\"},{\"boundingBox\":[{\"x\":837,\"y\":17},{\"x\":981,\"y\":18},{\"x\":982,\"y\":73},{\"x\":838,\"y\":73}],\"text\":\"2021\"}]}" ] }, { "@search.score": 2.4152527, "content": "\nRESEARCH Open Access\n\nAugmented reality virtual glasses try-on\ntechnology based on iOS platform\nBoping Zhang\n\nAbstract\n\nWith the development of e-commerce, network virtual try-on, as a new online shopping mode, fills the gap that\nthe goods cannot be tried on in traditional online shopping. In the work, we discussed augmented reality virtual\nglasses try-on technology on iOS platform to achieve optimal purchase of online glasses, improving try-on speed of\nvirtual glasses, user senses of reality, and immersion. Face information was collected by the input device-monocular\ncamera. After face detection by SVM classifier, the local face features were extracted by robust SIFT. Combined with\nSDM, the feature points were iteratively solved to obtain more accurate feature point alignment model. Through\nthe head pose estimation, the virtual model was accurately superimposed on the human face, thus realizing the\ntry-on of virtual glasses. The above research was applied in iOS glasses try-on APP system to design the try-on system\nof augmented reality virtual glasses on iOS mobile platform. It is proved that the method can achieve accurate\nidentification of face features and quick try-on of virtual glasses.\n\nKeywords: Virtual try-on, Virtual glasses, Augmented reality, Computer vision, Pose estimation, iOS\n\n1 Introduction\nNetwork virtual try-on is a new way of online shopping.\nWith the development of e-commerce, it broadens the\nexternal propaganda channels of merchants to enhance\nthe interaction between consumers and merchants.\nVirtual try-on fills the gap that the goods cannot be\ntried on in traditional online shopping. As an important\npart of network virtual try-on, virtual glasses try-on\ntechnology has become a key research issue in this field\nrecently [1–4]. During virtual glasses try-on process,\nconsumers can select their favorite glasses by compar-\ning the actual wearing effects of different glasses in the\nonline shopping. The research key of virtual glasses\ntry-on system is the rapid achievement of experiential\nonline shopping.\nAR (augmented reality) calculates the position and\n\nangle of camera image in real time while adding corre-\nsponding images. The virtual world scene is superim-\nposed on a screen in real world for real-time\ninteraction [5]. Using computer technology, AR simu-\nlates physical information (vision, sound, taste, touch,\netc.) that is difficult to experience within certain time\n\nand space of real world. After superimposition of phys-\nical information, the virtual information is perceived by\nhuman senses in real world, thus achieving sensory ex-\nperience beyond reality [6].\nBased on AR principle, virtual glasses try-on technol-\n\nogy achieves optimal purchase of user online glasses and\nquick try-on of virtual glasses, improving the senses of\nreality and immersion. Monocular camera is used as the\ninput device to discuss try-on technology of AR glasses\non iOS platform. Face information is collected by mon-\nocular camera. After face detection by SVM (support\nvector machine) classifier, the local features of faces are\nextracted by robust SIFT (scale-invariant feature trans-\nform). Combined with SDM (supervised descent\nmethod), the feature points were iteratively solved to ob-\ntain more accurate feature point alignment model.\nThrough the head pose estimation, the virtual glasses\nmodel was accurately superimposed on the human face,\nthus realizing the try-on of virtual glasses. The above re-\nsearch is applied in iOS glasses try-on APP system to de-\nsign the try-on system of AR glasses on iOS mobile\nplatform. It is proved that the method can achieve ac-\ncurate identification of face features and quick try-on of\nvirtual glasses.Correspondence: bopingzhang@yeah.net\n\nSchool of Information Engineer, Xuchang University, Xuchang 461000,\nHenan, China\n\nEURASIP Journal on Image\nand Video Processing\n\n© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0\nInternational License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and\nreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to\nthe Creative Commons license, and indicate if changes were made.\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 \nhttps://doi.org/10.1186/s13640-018-0373-8\n\n\n\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13640-018-0373-8&domain=pdf\nhttp://orcid.org/0000-0001-7835-7622\nmailto:bopingzhang@yeah.net\nhttp://creativecommons.org/licenses/by/4.0/\n\n\n2 Research status of network virtual try-on\ntechnology\nGlasses try-on system was first applied in the USA.\nGlasses companies such as Camirror, Smart Look, Ipoint\nKisok, and Xview pioneered the online try-on function [7].\nUsers freely feel the wearing effect, enhancing the online\nshopping experience. Recently, online try-on function is\nexplored by domestic and foreign glasses sellers, such as\nMeijing [8], Kede [9] and Biyao [10].\nVirtual glasses try-on system involves computer vision,\n\naugmented reality, and image processing technology.\nRecently, research hotspots are speed, experience, and\nimmersion of try-on. At present, research results can be di-\nvided into four categories, namely 2D image superposition,\n3D glasses superimposed on 2D face images, 3D face mod-\neling, and AR technology based on video stream [11–14].\nHuang [15] introduced virtual optician system based on\n\nvision, which detects user’s face before locating user’s eyes.\nThree points are selected from face and glasses images.\nTwo corresponding isosceles triangles are formed for af-\nfine transformation, thus estimating the pose and scale of\nface in real time. This method realizes real-time head mo-\ntion tracking. However, the glasses model easily produces\nunrealistic deformation, affecting the realism of the\nglasses.\nAR technology is also applied in the virtual glasses\n\ntry-on system. Cheng et al. [16] selected a monocular\nCCD (charge-coupled device) camera as the input sensor\nto propose AR technology design based on the inter-\naction of marker and face features. Virtual glasses try-on\nsystem is established based on Android mobile platform,\nachieving good results. During virtual try-on process, we\nuse 2D image overlay or 3D modeling approach. There\nare still different defects although all kinds of virtual\nglasses try-on techniques have certain advantages. The\nsuperposition of 2D images is unsatisfactory in the sense\nof reality. Besides, the 3D modeling takes too long to\nmeet the real-time requirements of online shopping.\n\nIn-depth research is required to realize accurate tracking\nand matching. These problems can be solved by\nAR-based glasses try-on technology to a large extent,\nthus providing new ideas for virtual try-on technology.\n\n3 Methods of face recognition\nIt is necessary to integrate virtual objects into real envir-\nonment for the application of AR technology in virtual\nglasses try-on system, wherein face recognition is the\nprecondition for virtual glasses try-on system. During\ntry-on process, it is necessary to detect the face in each\nframe of the video. However, the problems of posture, il-\nlumination, and occlusion can increase the omission and\nfalse ratios of face detection. The real time of detection\nis an important indicator of system performance to en-\nhance people’s experience senses.\nGeneral face recognition process consists of face de-\n\ntection, tracking, feature extraction, dimension reduc-\ntion, and matching recognition (see Fig. 1) [17].\nIn Fig. 1, face detection is the first step to realize face\n\nrecognition. Its purpose is to automatically find face re-\ngion in an input image. If there is a face area, the spe-\ncific location and range of face needs to be located. Face\ndetection is divided into image-based and video-based\ndetection. If the input is a still image, each image is de-\ntected; if the input is a video, face detection is performed\nthroughout the video sequence.\nFeature extraction is based on face detection, and the\n\ninput is the detected face image. Common features are\nLBP (local binary patterns), HOG (histogram of oriented\ngradient), Gabor, etc. HOG [18] describes the edge fea-\ntures. Due to insensitiveness to illumination changes and\nsmall displacements, it describes the overall and local in-\nformation of human face. LBP [19] shows the local tex-\nture changes of an image, with brightness invariance.\nGabo feature [20] captures the local structural content\nof spatial position, direction selectivity, and spatial fre-\nquency. It is suitable for description of human faces.\n\nFig. 1 Face recognition process\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 Page 2 of 19\n\n\n\n\n\nFeature dimension reduction is described as follows.\nFace feature is generally high-dimensional feature vector.\nFace recognition of high-dimensional feature vector\nincreases time and space complexity. Besides, it is difficult\nto effectively judge the description ability of high-dimen-\nsional face features. The high-dimensional face feature\nvector can be projected to the low-dimensional subspace.\nThe low-dimensional subspace information can complete\nface feature identification. After feature extraction, the ori-\nginal features are recombined to reduce vector dimension\nof face feature.\nAfter the previous links, we compare the existing tar-\n\ngets in face database and the faces to be identified based\non certain matching strategy, making final decision.\nMatching recognition can be represented by offline\nlearning and online matching models.\n\n3.1 SVM-based face detection\nFace detection is the premise of virtual glasses try-on\ntechnology. Recently, scholars proposed face detection\nmethods, such as neural network, SVM (support vector\nmachine), HMM (hidden Markov model), and AdaBoost.\nIn the work, the classic SVM algorithm is used for face\ndetection. SVM algorithm is a machine learning method\nbased on statistical theory. Figure 2 shows the network\nstructure of SVM [21]. SVM algorithm can be regarded\nas a three-layer feedforward neural network with a hid-\nden layer. Firstly, the input vector is mapped from\nlow-dimensional input space to the high-dimensional\nfeature space by nonlinear mapping. After that, the opti-\nmal hyperplane with the largest interval is constructed\nin the high-dimensional feature space.\nIt is denoted that the input vector of SVM x= (x1, x2,…, xn).\n\nEquation (1) shows the network output of output layer\nbased on x.\n\ny xð Þ ¼ sgn\nXN train\n\ni¼1\nyi∂\n\n�\ni K xi; x\n\n� �þ b�\n� �\n\nð1Þ\n\nwherein the inner product K(x(i), x) is a kernel function\nsatisfying the Mercer condition. Common kernel func-\ntions consist of polynomial, Gauss, and Sigmoid kernel\n\nfunctions. The Gaussian kernel function Kðx; zÞ ¼ e\njjx−zjj2\n2σ2 ,\n\nand σ is the width function.\nOptimization problem of quadratic function (Eq. (2)) is\n\nsolved to obtain the optimal parameter vector ∂�\n\n¼ ð∂�1; ∂�2;…; ∂�N train\nÞT in discriminant function.\n\nmin\n1\n2\nð\nXN train\n\ni¼1\n\nXN train\n\ni¼1\n∂i∂ jy\n\niy jK xi; x j\n� �\n\n−\nXN train\n\ni¼1\n∂i ð2Þ\n\ns:t:\nXNtrain\n\ni¼1\n\n∂iyi i ¼ 1; 2;…;N train\n\n0≤∂i≤C\n\nThe training sample xi corresponding to ∂i > 0 is used\nas a support vector. The optimization parameter b∗ can\nbe calculated by Eq. (3).\n\nb� ¼ 1\nNsv\n\nX\ni∈SV\n\nyi−\nX\n\nj∈SV\n∂�jK xi; x j\n\n� �� �\nð3Þ\n\nSVM classifier is used to determine whether the de-\ntected image is a human face. If it is not human face,\nthen the image is discarded. If it is, then the image is\nretained to output the detection result. Figure 3 shows\nthe detection process.\n\n3.2 Face recognition based on SIFT\nAfter face detection, face features are extracted for face\nrecognition, providing conditions for face alignment. In\nthe work, the robust SIFT algorithm is used for local fea-\nture extraction [22]. The algorithm finds feature points in\ndifferent scale spaces. It is irrelevant to rotation, scale, and\nbrightness changes. Besides, the algorithm has certain sta-\nbility to noise, affine transformation, and angle change.\n\n3.2.1 Basic principle of SIFT algorithm\nIn the process of feature construction by SIFT algorithm,\nit is necessary to deal with multiple details, achieving faster\noperation and higher positioning accuracy. Figure 4 shows\nflow block diagram of SIFT algorithm [21]. The generation\nprocess of local feature is described as follows [22]:\n\nFig. 2 SVM network structure\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 Page 3 of 19\n\n\n\n\n\n① Detect extreme points\nGaussian differential functions are used for image search\n\non all scales, thus identifying potential fixed points.\n② Position key points\nThe scale on candidate position of model is confirmed.\n\nThe stability degree determines the selection of key points.\n③ Determine the direction of key points\nUsing the gradient direction histogram, each key point\n\nis assigned a direction with the highest gradient value to\ndetermine the main direction of key point.\n④ Describe the key points\nThe local gradients of image are calculated and repre-\n\nsented by a kind of symbol.\n\n3.2.2 Key point matching\n3.2.2.1 Scale space Scale space introduces a scale par-\nameter into image matching model. The continuously\nvariable scale parameter is used to obtain the scale space\nsequence. After that, the main contour of scale space is\n\ntaken as the feature vector to extract the edge features\n[23]. The larger scale leads to the more blurred image.\nTherefore, scale space can simulate the formation\nprocess of target on the retina of the human eye.\nScale space of image can be expressed as Eq. (4).\n\nL x; y; σð Þ ¼ G x; y; σð Þ � I x; yð Þ ð4Þ\nIn Eq. (4), G(x, y, σ) is the Gaussian function, I(x, y) the\n\noriginal image, and * the convolution operation.\n\n3.2.2.2 Establishing Gaussian pyramid\n\nG x; y; σð Þ ¼ 1\n2πσ2\n\ne− x−d=2ð Þ2þ y−b=2ð Þ2ð Þ=2σ2 ð5Þ\n\nIn Eq. (5), d and b are the dimensions of Gaussian\ntemplate, (x, y) is the pixel location, and σ the scale space\nfactor.\nGaussian pyramid is established according to Eq. (5),\n\nincluding Gaussian blur and down-sampling (see Fig. 5).\nIt is observed that the pyramids with different sizes con-\nstitute tower model from bottom to top. The original\nimage is used for the first layer, the new image obtained\nby down-sampling for the second layer. There are n\nlayers in each tower. The number of layers can be calcu-\nlated by Eq. (6).\n\nn ¼ log2 minf p; qð Þg−d dϵ 0; log2 minf p; qð Þg½ �\nð6Þ\n\nIn Eq. (6), p and q are the sizes of the original image and d\nis the logarithm of minimum dimension of tower top image.\n\n3.2.2.3 Gaussian difference pyramid After scale\nnormalization of maxima and minima of the Gaussian La-\nplace function σ2∇2G, we obtain the most stable image fea-\ntures using other feature extraction functions. The\nGaussian difference function is approximated to the Gauss-\nian Laplace function σ2∇2G after scale normalization. The\nrelationship is described as follows:\n\n∂G\n∂σ\n\n¼ σ2∇ 2G ð7Þ\n\nDifferential is approximately replaced by the difference:\n\nσ2∇ 2G ¼ ∂G\n∂σ\n\n≈\nG x; y; kσð Þ−G x; y; σð Þ\n\nkσ−σ\nð8Þ\n\nTherefore,\n\nG x; y; kσð Þ−G x; y; σð Þ ≈ k−1ð Þσ2∇ 2G ð9Þ\nIn Eq. (9), k − 1 is a constant.\nIn Fig. 6, the red line is the DoG operator curve; the\n\nblue line the Gauss-Laplacian curve. In extreme detection\n\nFig. 3 The detection process of SVM classifier\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 Page 4 of 19\n\n\n\n\n\nmethod, the Laplacian operator is replaced by the DoG\noperator [24] (see Eq. (10).\n\nD x; y; σð Þ ¼ G x; y; kσð Þ−G x; y; σð Þð Þ � I x; yð Þ\n¼ L x; y; kσð Þ−L x; y; σð Þ ð10Þ\n\n3.2.2.4 Spatial extreme detection In Gaussian differ-\nence space, local extreme points constitute the key\npoints. When searching for key points, we compare the\n\nimages between two adjacent layers in the same group.\nAfter that, each pixel point is compared with all the ad-\njacent points to judge whether it is large or small (see\nFig. 6). The red intermediate detection point is com-\npared with 26 points in the surrounding, upper, and\nlower scale spaces to detect extreme points.\nIn the calculation, the Gaussian difference image is the\n\ndifference between the adjacent upper and lower images\nin each group of the Gaussian pyramid (see Fig. 7).\n\n3.2.2.5 Spatial extreme detection In Gaussian differ-\nence space, local extreme points constitute the key\npoints. When searching for key points, we compare the\nimages between two adjacent layers in the same group.\nAfter that, each pixel point is compared with all the\nadjacent points to judge whether it is large or small\n(see Fig. 8). The red intermediate detection point is com-\npared with 26 points in the surrounding, upper, and lower\nscale spaces to detect extreme points.\nIf there are N extreme points in each group, then we\n\nneed N+ 2-layer DoG pyramid and N+ 3-layer Gaussian\npyramid (see Fig. 8). Due to edge response, the extreme\npoints generated in this case are not all stable.\n\n3.2.2.6 Key point matching At first, the key point is\ncharacterized by position, scale, and direction. To main-\ntain the invariance of perspective and illumination\nchanges, the key point should be described by a set of vec-\ntors. Then, the descriptor consists of key points and other\ncontributive pixels. Besides, the independent characteristic\n\nFig. 4 SIFT algorithm flow chart\n\nFig. 5 Gaussian pyramid\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 Page 5 of 19\n\n\n\n\n\n\n\nof descriptor is guaranteed to improve the probability of\ncorrect matching of feature points.\nThe gradient value of key point is calculated. The gra-\n\ndient value and direction are determined by Eq. (11).\n\nm x; yð Þ ¼\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\nN xþ 1; yð Þ−N x−1; yð Þð Þ2 þ N x; yþ 1ð Þ−N x; y−1ð Þð Þ2\n\nq\n\nθ x; yð Þ ¼ α tan2\nN x; yþ 1ð Þ−N x; y−1ð Þ\nN xþ 1; yð Þ−N x−1; yð Þ\n\n� �\nð11Þ\n\nIn Eq. (11), N represents the scale space value of key point.\nGradient histogram statistics. The gradient and direc-\n\ntion of pixels in the neighborhood are represented by\nhistogram. The direction ranges from 0 to 360°. There is a\n\nFig. 6 Comparison of Gauss-Laplacian and DoG\n\nFig. 7 Gaussian pyramid of each group\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 Page 6 of 19\n\n\n\n\n\n\n\nsquare column for every 10°, forming 36 columns [25]\n(see Fig. 9). In feature point field, the peak represents the\ngradient direction. The histogram of maximum is the\nmain direction of key point. Meanwhile, the histogram\nwith peak value greater than 80% of main direction is se-\nlected for auxiliary direction to improve the matching\nrobustness.\nAfter successful matching of key points, the entire al-\n\ngorithm is not over yet. This is because substantial mis-\nmatched points appear in the matching process. These\nmismatched points are eliminated by Ransac method in\nSIFT matching algorithm [26].\n\n3.2.3 Face recognition experiment\nTo evaluate the algorithm, the experiment is conducted\nbased on face infrared database provided by Terravic Re-\nsearch Corporation. There are a total of 20 infrared\nimage sequences with head rotation, glasses, hats, and\nlight-illuminated pictures. Three pairs of images are se-\nlected from each face, with a total of 60 pairs. Figure 10\nshows the selected 120 images. In the work, the classic\n\nSIFT matching algorithm is used as the initial matching\nmethod to manually determine matching accuracy and\nmismatch rate of each group. In other words, the match-\ning performance is described by accuracy and error de-\ngrees. Accuracy is defined by the ratio of the number of\ncorrect matches in total number. Error degree is the ra-\ntio of the number difference (between key and matched\npoints) in the total number of key points.\nThese 120 samples are conducted with abstract match-\n\ning contrast according to the variables including head\nrotation angle, illumination transformation, glasses, and\nhat wearing. Meanwhile, other variables remain the\nsame. Figures 11, 12, 13, and 14 show the matching re-\nsults, respectively:\n\n1. Matching results when head rotation angle changes\n2. Matching results when wearing glasses\n3. Matching results when wearing a hat\n4. Matching results when light and shade change\n\nThe experimental data are shown in Table 1.The ex-\nperimental image and Table 1 show:\n① SIFT matching performance is more easily affected\n\nby wearing glasses than head rotation angle, light illu-\nmination, darkness, and wearing hat.\n② In the case of the same number of matches, the\n\nsuccess rate of SIFT matching is higher than that of the\nHarris matching method [27].\nThe overall trend of results can be well presented al-\n\nthough there are inevitable errors due to the finiteness\nof experimental samples.\n\n3.3 Face alignment\nFace alignment is the positioning of face feature points.\nAfter face image detection, the SIFT algorithm automat-\nically positions the contour points of the eyebrows, eyes,\nnose, and mouth. In the try-on process of AR glasses,\nthe eyes are positioned to estimate the head posture.\nThe pose estimation is applied to the tracking registra-\ntion subsystem of glasses, thus producing perspective\n\nFig. 8 The detection of DoG space extreme point\n\nFig. 9 The histogram of the main direction\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 Page 7 of 19\n\n\n\n\n\n\n\ntransformation. However, the pose estimation is easily af-\nfected by the positioning of face feature points, resulting\nin estimation error. The feature points are accurately posi-\ntioned to achieve good effect of head pose estimation.\nAt present, there are many face alignment algorithms.\n\nSDM is a method of finding function approximation\nproposed by Zhu et al [28] by calculating the average\nface, and local features around each feature point are ex-\ntracted to form feature vector descriptor. The offset be-\ntween average and real face is calculated to obtain the\nstep size and motion direction for iteration. The current\nface feature points are converged to the optimal position\nby repeated iterations.\nFigure 15 shows the SDM-based face alignment process.\n\nThe face alignment process is described as follows.\n\n3.3.1 Image normalization\nThe image is normalized to achieve face alignment, thus\nimproving the efficiency of training. The face image to be\ntrained is manually labeled with feature points. After rea-\nsonable translation, rotation, and scaling transformation,\nthe image is aligned to the first sample. The sample size is\nunified to arrange the original data information with con-\nfused, reducing interference other than shape factors. Fi-\nnally, the calculated average face is placed on the sample\nas the estimated face. The average is aligned with the ori-\nginal face image in the center.\n\nIt is denoted that x∗ is the optimal solution in face fea-\nture point location, x0 the initialization feature point,\nd(x) ∈ Rn × 1 the coordinates of n feature points in the\nimage, and h the nonlinear feature extraction function\nnear each feature point. If the SIFT features of 128 di-\nmensions are extracted from each feature point, then\nh(d(x)) ∈ R128n × 1. The SIFT feature extracted at x∗ can\nbe expressed as ∅∗ = h(d(x∗)). Then, the face feature\npoint alignment is converted into the operation of solv-\ning Δx, which minimizes Eq. (12).\n\nf x0 þ Δxð Þ ¼ hðd x0 þ Δxð Þk k22 ð12Þ\nThe step size Δx is calculated based on the SDM\n\nalgorithm.\n\nxk ¼ xk þ Δxk ð13Þ\nIf Rk and bk are the paths of each iteration, then\n\nEq. (11) can converge the feature point from the initial\nvalue x0 to x∗.\n\nxkþ1 ¼ xk−1 þ Rk−1∅k−1 þ bk−1 ð14Þ\nDuring training process, {di} is the set of face images,\n\n{di} the set of manually labeled feature points, and x0 the\nfeature point of each image. Face feature point location\nis transformed into a linear regression problem. For the\nproblem, the input feature is the SIFT feature ∅i\n\n0 at x0;\n\nFig. 10 Sample sequence set\n\nFig. 11 Matching results when head rotation angle changes\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 Page 8 of 19\n\n\n\n\n\n\n\nthe result the iteration step size Δxi� ¼ xi� þ Δxi0 from x0\nto x∗; and the objective function Eq. (15).\n\nargminR0b0\n\nX\ndi\n\nX\nxi\n\nΔxi�−R0∅i\n�−b0\n\n\t\t \t\t2\n2 ð15Þ\n\nIn this way, R0 and b0 from the training set are iterated\nto obtain Rk and Rk. The two parameters are used for\nthe test phase to achieve the alignment of test images.\n\n3.3.2 Local feature extraction of SIFT algorithm\nIn the work, the principal component analysis is used to\nreduce the dimension of image [29], the impact of\nnon-critical dimensions, and the amount of data, thus\nimproving the efficiency. After the dimension reduction,\nthe local feature points are extracted from the face\nimage. To improve the alignment accuracy of feature\npoints, the robust SIFT algorithm is applied for local fea-\nture extraction. Section 3.2.2 introduces the extraction\nprocess in detail.\n\n3.3.3 SDM algorithm alignment result\nTraining samples are selected from IBUG and LFW face\ndatabases. The former contains 132 face images. Each\nimage is labeled with 71 face feature points, which are\nsaved in pts file. The latter consists of the sets of test and\ntraining samples, wherein, the set of test sample contains\n206 face images. Each image is labeled with 71 face feature\npoints, which are saved in pts file. The set of training\n\nsample contains 803 face images. Each image is labeled\nwith 68 face feature points. Figures 16 and 17 show frontal\nand lateral face alignment results, respectively.\n\n3.4 Face pose estimation\nBased on computer vision, the pose of object refers to\nits orientation and position relative to the camera. The\npose can be changed by moving the camera or object.\nGeometric model of camera imaging determines the re-\nlationship between 3D geometric position of certain\npoint on head surface and corresponding point of image.\nThese geometric model parameters are camera parame-\nters. In most cases, these parameters are obtained by ex-\nperiments. This process is called labeling [27, 29].\nCamera labeling determines the geometric and optical\nproperties, 3D position, and direction of camera relative\nto certain world coordinate system.\nThe idea of face pose estimation is described as fol-\n\nlows. Firstly, we find the projection relationship between\n2D coordinates on face image and 3D coordinates of\ncorresponding points on 3D face model. Then, the mo-\ntion coordinates of camera are calculated to estimate\nhead posture.\nA 3D rigid object has two movements relative to the camera:\n① Translation movement\nThe camera is moved from current spatial position\n\n(X,Y, Z) to new spatial position (X′,Y′, Z′), which is called\n\nFig. 12 Matching results when wearing glasses\n\nFig. 13 Matching results when wearing a hat\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 Page 9 of 19\n\n\n\n\n\n\n\ntranslation. Translation vector is expressed as τ = (X′ −X,\nY′ −Y, Z′ −Z).\n② Rotary movement\nIf the camera is rotated around the XYZ axis, the rota-\n\ntion has six degrees of freedom. Therefore, pose estima-\ntion of 3D object means finding six numbers (three for\ntranslation and three for rotation).\n\n3.4.1 Feature point labelling\nThe 2D coordinates of N points are determined to calcu-\nlate 3D coordinates of points, thus obtaining 3D pose of\nobject in an image.\nTo determine the 2D coordinates of N points, we se-\n\nlect the points with rigid body invariance, such as the\nnose tip, corners of eyes, and mouth. In the work, there\nare six points including the nose tip, chin, left, and right\ncorners of eyes and mouth.\nSFM (Surrey Face Model) is used as general 3D face\n\nmodel to obtain 3D coordinates corresponding to se-\nlected 2D coordinates [30]. By manual labeling, we ob-\ntain the 3D coordinates (x, y, z) of six points for pose\nestimation. These points are called world coordinates in\nsome arbitrary reference/coordinate system.\n\n3.4.2 Camera labeling\nAfter determining world coordinates, the camera is reg-\nistered to obtain the camera matrix, namely focal length\nof camera, optical center, and radial distortion parame-\nters of image. Therefore, camera labeling is required. In\nthe work, the camera is labeled by Yang and Patras [31]\nto obtain the camera matrix.\n\n3.4.3 Feature point mapping\nFigure 18 shows the world, camera, and image coordin-\nate systems. In Fig. 18, O is the center of camera, c the\noptical center of 2D image plane, P the point in world\ncoordinate system, and P′ the projection of P on image\nplane. P′ can be determined according to the projection\nof the P point.\nIt is denoted that the world coordinate of P is (U,V, W).\n\nBesides, the known parameters are the rotation matrix R\n\nFig. 14 Matching results when light and shade change\n\nTable 1 Match result analysis table\n\nVariate Number of\nmatches\n\nTotal number\nof key points\n\nMatch\nratio\n\nFalse\nmatch rate\n\nHead rotation 18 158 0.129 0.871\n\nWearing glasses 15 167 0.099 0.901\n\nWearing a hat 21 106 0.247 0.753\n\nLight and shade\nchange\n\n45 281 0.191 0.809\nFig. 15 The face alignment process based on SDM\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 Page 10 of 19\n\n\n\n\n\n\n\n(matrix 3 × 3) and translation vector τ (vector 3 × 1) from\ncamera to world coordinate. It is possible to determine\nposition O(X, Y, Z) of P in camera coordinate system.\n\nx\ny\nz\n\n2\n4\n\n3\n5 ¼ R\n\nu\nv\nw\n\n2\n4\n\n3\n5þ τ⇒\n\nx\ny\nz\n\n2\n4\n\n3\n5 ¼ Rjτ½ �\n\nu\nv\nw\n\n2\n4\n\n3\n5 ð16Þ\n\nEquation (16) is expanded as follows:\n\nx\ny\nz\n\n2\n4\n\n3\n5 ¼\n\nr00 r01 r02 τx\nr10 r11 r12 τy\nr20 r21 r22 τz\n\n2\n4\n\n3\n5\n\nu\nv\nw\nl\n\n2\n664\n\n3\n775 ð17Þ\n\nIf plenty of points are mapped to (X, Y, Z) and (U,V, W),\nthe above problem is transformed into a system of linear\nequations with unknown (τx, τy, τz) . Then, the system of\nlinear equations can be solved.\nFirstly, the six points on 3D model are manually la-\n\nbeled to derive their world coordinates (U, V, W). Equa-\ntion (18) is used to determine 2D coordinates (X, Y) of\nsix points in image coordinate system.\n\nx\ny\n1\n\n2\n4\n\n3\n5 ¼ S\n\nf x 0 0\n0 f y 0\n0 0 1\n\n2\n4\n\n3\n5 x\n\ny\nZ\n\n2\n4\n\n3\n5 ð18Þ\n\nwhere fx and fy are the focal lengths in the x and y direc-\ntions, (cx, cy) is the optical center, and S the unknown scaling\nfactor. If P in 3D is connected to O, then P′ where light in-\ntersects image plane is the same image connecting all points\nin the center of the camera produced by P along the ray.\nEquation (18) is converted to the following form:\n\nS\nX\nY\nZ\n\n2\n4\n\n3\n5 ¼\n\nr00 r01 r02 τx\nr10 r11 r12 τy\nr20 r21 r22 τz\n\n2\n4\n\n3\n5\n\nu\nv\nw\nl\n\n2\n664\n\n3\n775 ð19Þ\n\nThe image and world coordinates are known in the\nwork. Therefore, Eqs. (18) and (19) are transformed into\nthe following form:\n\nx\ny\n1\n\n2\n4\n\n3\n5 ¼ S\n\nf x 0 0\n0 f y 0\n0 0 1\n\n2\n4\n\n3\n5 r00 r01 r02 τx\n\nr10 r11 r12 τy\nr20 r21 r22 τz\n\n2\n4\n\n3\n5\n\nu\nv\nw\nl\n\n2\n664\n\n3\n775 ð20Þ\n\nIf the correct poses R and τ are known, then the 2D\nposition of 3D facial point on image can be predicted by\nprojecting the 3D point onto the image (see Eq. (20)).\nThe 2D facial feature points are known. Pose estimation\ncan be performed by calculating the distance between\nthe projected 3D point and 2D facial feature. If the pose\nis correctly estimated, the 3D points projected onto\nimage plane will almost coincide with the 2D facial fea-\ntures. Otherwise, the re-projection error can be mea-\nsured. The least square method is used to calculate the\nsum of squares of the distance between the projected 3D\nand 2D facial feature points.\n\n3.5 Tracking registration system\nTracking registration technology is the process of align-\ning computer-generated virtual objects with scenes in\nthe real world. At present, there are two tracking regis-\ntration techniques. The first superimposes certain point\nof face feature with a point of virtual glasses based on\nthe face feature point tracking method [32]. The second\nis based on the geometric transformation relation track-\ning method. Face geometry and virtual glasses model are\nconducted with affine transformation. Virtual glasses\nmodel moves with the movement of human head, mak-\ning corresponding perspective changes and realizing 3D\ntry-on effect [33]. For the first technique, the virtual\nglasses cannot be changed with the movement of user\nhead, causing poor user experience. The second tech-\nnique has good tacking effect. The virtual glasses will be\ndistorted with overlarge head corner. Combined with the\ntwo methods, the glasses model is conducted with per-\nspective transformation using six degrees of freedom ob-\ntained by pose estimation in Section 3.3. After face\nsuperposition, accurate tracking is realized through bet-\nter stereoscopic changes.\n\nFig. 16 The picture", "metadata_storage_path": "aHR0cHM6Ly9zdG9yYWdlYWNjb3VudGxpZ2lyay5ibG9iLmNvcmUud2luZG93cy5uZXQvcGFwZXIvczEzNjQwLTAxOC0wMzczLTgucGRm0", "metadata_author": "Boping Zhang", "metadata_title": "Augmented reality virtual glasses try-on technology based on iOS platform", "keyphrases": [ "accurate feature point alignment model", "new online shopping mode", "Augmented reality virtual glasses", "external propaganda channels", "actual wearing effects", "Creative Commons Attribution", "lates physical information", "phys- ical information", "head pose estimation", "traditional online shopping", "virtual glasses model", "input device-monocular camera", "key research issue", "virtual world scene", "user online glasses", "RESEARCH Open Access", "1 Introduction Network virtual", "iOS mobile platform", "local face features", "virtual model", "feature points", "scale-invariant feature", "new way", "2018 Open Access", "research key", "local features", "virtual information", "Face information", "Information Engineer", "real world", "Virtual try", "favorite glasses", "different glasses", "user senses", "face detection", "human face", "iOS glasses", "Boping Zhang", "optimal purchase", "robust SIFT", "rapid achievement", "sponding images", "vector machine", "EURASIP Journal", "Video Processing", "The Author", "iOS platform", "AR glasses", "camera image", "quick try", "human senses", "AR principle", "Computer vision", "real time", "real-time interaction", "Xuchang University", "APP system", "descent method", "computer technology", "SVM classifier", "Abstract", "development", "commerce", "gap", "goods", "speed", "immersion", "SDM", "identification", "Keywords", "merchants", "consumers", "important", "part", "field", "experiential", "position", "angle", "corre", "screen", "sound", "taste", "touch", "space", "sensory", "perience", "faces", "bopingzhang", "yeah", "School", "Henan", "China", "article", "terms", "real-time head mo- tion tracking", "Two corresponding isosceles triangles", "General face recognition process", "original author(s", "charge-coupled device) camera", "Android mobile platform", "Creative Commons license", "real envir- onment", "foreign glasses sellers", "3D modeling approach", "2D image overlay", "2D image superposition", "AR technology design", "2D face images", "virtual optician system", "image processing technology", "real-time requirements", "2D images", "accurate tracking", "International License", "The superposition", "3D glasses", "glasses images", "network virtual", "virtual try", "virtual objects", "Virtual glasses", "unrestricted use", "appropriate credit", "Research status", "Smart Look", "wearing effect", "research hotspots", "research results", "four categories", "Three points", "fine transformation", "unrealistic deformation", "inter- action", "good results", "different defects", "depth research", "large extent", "new ideas", "false ratios", "important indicator", "feature extraction", "dimension reduc", "first step", "cific location", "Glasses companies", "glasses model", "AR-based glasses", "face features", "face area", "face needs", "matching recognition", "doi.org", "orcid.org", "shopping experience", "input sensor", "experience senses", "input image", "computer vision", "augmented reality", "video stream", "system performance", "online shopping", "based detection", "creativecommons", "licenses", "distribution", "reproduction", "medium", "source", "link", "changes", "Zhang", "crossmark", "crossref", "dialog", "USA", "Camirror", "Ipoint", "Kisok", "Xview", "function", "Users", "domestic", "Meijing", "Kede", "Biyao", "present", "Huang", "eyes", "pose", "scale", "method", "realism", "Cheng", "monocular", "CCD", "marker", "kinds", "techniques", "advantages", "problems", "application", "precondition", "frame", "posture", "lumination", "occlusion", "omission", "people", "Fig.", "gion", "range", "Offline Learning Training Feature Feature Image Extraction Space Testing Feature Feature Image Extraction Macthing Online Learning", "Face Macthing Recognition Recognition Results CLassification Face Result Input Feature Feature Searching", "The Gaussian kernel function Kðx", "Extraction Dimensionality Image Tracing", "Face recognition process Zhang", "high-dimensional face feature vector", "high-dimensional feature space", "machine learning method", "online matching models", "Feature extraction", "∂ jy iy jK xi", "three-layer feedforward neural network", "face feature identification", "high-dimensional feature vector", "Feature dimension reduction", "low-dimensional input space", "training sample", "de- tected image", "Sigmoid kernel functions", "detection result", "local binary patterns", "local structural content", "Common kernel func", "3.2 Face recognition", "Gabo feature", "sional face features", "3.1 SVM-based face detection", "face detection methods", "spatial fre- quency", "face image", "low-dimensional subspace information", "support vector machine", "optimal parameter vector", "Matching recognition", "¼ sgn XN train", "space complexity", "classic SVM algorithm", "input vector", "Nsv X", "∂�jK xi", "face database", "detection process", "vector dimension", "Common features", "width function", "quadratic function", "discriminant function", "spatial position", "optimization parameter", "ginal features", "matching strategy", "network output", "∂�N train", "oriented gradient", "illumination changes", "small displacements", "ture changes", "brightness invariance", "direction selectivity", "previous links", "final decision", "virtual glasses", "Markov model", "statistical theory", "nonlinear mapping", "mal hyperplane", "largest interval", "inner product", "Mercer condition", "Optimization problem", "video sequence", "description ability", "output layer", "Figure 3 shows", "LBP", "HOG", "histogram", "Gabor", "insensitiveness", "overall", "Page", "time", "premise", "technology", "scholars", "HMM", "AdaBoost", "structure", "Equation", "polynomial", "zÞ", "Eq.", "ÞT", "XNtrain", "∂i", "SIFT", "ð1Þ", "2σ", "ð3Þ", "Figure 4 shows flow block diagram", "SVM Classifier detection Output results", "SVM network structure Zhang", "stable image fea- tures", "other feature extraction functions", "SVM classifier Zhang", "higher positioning accuracy", "highest gradient value", "ian Laplace function", "DoG operator curve", "potential fixed points", "Establishing Gaussian pyramid", "Gaussian differential functions", "variable scale parameter", "gradient direction histogram", "stitute tower model", "Key point matching", "different scale spaces", "Gaussian difference pyramid", "scale space sequence", "scale space factor", "Gaussian difference function", "image matching model", "robust SIFT algorithm", "② Position key points", "tower top image", "Gaussian function", "extreme detection", "candidate position", "Gaussian template", "Gaussian blur", "Gauss-Laplacian curve", "feature construction", "local feature", "feature vector", "extreme points", "face recognition", "face alignment", "brightness changes", "sta- bility", "affine transformation", "angle change", "Basic principle", "multiple details", "X1 K", "Sgn Otrain", "train Xtrain", "stability degree", "local gradients", "main contour", "edge features", "larger scale", "human eye", "different sizes", "first layer", "second layer", "minimum dimension", "red line", "blue line", "image search", "blurred image", "original image", "new image", "generation process", "main direction", "formation process", "convolution operation", "scale normalization", "The relationship", "conditions", "rotation", "noise", "scales", "selection", "kind", "symbol", "target", "retina", "dimensions", "pixel", "location", "down-sampling", "pyramids", "bottom", "layers", "number", "¼ log2", "qð", "logarithm", "maxima", "minima", "2G", "constant", "Import", "End", "3.2.1", "3.2.2", "σ", "∇", "Fig. 4 SIFT algorithm flow chart", "Image input template input Detection", "red intermediate detection point", "local characteristic region characteristic", "N+ 3-layer Gaussian pyramid", "N+ 2-layer DoG pyramid", "Gaussian DOG square column", "Wrong image matching characteristic", "Spatial extreme detection", "two adjacent layers", "feature point field", "local extreme points", "N extreme points", "Gaussian pyramid Zhang", "Gaussian difference image", "lower scale spaces", "scale space value", "Gradient histogram statistics", "7 Gaussian pyramid", "independent characteristic", "matching image", "adjacent points", "extreme extracti", "pixel point", "adjacent upper", "key point", "DoG operator", "ence space", "gradient value", "correct matching", "successful matching", "edge response", "stability characteristicmatching", "Octave2 Octave1", "direc- tion", "next octave", "lower images", "peak value", "gradient direction", "auxiliary direction", "Laplacian operator", "contributive pixels", "same group", "26 points", "¼ L", "calculation", "surrounding", "case", "invariance", "perspective", "set", "tors", "descriptor", "other", "Location", "Octave5", "Octave4", "Octave3", "probability", "ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi", "neighborhood", "Comparison", "Gauss-Laplacian", "36 columns", "maximum", "robustness", "2.5", "3.", "θ", "360°", "10°", "Terravic Re- search Corporation", "tracking registra- tion subsystem", "DoG space extreme point", "many face alignment algorithms", "SDM-based face alignment process", "match- ing performance", "main direction Zhang", "feature vector descriptor", "original data information", "face infrared database", "① SIFT matching performance", "main direction transformation", "20 infrared image sequences", "initial matching method", "Harris matching method", "Face recognition experiment", "SIFT matching algorithm", "head rotation angle", "face feature points", "face image detection", "3.3 Face alignment", "matching process", "SIFT algorithm", "motion direction", "matching re", "experimental data", "real face", "illumination transformation", "head posture", "scaling transformation", "Ransac method", "mismatched points", "key points", "contour points", "light-illuminated pictures", "mismatch rate", "other words", "success rate", "overall trend", "inevitable errors", "good effect", "function approximation", "step size", "optimal position", "sonable translation", "shape factors", "average face", "estimation error", "matching accuracy", "perimental image", "Image normalization", "number difference", "same number", "Error degree", "first sample", "sample size", "Three pairs", "correct matches", "other variables", "Table 1 show", "experimental samples", "tween average", "total number", "60 pairs", "120 samples", "hats", "images", "Figure", "work", "classic", "group", "grees", "sults", "shade", "darkness", "finiteness", "positioning", "eyebrows", "nose", "mouth", "Scale", "Zhu", "offset", "iteration", "current", "efficiency", "training", "interference", "3.1", "nonlinear feature extraction function", "lateral face alignment results", "Face feature point location", "3.3.3 SDM algorithm alignment result", "face feature point alignment", "principal component analysis", "world coordinate system", "LFW face databases", "Local feature extraction", "current spatial position", "new spatial position", "n feature points", "local feature points", "initialization feature point", "objective function Eq", "71 face feature points", "68 face feature points", "3D face model", "3.4 Face pose estimation", "linear regression problem", "camera parame- ters", "3D rigid object", "3D geometric position", "ginal face image", "iteration step size", "geometric model parameters", "10 Sample sequence set", "Matching results", "alignment accuracy", "3D position", "SIFT feature", "face images", "input feature", "corresponding points", "extraction process", "3D coordinates", "test sample", "optimal solution", "initial value", "head rotation", "two parameters", "critical dimensions", "pts file", "head surface", "most cases", "optical properties", "projection relationship", "two movements", "① Translation movement", "test phase", "test images", "The pose", "2D coordinates", "tion coordinates", "training process", "camera imaging", "Camera labeling", "camera relative", "training set", "dimension reduction", "argminR0b0 X", "x∗", "Δx", "center", "128 di", "R128n", "operation", "Þk", "xk", "Rk", "bk", "paths", "x0", "xi", "way", "impact", "amount", "Section", "detail", "IBUG", "former", "sets", "Figures", "orientation", "periments", "direction", "idea", "lows", "∅", "100", "200 300", "500", "radial distortion parame- ters", "Match result analysis table", "key points Match ratio", "general 3D face model", "False match rate", "Surrey Face Model", "face alignment process", "training face sample", "average face Extract", "face detection model", "rigid body invariance", "y direc- tions", "SDM Iterative Solution", "3.4.1 Feature point labelling", "4.3 Feature point mapping", "arbitrary reference/coordinate system", "unknown scaling factor", "tersects image plane", "late 3D coordinates", "2D image plane", "image coordinate system", "rotation matrix R", "3D model", "SDM Zhang", "3D pose", "P point", "world coordinates", "② Rotary movement", "XYZ axis", "six degrees", "six numbers", "nose tip", "manual labeling", "pose estimation", "focal length", "Variate Number", "Total number", "Head rotation", "position O", "linear equations", "following form", "same image", "3D object", "N points", "six points", "rota- tion", "Equa- tion", "optical center", "hat Zhang", "Wearing glasses", "S X", "camera matrix", "Translation vector", "Table 1", "5 ¼ R", "5 ¼ S", "freedom", "corners", "SFM", "Yang", "Patras", "systems", "projection", "parameters", "light", "matches", "change", "Input", "plenty", "problem", "fx", "fy", "ray", "300", "τ", "4.2", "geometric transformation relation track- ing method", "face feature point tracking method", "2D facial feature points", "least square method", "3.5 Tracking registration system", "Tracking registration technology", "computer-generated virtual objects", "corresponding perspective changes", "poor user experience", "overlarge head corner", "good tacking effect", "3D facial point", "spective transformation", "2D position", "user head", "Face geometry", "face superposition", "stereoscopic changes", "human head", "correct poses", "3D point", "projection error", "tration techniques", "first technique", "tech- nique", "two methods", "image plane", "Pose estimation", "Eqs", "distance", "tures", "sum", "squares", "process", "scenes", "movement", "picture", "664" ], "merged_content": "\nRESEARCH Open Access\n\nAugmented reality virtual glasses try-on\ntechnology based on iOS platform\nBoping Zhang\n\nAbstract\n\nWith the development of e-commerce, network virtual try-on, as a new online shopping mode, fills the gap that\nthe goods cannot be tried on in traditional online shopping. In the work, we discussed augmented reality virtual\nglasses try-on technology on iOS platform to achieve optimal purchase of online glasses, improving try-on speed of\nvirtual glasses, user senses of reality, and immersion. Face information was collected by the input device-monocular\ncamera. After face detection by SVM classifier, the local face features were extracted by robust SIFT. Combined with\nSDM, the feature points were iteratively solved to obtain more accurate feature point alignment model. Through\nthe head pose estimation, the virtual model was accurately superimposed on the human face, thus realizing the\ntry-on of virtual glasses. The above research was applied in iOS glasses try-on APP system to design the try-on system\nof augmented reality virtual glasses on iOS mobile platform. It is proved that the method can achieve accurate\nidentification of face features and quick try-on of virtual glasses.\n\nKeywords: Virtual try-on, Virtual glasses, Augmented reality, Computer vision, Pose estimation, iOS\n\n1 Introduction\nNetwork virtual try-on is a new way of online shopping.\nWith the development of e-commerce, it broadens the\nexternal propaganda channels of merchants to enhance\nthe interaction between consumers and merchants.\nVirtual try-on fills the gap that the goods cannot be\ntried on in traditional online shopping. As an important\npart of network virtual try-on, virtual glasses try-on\ntechnology has become a key research issue in this field\nrecently [1–4]. During virtual glasses try-on process,\nconsumers can select their favorite glasses by compar-\ning the actual wearing effects of different glasses in the\nonline shopping. The research key of virtual glasses\ntry-on system is the rapid achievement of experiential\nonline shopping.\nAR (augmented reality) calculates the position and\n\nangle of camera image in real time while adding corre-\nsponding images. The virtual world scene is superim-\nposed on a screen in real world for real-time\ninteraction [5]. Using computer technology, AR simu-\nlates physical information (vision, sound, taste, touch,\netc.) that is difficult to experience within certain time\n\nand space of real world. After superimposition of phys-\nical information, the virtual information is perceived by\nhuman senses in real world, thus achieving sensory ex-\nperience beyond reality [6].\nBased on AR principle, virtual glasses try-on technol-\n\nogy achieves optimal purchase of user online glasses and\nquick try-on of virtual glasses, improving the senses of\nreality and immersion. Monocular camera is used as the\ninput device to discuss try-on technology of AR glasses\non iOS platform. Face information is collected by mon-\nocular camera. After face detection by SVM (support\nvector machine) classifier, the local features of faces are\nextracted by robust SIFT (scale-invariant feature trans-\nform). Combined with SDM (supervised descent\nmethod), the feature points were iteratively solved to ob-\ntain more accurate feature point alignment model.\nThrough the head pose estimation, the virtual glasses\nmodel was accurately superimposed on the human face,\nthus realizing the try-on of virtual glasses. The above re-\nsearch is applied in iOS glasses try-on APP system to de-\nsign the try-on system of AR glasses on iOS mobile\nplatform. It is proved that the method can achieve ac-\ncurate identification of face features and quick try-on of\nvirtual glasses.Correspondence: bopingzhang@yeah.net\n\nSchool of Information Engineer, Xuchang University, Xuchang 461000,\nHenan, China\n\nEURASIP Journal on Image\nand Video Processing\n\n© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0\nInternational License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and\nreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to\nthe Creative Commons license, and indicate if changes were made.\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 \nhttps://doi.org/10.1186/s13640-018-0373-8\n\n \n\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13640-018-0373-8&domain=pdf\nhttp://orcid.org/0000-0001-7835-7622\nmailto:bopingzhang@yeah.net\nhttp://creativecommons.org/licenses/by/4.0/\n\n\n2 Research status of network virtual try-on\ntechnology\nGlasses try-on system was first applied in the USA.\nGlasses companies such as Camirror, Smart Look, Ipoint\nKisok, and Xview pioneered the online try-on function [7].\nUsers freely feel the wearing effect, enhancing the online\nshopping experience. Recently, online try-on function is\nexplored by domestic and foreign glasses sellers, such as\nMeijing [8], Kede [9] and Biyao [10].\nVirtual glasses try-on system involves computer vision,\n\naugmented reality, and image processing technology.\nRecently, research hotspots are speed, experience, and\nimmersion of try-on. At present, research results can be di-\nvided into four categories, namely 2D image superposition,\n3D glasses superimposed on 2D face images, 3D face mod-\neling, and AR technology based on video stream [11–14].\nHuang [15] introduced virtual optician system based on\n\nvision, which detects user’s face before locating user’s eyes.\nThree points are selected from face and glasses images.\nTwo corresponding isosceles triangles are formed for af-\nfine transformation, thus estimating the pose and scale of\nface in real time. This method realizes real-time head mo-\ntion tracking. However, the glasses model easily produces\nunrealistic deformation, affecting the realism of the\nglasses.\nAR technology is also applied in the virtual glasses\n\ntry-on system. Cheng et al. [16] selected a monocular\nCCD (charge-coupled device) camera as the input sensor\nto propose AR technology design based on the inter-\naction of marker and face features. Virtual glasses try-on\nsystem is established based on Android mobile platform,\nachieving good results. During virtual try-on process, we\nuse 2D image overlay or 3D modeling approach. There\nare still different defects although all kinds of virtual\nglasses try-on techniques have certain advantages. The\nsuperposition of 2D images is unsatisfactory in the sense\nof reality. Besides, the 3D modeling takes too long to\nmeet the real-time requirements of online shopping.\n\nIn-depth research is required to realize accurate tracking\nand matching. These problems can be solved by\nAR-based glasses try-on technology to a large extent,\nthus providing new ideas for virtual try-on technology.\n\n3 Methods of face recognition\nIt is necessary to integrate virtual objects into real envir-\nonment for the application of AR technology in virtual\nglasses try-on system, wherein face recognition is the\nprecondition for virtual glasses try-on system. During\ntry-on process, it is necessary to detect the face in each\nframe of the video. However, the problems of posture, il-\nlumination, and occlusion can increase the omission and\nfalse ratios of face detection. The real time of detection\nis an important indicator of system performance to en-\nhance people’s experience senses.\nGeneral face recognition process consists of face de-\n\ntection, tracking, feature extraction, dimension reduc-\ntion, and matching recognition (see Fig. 1) [17].\nIn Fig. 1, face detection is the first step to realize face\n\nrecognition. Its purpose is to automatically find face re-\ngion in an input image. If there is a face area, the spe-\ncific location and range of face needs to be located. Face\ndetection is divided into image-based and video-based\ndetection. If the input is a still image, each image is de-\ntected; if the input is a video, face detection is performed\nthroughout the video sequence.\nFeature extraction is based on face detection, and the\n\ninput is the detected face image. Common features are\nLBP (local binary patterns), HOG (histogram of oriented\ngradient), Gabor, etc. HOG [18] describes the edge fea-\ntures. Due to insensitiveness to illumination changes and\nsmall displacements, it describes the overall and local in-\nformation of human face. LBP [19] shows the local tex-\nture changes of an image, with brightness invariance.\nGabo feature [20] captures the local structural content\nof spatial position, direction selectivity, and spatial fre-\nquency. It is suitable for description of human faces.\n\nFig. 1 Face recognition process\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 Page 2 of 19\n\n Face Macthing Recognition Recognition Results CLassification Face Result Input Feature Feature Searching Extraction Dimensionality Image Tracing Offline Learning Training Feature Feature Image Extraction Space Testing Feature Feature Image Extraction Macthing Online Learning \n\n\n\nFeature dimension reduction is described as follows.\nFace feature is generally high-dimensional feature vector.\nFace recognition of high-dimensional feature vector\nincreases time and space complexity. Besides, it is difficult\nto effectively judge the description ability of high-dimen-\nsional face features. The high-dimensional face feature\nvector can be projected to the low-dimensional subspace.\nThe low-dimensional subspace information can complete\nface feature identification. After feature extraction, the ori-\nginal features are recombined to reduce vector dimension\nof face feature.\nAfter the previous links, we compare the existing tar-\n\ngets in face database and the faces to be identified based\non certain matching strategy, making final decision.\nMatching recognition can be represented by offline\nlearning and online matching models.\n\n3.1 SVM-based face detection\nFace detection is the premise of virtual glasses try-on\ntechnology. Recently, scholars proposed face detection\nmethods, such as neural network, SVM (support vector\nmachine), HMM (hidden Markov model), and AdaBoost.\nIn the work, the classic SVM algorithm is used for face\ndetection. SVM algorithm is a machine learning method\nbased on statistical theory. Figure 2 shows the network\nstructure of SVM [21]. SVM algorithm can be regarded\nas a three-layer feedforward neural network with a hid-\nden layer. Firstly, the input vector is mapped from\nlow-dimensional input space to the high-dimensional\nfeature space by nonlinear mapping. After that, the opti-\nmal hyperplane with the largest interval is constructed\nin the high-dimensional feature space.\nIt is denoted that the input vector of SVM x= (x1, x2,…, xn).\n\nEquation (1) shows the network output of output layer\nbased on x.\n\ny xð Þ ¼ sgn\nXN train\n\ni¼1\nyi∂\n\n�\ni K xi; x\n\n� �þ b�\n� �\n\nð1Þ\n\nwherein the inner product K(x(i), x) is a kernel function\nsatisfying the Mercer condition. Common kernel func-\ntions consist of polynomial, Gauss, and Sigmoid kernel\n\nfunctions. The Gaussian kernel function Kðx; zÞ ¼ e\njjx−zjj2\n2σ2 ,\n\nand σ is the width function.\nOptimization problem of quadratic function (Eq. (2)) is\n\nsolved to obtain the optimal parameter vector ∂�\n\n¼ ð∂�1; ∂�2;…; ∂�N train\nÞT in discriminant function.\n\nmin\n1\n2\nð\nXN train\n\ni¼1\n\nXN train\n\ni¼1\n∂i∂ jy\n\niy jK xi; x j\n� �\n\n−\nXN train\n\ni¼1\n∂i ð2Þ\n\ns:t:\nXNtrain\n\ni¼1\n\n∂iyi i ¼ 1; 2;…;N train\n\n0≤∂i≤C\n\nThe training sample xi corresponding to ∂i > 0 is used\nas a support vector. The optimization parameter b∗ can\nbe calculated by Eq. (3).\n\nb� ¼ 1\nNsv\n\nX\ni∈SV\n\nyi−\nX\n\nj∈SV\n∂�jK xi; x j\n\n� �� �\nð3Þ\n\nSVM classifier is used to determine whether the de-\ntected image is a human face. If it is not human face,\nthen the image is discarded. If it is, then the image is\nretained to output the detection result. Figure 3 shows\nthe detection process.\n\n3.2 Face recognition based on SIFT\nAfter face detection, face features are extracted for face\nrecognition, providing conditions for face alignment. In\nthe work, the robust SIFT algorithm is used for local fea-\nture extraction [22]. The algorithm finds feature points in\ndifferent scale spaces. It is irrelevant to rotation, scale, and\nbrightness changes. Besides, the algorithm has certain sta-\nbility to noise, affine transformation, and angle change.\n\n3.2.1 Basic principle of SIFT algorithm\nIn the process of feature construction by SIFT algorithm,\nit is necessary to deal with multiple details, achieving faster\noperation and higher positioning accuracy. Figure 4 shows\nflow block diagram of SIFT algorithm [21]. The generation\nprocess of local feature is described as follows [22]:\n\nFig. 2 SVM network structure\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 Page 3 of 19\n\n X1 K(x ,x) K(x ,x) Sgn Otrain y train Xtrain ,x) \n\n\n\n① Detect extreme points\nGaussian differential functions are used for image search\n\non all scales, thus identifying potential fixed points.\n② Position key points\nThe scale on candidate position of model is confirmed.\n\nThe stability degree determines the selection of key points.\n③ Determine the direction of key points\nUsing the gradient direction histogram, each key point\n\nis assigned a direction with the highest gradient value to\ndetermine the main direction of key point.\n④ Describe the key points\nThe local gradients of image are calculated and repre-\n\nsented by a kind of symbol.\n\n3.2.2 Key point matching\n3.2.2.1 Scale space Scale space introduces a scale par-\nameter into image matching model. The continuously\nvariable scale parameter is used to obtain the scale space\nsequence. After that, the main contour of scale space is\n\ntaken as the feature vector to extract the edge features\n[23]. The larger scale leads to the more blurred image.\nTherefore, scale space can simulate the formation\nprocess of target on the retina of the human eye.\nScale space of image can be expressed as Eq. (4).\n\nL x; y; σð Þ ¼ G x; y; σð Þ � I x; yð Þ ð4Þ\nIn Eq. (4), G(x, y, σ) is the Gaussian function, I(x, y) the\n\noriginal image, and * the convolution operation.\n\n3.2.2.2 Establishing Gaussian pyramid\n\nG x; y; σð Þ ¼ 1\n2πσ2\n\ne− x−d=2ð Þ2þ y−b=2ð Þ2ð Þ=2σ2 ð5Þ\n\nIn Eq. (5), d and b are the dimensions of Gaussian\ntemplate, (x, y) is the pixel location, and σ the scale space\nfactor.\nGaussian pyramid is established according to Eq. (5),\n\nincluding Gaussian blur and down-sampling (see Fig. 5).\nIt is observed that the pyramids with different sizes con-\nstitute tower model from bottom to top. The original\nimage is used for the first layer, the new image obtained\nby down-sampling for the second layer. There are n\nlayers in each tower. The number of layers can be calcu-\nlated by Eq. (6).\n\nn ¼ log2 minf p; qð Þg−d dϵ 0; log2 minf p; qð Þg½ �\nð6Þ\n\nIn Eq. (6), p and q are the sizes of the original image and d\nis the logarithm of minimum dimension of tower top image.\n\n3.2.2.3 Gaussian difference pyramid After scale\nnormalization of maxima and minima of the Gaussian La-\nplace function σ2∇2G, we obtain the most stable image fea-\ntures using other feature extraction functions. The\nGaussian difference function is approximated to the Gauss-\nian Laplace function σ2∇2G after scale normalization. The\nrelationship is described as follows:\n\n∂G\n∂σ\n\n¼ σ2∇ 2G ð7Þ\n\nDifferential is approximately replaced by the difference:\n\nσ2∇ 2G ¼ ∂G\n∂σ\n\n≈\nG x; y; kσð Þ−G x; y; σð Þ\n\nkσ−σ\nð8Þ\n\nTherefore,\n\nG x; y; kσð Þ−G x; y; σð Þ ≈ k−1ð Þσ2∇ 2G ð9Þ\nIn Eq. (9), k − 1 is a constant.\nIn Fig. 6, the red line is the DoG operator curve; the\n\nblue line the Gauss-Laplacian curve. In extreme detection\n\nFig. 3 The detection process of SVM classifier\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 Page 4 of 19\n\n Start Import Image No Does it include faces yes SVM Classifier detection Output results for face detection End \n\n\n\nmethod, the Laplacian operator is replaced by the DoG\noperator [24] (see Eq. (10).\n\nD x; y; σð Þ ¼ G x; y; kσð Þ−G x; y; σð Þð Þ � I x; yð Þ\n¼ L x; y; kσð Þ−L x; y; σð Þ ð10Þ\n\n3.2.2.4 Spatial extreme detection In Gaussian differ-\nence space, local extreme points constitute the key\npoints. When searching for key points, we compare the\n\nimages between two adjacent layers in the same group.\nAfter that, each pixel point is compared with all the ad-\njacent points to judge whether it is large or small (see\nFig. 6). The red intermediate detection point is com-\npared with 26 points in the surrounding, upper, and\nlower scale spaces to detect extreme points.\nIn the calculation, the Gaussian difference image is the\n\ndifference between the adjacent upper and lower images\nin each group of the Gaussian pyramid (see Fig. 7).\n\n3.2.2.5 Spatial extreme detection In Gaussian differ-\nence space, local extreme points constitute the key\npoints. When searching for key points, we compare the\nimages between two adjacent layers in the same group.\nAfter that, each pixel point is compared with all the\nadjacent points to judge whether it is large or small\n(see Fig. 8). The red intermediate detection point is com-\npared with 26 points in the surrounding, upper, and lower\nscale spaces to detect extreme points.\nIf there are N extreme points in each group, then we\n\nneed N+ 2-layer DoG pyramid and N+ 3-layer Gaussian\npyramid (see Fig. 8). Due to edge response, the extreme\npoints generated in this case are not all stable.\n\n3.2.2.6 Key point matching At first, the key point is\ncharacterized by position, scale, and direction. To main-\ntain the invariance of perspective and illumination\nchanges, the key point should be described by a set of vec-\ntors. Then, the descriptor consists of key points and other\ncontributive pixels. Besides, the independent characteristic\n\nFig. 4 SIFT algorithm flow chart\n\nFig. 5 Gaussian pyramid\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 Page 5 of 19\n\n Image input Image input template input Detection extreme extracti on local characteristic region characteristic set Location the Key points of stability characteristicmatching Describing local characteristic region Dislodge the Wrong image matching characteristic set The input of matching image \n\n Octave5 80 Octave4 40 Octave3 20 Octave2 Octave1 \n\n\n\nof descriptor is guaranteed to improve the probability of\ncorrect matching of feature points.\nThe gradient value of key point is calculated. The gra-\n\ndient value and direction are determined by Eq. (11).\n\nm x; yð Þ ¼\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\nN xþ 1; yð Þ−N x−1; yð Þð Þ2 þ N x; yþ 1ð Þ−N x; y−1ð Þð Þ2\n\nq\n\nθ x; yð Þ ¼ α tan2\nN x; yþ 1ð Þ−N x; y−1ð Þ\nN xþ 1; yð Þ−N x−1; yð Þ\n\n� �\nð11Þ\n\nIn Eq. (11), N represents the scale space value of key point.\nGradient histogram statistics. The gradient and direc-\n\ntion of pixels in the neighborhood are represented by\nhistogram. The direction ranges from 0 to 360°. There is a\n\nFig. 6 Comparison of Gauss-Laplacian and DoG\n\nFig. 7 Gaussian pyramid of each group\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 Page 6 of 19\n\n 02 0.1 -0.1 02 -03 Laplacian - DOG -0.4 \n\n Scale (next octave) Gaussian DOG \n\n\n\nsquare column for every 10°, forming 36 columns [25]\n(see Fig. 9). In feature point field, the peak represents the\ngradient direction. The histogram of maximum is the\nmain direction of key point. Meanwhile, the histogram\nwith peak value greater than 80% of main direction is se-\nlected for auxiliary direction to improve the matching\nrobustness.\nAfter successful matching of key points, the entire al-\n\ngorithm is not over yet. This is because substantial mis-\nmatched points appear in the matching process. These\nmismatched points are eliminated by Ransac method in\nSIFT matching algorithm [26].\n\n3.2.3 Face recognition experiment\nTo evaluate the algorithm, the experiment is conducted\nbased on face infrared database provided by Terravic Re-\nsearch Corporation. There are a total of 20 infrared\nimage sequences with head rotation, glasses, hats, and\nlight-illuminated pictures. Three pairs of images are se-\nlected from each face, with a total of 60 pairs. Figure 10\nshows the selected 120 images. In the work, the classic\n\nSIFT matching algorithm is used as the initial matching\nmethod to manually determine matching accuracy and\nmismatch rate of each group. In other words, the match-\ning performance is described by accuracy and error de-\ngrees. Accuracy is defined by the ratio of the number of\ncorrect matches in total number. Error degree is the ra-\ntio of the number difference (between key and matched\npoints) in the total number of key points.\nThese 120 samples are conducted with abstract match-\n\ning contrast according to the variables including head\nrotation angle, illumination transformation, glasses, and\nhat wearing. Meanwhile, other variables remain the\nsame. Figures 11, 12, 13, and 14 show the matching re-\nsults, respectively:\n\n1. Matching results when head rotation angle changes\n2. Matching results when wearing glasses\n3. Matching results when wearing a hat\n4. Matching results when light and shade change\n\nThe experimental data are shown in Table 1.The ex-\nperimental image and Table 1 show:\n① SIFT matching performance is more easily affected\n\nby wearing glasses than head rotation angle, light illu-\nmination, darkness, and wearing hat.\n② In the case of the same number of matches, the\n\nsuccess rate of SIFT matching is higher than that of the\nHarris matching method [27].\nThe overall trend of results can be well presented al-\n\nthough there are inevitable errors due to the finiteness\nof experimental samples.\n\n3.3 Face alignment\nFace alignment is the positioning of face feature points.\nAfter face image detection, the SIFT algorithm automat-\nically positions the contour points of the eyebrows, eyes,\nnose, and mouth. In the try-on process of AR glasses,\nthe eyes are positioned to estimate the head posture.\nThe pose estimation is applied to the tracking registra-\ntion subsystem of glasses, thus producing perspective\n\nFig. 8 The detection of DoG space extreme point\n\nFig. 9 The histogram of the main direction\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 Page 7 of 19\n\n Scale \n\n The main direction \n\n\n\ntransformation. However, the pose estimation is easily af-\nfected by the positioning of face feature points, resulting\nin estimation error. The feature points are accurately posi-\ntioned to achieve good effect of head pose estimation.\nAt present, there are many face alignment algorithms.\n\nSDM is a method of finding function approximation\nproposed by Zhu et al [28] by calculating the average\nface, and local features around each feature point are ex-\ntracted to form feature vector descriptor. The offset be-\ntween average and real face is calculated to obtain the\nstep size and motion direction for iteration. The current\nface feature points are converged to the optimal position\nby repeated iterations.\nFigure 15 shows the SDM-based face alignment process.\n\nThe face alignment process is described as follows.\n\n3.3.1 Image normalization\nThe image is normalized to achieve face alignment, thus\nimproving the efficiency of training. The face image to be\ntrained is manually labeled with feature points. After rea-\nsonable translation, rotation, and scaling transformation,\nthe image is aligned to the first sample. The sample size is\nunified to arrange the original data information with con-\nfused, reducing interference other than shape factors. Fi-\nnally, the calculated average face is placed on the sample\nas the estimated face. The average is aligned with the ori-\nginal face image in the center.\n\nIt is denoted that x∗ is the optimal solution in face fea-\nture point location, x0 the initialization feature point,\nd(x) ∈ Rn × 1 the coordinates of n feature points in the\nimage, and h the nonlinear feature extraction function\nnear each feature point. If the SIFT features of 128 di-\nmensions are extracted from each feature point, then\nh(d(x)) ∈ R128n × 1. The SIFT feature extracted at x∗ can\nbe expressed as ∅∗ = h(d(x∗)). Then, the face feature\npoint alignment is converted into the operation of solv-\ning Δx, which minimizes Eq. (12).\n\nf x0 þ Δxð Þ ¼ hðd x0 þ Δxð Þk k22 ð12Þ\nThe step size Δx is calculated based on the SDM\n\nalgorithm.\n\nxk ¼ xk þ Δxk ð13Þ\nIf Rk and bk are the paths of each iteration, then\n\nEq. (11) can converge the feature point from the initial\nvalue x0 to x∗.\n\nxkþ1 ¼ xk−1 þ Rk−1∅k−1 þ bk−1 ð14Þ\nDuring training process, {di} is the set of face images,\n\n{di} the set of manually labeled feature points, and x0 the\nfeature point of each image. Face feature point location\nis transformed into a linear regression problem. For the\nproblem, the input feature is the SIFT feature ∅i\n\n0 at x0;\n\nFig. 10 Sample sequence set\n\nFig. 11 Matching results when head rotation angle changes\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 Page 8 of 19\n\n \n\n 50 100 150 200 100 200 300 400 500 600 \n\n\n\nthe result the iteration step size Δxi� ¼ xi� þ Δxi0 from x0\nto x∗; and the objective function Eq. (15).\n\nargminR0b0\n\nX\ndi\n\nX\nxi\n\nΔxi�−R0∅i\n�−b0\n\n\t\t \t\t2\n2 ð15Þ\n\nIn this way, R0 and b0 from the training set are iterated\nto obtain Rk and Rk. The two parameters are used for\nthe test phase to achieve the alignment of test images.\n\n3.3.2 Local feature extraction of SIFT algorithm\nIn the work, the principal component analysis is used to\nreduce the dimension of image [29], the impact of\nnon-critical dimensions, and the amount of data, thus\nimproving the efficiency. After the dimension reduction,\nthe local feature points are extracted from the face\nimage. To improve the alignment accuracy of feature\npoints, the robust SIFT algorithm is applied for local fea-\nture extraction. Section 3.2.2 introduces the extraction\nprocess in detail.\n\n3.3.3 SDM algorithm alignment result\nTraining samples are selected from IBUG and LFW face\ndatabases. The former contains 132 face images. Each\nimage is labeled with 71 face feature points, which are\nsaved in pts file. The latter consists of the sets of test and\ntraining samples, wherein, the set of test sample contains\n206 face images. Each image is labeled with 71 face feature\npoints, which are saved in pts file. The set of training\n\nsample contains 803 face images. Each image is labeled\nwith 68 face feature points. Figures 16 and 17 show frontal\nand lateral face alignment results, respectively.\n\n3.4 Face pose estimation\nBased on computer vision, the pose of object refers to\nits orientation and position relative to the camera. The\npose can be changed by moving the camera or object.\nGeometric model of camera imaging determines the re-\nlationship between 3D geometric position of certain\npoint on head surface and corresponding point of image.\nThese geometric model parameters are camera parame-\nters. In most cases, these parameters are obtained by ex-\nperiments. This process is called labeling [27, 29].\nCamera labeling determines the geometric and optical\nproperties, 3D position, and direction of camera relative\nto certain world coordinate system.\nThe idea of face pose estimation is described as fol-\n\nlows. Firstly, we find the projection relationship between\n2D coordinates on face image and 3D coordinates of\ncorresponding points on 3D face model. Then, the mo-\ntion coordinates of camera are calculated to estimate\nhead posture.\nA 3D rigid object has two movements relative to the camera:\n① Translation movement\nThe camera is moved from current spatial position\n\n(X,Y, Z) to new spatial position (X′,Y′, Z′), which is called\n\nFig. 12 Matching results when wearing glasses\n\nFig. 13 Matching results when wearing a hat\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 Page 9 of 19\n\n 50 100 150 200 100 200 300 400 500 600 \n\n 50 100 150 200 100 200 300 400 500 600 \n\n\n\ntranslation. Translation vector is expressed as τ = (X′ −X,\nY′ −Y, Z′ −Z).\n② Rotary movement\nIf the camera is rotated around the XYZ axis, the rota-\n\ntion has six degrees of freedom. Therefore, pose estima-\ntion of 3D object means finding six numbers (three for\ntranslation and three for rotation).\n\n3.4.1 Feature point labelling\nThe 2D coordinates of N points are determined to calcu-\nlate 3D coordinates of points, thus obtaining 3D pose of\nobject in an image.\nTo determine the 2D coordinates of N points, we se-\n\nlect the points with rigid body invariance, such as the\nnose tip, corners of eyes, and mouth. In the work, there\nare six points including the nose tip, chin, left, and right\ncorners of eyes and mouth.\nSFM (Surrey Face Model) is used as general 3D face\n\nmodel to obtain 3D coordinates corresponding to se-\nlected 2D coordinates [30]. By manual labeling, we ob-\ntain the 3D coordinates (x, y, z) of six points for pose\nestimation. These points are called world coordinates in\nsome arbitrary reference/coordinate system.\n\n3.4.2 Camera labeling\nAfter determining world coordinates, the camera is reg-\nistered to obtain the camera matrix, namely focal length\nof camera, optical center, and radial distortion parame-\nters of image. Therefore, camera labeling is required. In\nthe work, the camera is labeled by Yang and Patras [31]\nto obtain the camera matrix.\n\n3.4.3 Feature point mapping\nFigure 18 shows the world, camera, and image coordin-\nate systems. In Fig. 18, O is the center of camera, c the\noptical center of 2D image plane, P the point in world\ncoordinate system, and P′ the projection of P on image\nplane. P′ can be determined according to the projection\nof the P point.\nIt is denoted that the world coordinate of P is (U,V, W).\n\nBesides, the known parameters are the rotation matrix R\n\nFig. 14 Matching results when light and shade change\n\nTable 1 Match result analysis table\n\nVariate Number of\nmatches\n\nTotal number\nof key points\n\nMatch\nratio\n\nFalse\nmatch rate\n\nHead rotation 18 158 0.129 0.871\n\nWearing glasses 15 167 0.099 0.901\n\nWearing a hat 21 106 0.247 0.753\n\nLight and shade\nchange\n\n45 281 0.191 0.809\nFig. 15 The face alignment process based on SDM\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 Page 10 of 19\n\n 50 100 150 200 \n\n Start Input the training face sample Image normalization. Calculate the average face Extract local features SDM Iterative Solution Generate face detection model End \n\n\n\n(matrix 3 × 3) and translation vector τ (vector 3 × 1) from\ncamera to world coordinate. It is possible to determine\nposition O(X, Y, Z) of P in camera coordinate system.\n\nx\ny\nz\n\n2\n4\n\n3\n5 ¼ R\n\nu\nv\nw\n\n2\n4\n\n3\n5þ τ⇒\n\nx\ny\nz\n\n2\n4\n\n3\n5 ¼ Rjτ½ �\n\nu\nv\nw\n\n2\n4\n\n3\n5 ð16Þ\n\nEquation (16) is expanded as follows:\n\nx\ny\nz\n\n2\n4\n\n3\n5 ¼\n\nr00 r01 r02 τx\nr10 r11 r12 τy\nr20 r21 r22 τz\n\n2\n4\n\n3\n5\n\nu\nv\nw\nl\n\n2\n664\n\n3\n775 ð17Þ\n\nIf plenty of points are mapped to (X, Y, Z) and (U,V, W),\nthe above problem is transformed into a system of linear\nequations with unknown (τx, τy, τz) . Then, the system of\nlinear equations can be solved.\nFirstly, the six points on 3D model are manually la-\n\nbeled to derive their world coordinates (U, V, W). Equa-\ntion (18) is used to determine 2D coordinates (X, Y) of\nsix points in image coordinate system.\n\nx\ny\n1\n\n2\n4\n\n3\n5 ¼ S\n\nf x 0 0\n0 f y 0\n0 0 1\n\n2\n4\n\n3\n5 x\n\ny\nZ\n\n2\n4\n\n3\n5 ð18Þ\n\nwhere fx and fy are the focal lengths in the x and y direc-\ntions, (cx, cy) is the optical center, and S the unknown scaling\nfactor. If P in 3D is connected to O, then P′ where light in-\ntersects image plane is the same image connecting all points\nin the center of the camera produced by P along the ray.\nEquation (18) is converted to the following form:\n\nS\nX\nY\nZ\n\n2\n4\n\n3\n5 ¼\n\nr00 r01 r02 τx\nr10 r11 r12 τy\nr20 r21 r22 τz\n\n2\n4\n\n3\n5\n\nu\nv\nw\nl\n\n2\n664\n\n3\n775 ð19Þ\n\nThe image and world coordinates are known in the\nwork. Therefore, Eqs. (18) and (19) are transformed into\nthe following form:\n\nx\ny\n1\n\n2\n4\n\n3\n5 ¼ S\n\nf x 0 0\n0 f y 0\n0 0 1\n\n2\n4\n\n3\n5 r00 r01 r02 τx\n\nr10 r11 r12 τy\nr20 r21 r22 τz\n\n2\n4\n\n3\n5\n\nu\nv\nw\nl\n\n2\n664\n\n3\n775 ð20Þ\n\nIf the correct poses R and τ are known, then the 2D\nposition of 3D facial point on image can be predicted by\nprojecting the 3D point onto the image (see Eq. (20)).\nThe 2D facial feature points are known. Pose estimation\ncan be performed by calculating the distance between\nthe projected 3D point and 2D facial feature. If the pose\nis correctly estimated, the 3D points projected onto\nimage plane will almost coincide with the 2D facial fea-\ntures. Otherwise, the re-projection error can be mea-\nsured. The least square method is used to calculate the\nsum of squares of the distance between the projected 3D\nand 2D facial feature points.\n\n3.5 Tracking registration system\nTracking registration technology is the process of align-\ning computer-generated virtual objects with scenes in\nthe real world. At present, there are two tracking regis-\ntration techniques. The first superimposes certain point\nof face feature with a point of virtual glasses based on\nthe face feature point tracking method [32]. The second\nis based on the geometric transformation relation track-\ning method. Face geometry and virtual glasses model are\nconducted with affine transformation. Virtual glasses\nmodel moves with the movement of human head, mak-\ning corresponding perspective changes and realizing 3D\ntry-on effect [33]. For the first technique, the virtual\nglasses cannot be changed with the movement of user\nhead, causing poor user experience. The second tech-\nnique has good tacking effect. The virtual glasses will be\ndistorted with overlarge head corner. Combined with the\ntwo methods, the glasses model is conducted with per-\nspective transformation using six degrees of freedom ob-\ntained by pose estimation in Section 3.3. After face\nsuperposition, accurate tracking is realized through bet-\nter stereoscopic changes.\n\nFig. 16 The picture", "text": [ "", "Face Macthing Recognition Recognition Results CLassification Face Result Input Feature Feature Searching Extraction Dimensionality Image Tracing Offline Learning Training Feature Feature Image Extraction Space Testing Feature Feature Image Extraction Macthing Online Learning", "X1 K(x ,x) K(x ,x) Sgn Otrain y train Xtrain ,x)", "Start Import Image No Does it include faces yes SVM Classifier detection Output results for face detection End", "Image input Image input template input Detection extreme extracti on local characteristic region characteristic set Location the Key points of stability characteristicmatching Describing local characteristic region Dislodge the Wrong image matching characteristic set The input of matching image", "Octave5 80 Octave4 40 Octave3 20 Octave2 Octave1", "02 0.1 -0.1 02 -03 Laplacian - DOG -0.4", "Scale (next octave) Gaussian DOG", "Scale", "The main direction", "", "50 100 150 200 100 200 300 400 500 600", "50 100 150 200 100 200 300 400 500 600", "50 100 150 200 100 200 300 400 500 600", "50 100 150 200", "Start Input the training face sample Image normalization. Calculate the average face Extract local features SDM Iterative Solution Generate face detection model End", "", "", "w P x World coordinate systems U C O Z plane-coordinate system R, T X Y Camera coordinate systems", "C", "B", "Real world coordinates Virtual world coordinates actual environment Glasses model Scene acquisition Tracking registry Model generation system system Virtual reality synthesis system Coordinates of real Coordinates of the world cameras virtual world camera display system User UI", "Augmented reality glasses sales system Commodity User Collection shopping Order discount photo setting browsing of goods trolley coupon wall up commodity list Glasses photogr uploading sharing Show photos comments sharing Try on browsing screening try-on aphing of trying on and likes quickly commodity Product modification Address Order statistics and deletion management Generated", "Start Import Image Input the training face sample Is there a face? Image normalization, Calculate the average face SVM classitier SIFT Extract local features Is there a face? SDM Iterative Solution Feature extraction Face Alignment Feature point Face rotation Calculate the perspective relationship The lens selection Angle between 2D and 3D coordinates library Shoot the The glasses 3d effect Transparency perspective picking try-on effect overlap the face design treatment transformation out a lens End", "Published online: 27 November 2018", "-108 PmBY 19:18 @¥( 100%* 15:48 < # AGLASSES = GLASSES Rayban xfett RB33 ... . - ...... *FEWQrb3025 112 ... ¥ 450 ¥ 560 2 3 5 6 7 8 O i - & @ .? ! ABC space Done V ¥ 1010" ], "layoutText": [ "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}", "{\"language\":\"en\",\"text\":\"Face Macthing Recognition Recognition Results CLassification Face Result Input Feature Feature Searching Extraction Dimensionality Image Tracing Offline Learning Training Feature Feature Image Extraction Space Testing Feature Feature Image Extraction Macthing Online Learning\",\"lines\":[{\"boundingBox\":[{\"x\":1000,\"y\":15},{\"x\":1201,\"y\":16},{\"x\":1201,\"y\":49},{\"x\":1000,\"y\":48}],\"text\":\"Face Macthing\"},{\"boundingBox\":[{\"x\":1248,\"y\":21},{\"x\":1405,\"y\":21},{\"x\":1405,\"y\":47},{\"x\":1248,\"y\":47}],\"text\":\"Recognition\"},{\"boundingBox\":[{\"x\":1026,\"y\":55},{\"x\":1181,\"y\":56},{\"x\":1180,\"y\":83},{\"x\":1026,\"y\":82}],\"text\":\"Recognition\"},{\"boundingBox\":[{\"x\":1278,\"y\":78},{\"x\":1376,\"y\":78},{\"x\":1376,\"y\":104},{\"x\":1278,\"y\":103}],\"text\":\"Results\"},{\"boundingBox\":[{\"x\":1011,\"y\":153},{\"x\":1194,\"y\":155},{\"x\":1194,\"y\":182},{\"x\":1011,\"y\":180}],\"text\":\"CLassification\"},{\"boundingBox\":[{\"x\":146,\"y\":180},{\"x\":207,\"y\":181},{\"x\":206,\"y\":205},{\"x\":145,\"y\":203}],\"text\":\"Face\"},{\"boundingBox\":[{\"x\":1061,\"y\":191},{\"x\":1146,\"y\":191},{\"x\":1146,\"y\":217},{\"x\":1062,\"y\":218}],\"text\":\"Result\"},{\"boundingBox\":[{\"x\":6,\"y\":194},{\"x\":78,\"y\":197},{\"x\":78,\"y\":221},{\"x\":8,\"y\":222}],\"text\":\"Input\"},{\"boundingBox\":[{\"x\":348,\"y\":197},{\"x\":448,\"y\":199},{\"x\":447,\"y\":225},{\"x\":348,\"y\":222}],\"text\":\"Feature\"},{\"boundingBox\":[{\"x\":596,\"y\":197},{\"x\":694,\"y\":198},{\"x\":693,\"y\":225},{\"x\":595,\"y\":223}],\"text\":\"Feature\"},{\"boundingBox\":[{\"x\":103,\"y\":212},{\"x\":240,\"y\":215},{\"x\":239,\"y\":248},{\"x\":102,\"y\":244}],\"text\":\"Searching\"},{\"boundingBox\":[{\"x\":332,\"y\":235},{\"x\":463,\"y\":236},{\"x\":462,\"y\":259},{\"x\":332,\"y\":258}],\"text\":\"Extraction\"},{\"boundingBox\":[{\"x\":546,\"y\":233},{\"x\":741,\"y\":234},{\"x\":741,\"y\":260},{\"x\":546,\"y\":260}],\"text\":\"Dimensionality\"},{\"boundingBox\":[{\"x\":9,\"y\":251},{\"x\":92,\"y\":253},{\"x\":92,\"y\":278},{\"x\":8,\"y\":277}],\"text\":\"Image\"},{\"boundingBox\":[{\"x\":123,\"y\":252},{\"x\":227,\"y\":255},{\"x\":227,\"y\":283},{\"x\":123,\"y\":280}],\"text\":\"Tracing\"},{\"boundingBox\":[{\"x\":873,\"y\":278},{\"x\":1091,\"y\":282},{\"x\":1090,\"y\":311},{\"x\":873,\"y\":305}],\"text\":\"Offline Learning\"},{\"boundingBox\":[{\"x\":862,\"y\":327},{\"x\":973,\"y\":329},{\"x\":972,\"y\":354},{\"x\":862,\"y\":351}],\"text\":\"Training\"},{\"boundingBox\":[{\"x\":1056,\"y\":325},{\"x\":1152,\"y\":326},{\"x\":1152,\"y\":350},{\"x\":1056,\"y\":348}],\"text\":\"Feature\"},{\"boundingBox\":[{\"x\":1260,\"y\":324},{\"x\":1357,\"y\":325},{\"x\":1356,\"y\":350},{\"x\":1259,\"y\":348}],\"text\":\"Feature\"},{\"boundingBox\":[{\"x\":874,\"y\":365},{\"x\":957,\"y\":366},{\"x\":957,\"y\":391},{\"x\":873,\"y\":389}],\"text\":\"Image\"},{\"boundingBox\":[{\"x\":1039,\"y\":361},{\"x\":1173,\"y\":362},{\"x\":1172,\"y\":387},{\"x\":1039,\"y\":385}],\"text\":\"Extraction\"},{\"boundingBox\":[{\"x\":1266,\"y\":362},{\"x\":1345,\"y\":362},{\"x\":1345,\"y\":388},{\"x\":1266,\"y\":387}],\"text\":\"Space\"},{\"boundingBox\":[{\"x\":872,\"y\":468},{\"x\":968,\"y\":472},{\"x\":966,\"y\":498},{\"x\":872,\"y\":493}],\"text\":\"Testing\"},{\"boundingBox\":[{\"x\":1070,\"y\":468},{\"x\":1167,\"y\":470},{\"x\":1167,\"y\":494},{\"x\":1069,\"y\":492}],\"text\":\"Feature\"},{\"boundingBox\":[{\"x\":1260,\"y\":468},{\"x\":1356,\"y\":470},{\"x\":1355,\"y\":494},{\"x\":1259,\"y\":491}],\"text\":\"Feature\"},{\"boundingBox\":[{\"x\":877,\"y\":505},{\"x\":961,\"y\":509},{\"x\":959,\"y\":534},{\"x\":875,\"y\":530}],\"text\":\"Image\"},{\"boundingBox\":[{\"x\":1053,\"y\":505},{\"x\":1184,\"y\":507},{\"x\":1184,\"y\":531},{\"x\":1053,\"y\":530}],\"text\":\"Extraction\"},{\"boundingBox\":[{\"x\":1247,\"y\":504},{\"x\":1368,\"y\":507},{\"x\":1367,\"y\":534},{\"x\":1246,\"y\":530}],\"text\":\"Macthing\"},{\"boundingBox\":[{\"x\":881,\"y\":552},{\"x\":1094,\"y\":553},{\"x\":1093,\"y\":581},{\"x\":881,\"y\":579}],\"text\":\"Online Learning\"}],\"words\":[{\"boundingBox\":[{\"x\":1000,\"y\":15},{\"x\":1063,\"y\":16},{\"x\":1062,\"y\":48},{\"x\":1000,\"y\":48}],\"text\":\"Face\"},{\"boundingBox\":[{\"x\":1069,\"y\":16},{\"x\":1202,\"y\":16},{\"x\":1202,\"y\":50},{\"x\":1069,\"y\":48}],\"text\":\"Macthing\"},{\"boundingBox\":[{\"x\":1249,\"y\":22},{\"x\":1404,\"y\":22},{\"x\":1405,\"y\":48},{\"x\":1249,\"y\":48}],\"text\":\"Recognition\"},{\"boundingBox\":[{\"x\":1027,\"y\":56},{\"x\":1180,\"y\":56},{\"x\":1180,\"y\":84},{\"x\":1026,\"y\":83}],\"text\":\"Recognition\"},{\"boundingBox\":[{\"x\":1279,\"y\":78},{\"x\":1376,\"y\":79},{\"x\":1376,\"y\":104},{\"x\":1279,\"y\":104}],\"text\":\"Results\"},{\"boundingBox\":[{\"x\":1012,\"y\":154},{\"x\":1194,\"y\":156},{\"x\":1194,\"y\":182},{\"x\":1011,\"y\":181}],\"text\":\"CLassification\"},{\"boundingBox\":[{\"x\":145,\"y\":180},{\"x\":206,\"y\":181},{\"x\":205,\"y\":205},{\"x\":145,\"y\":203}],\"text\":\"Face\"},{\"boundingBox\":[{\"x\":1062,\"y\":193},{\"x\":1146,\"y\":192},{\"x\":1146,\"y\":218},{\"x\":1062,\"y\":218}],\"text\":\"Result\"},{\"boundingBox\":[{\"x\":6,\"y\":194},{\"x\":78,\"y\":195},{\"x\":78,\"y\":222},{\"x\":6,\"y\":221}],\"text\":\"Input\"},{\"boundingBox\":[{\"x\":348,\"y\":197},{\"x\":448,\"y\":200},{\"x\":448,\"y\":225},{\"x\":348,\"y\":223}],\"text\":\"Feature\"},{\"boundingBox\":[{\"x\":596,\"y\":198},{\"x\":693,\"y\":200},{\"x\":693,\"y\":225},{\"x\":596,\"y\":224}],\"text\":\"Feature\"},{\"boundingBox\":[{\"x\":106,\"y\":213},{\"x\":240,\"y\":216},{\"x\":239,\"y\":248},{\"x\":105,\"y\":245}],\"text\":\"Searching\"},{\"boundingBox\":[{\"x\":333,\"y\":235},{\"x\":463,\"y\":237},{\"x\":462,\"y\":260},{\"x\":332,\"y\":259}],\"text\":\"Extraction\"},{\"boundingBox\":[{\"x\":547,\"y\":234},{\"x\":741,\"y\":235},{\"x\":740,\"y\":261},{\"x\":546,\"y\":260}],\"text\":\"Dimensionality\"},{\"boundingBox\":[{\"x\":9,\"y\":252},{\"x\":93,\"y\":254},{\"x\":92,\"y\":278},{\"x\":9,\"y\":277}],\"text\":\"Image\"},{\"boundingBox\":[{\"x\":129,\"y\":253},{\"x\":228,\"y\":256},{\"x\":226,\"y\":284},{\"x\":129,\"y\":280}],\"text\":\"Tracing\"},{\"boundingBox\":[{\"x\":874,\"y\":278},{\"x\":967,\"y\":279},{\"x\":967,\"y\":306},{\"x\":875,\"y\":305}],\"text\":\"Offline\"},{\"boundingBox\":[{\"x\":972,\"y\":279},{\"x\":1091,\"y\":284},{\"x\":1090,\"y\":312},{\"x\":972,\"y\":306}],\"text\":\"Learning\"},{\"boundingBox\":[{\"x\":862,\"y\":327},{\"x\":973,\"y\":330},{\"x\":973,\"y\":354},{\"x\":862,\"y\":351}],\"text\":\"Training\"},{\"boundingBox\":[{\"x\":1057,\"y\":326},{\"x\":1152,\"y\":327},{\"x\":1152,\"y\":350},{\"x\":1056,\"y\":349}],\"text\":\"Feature\"},{\"boundingBox\":[{\"x\":1260,\"y\":324},{\"x\":1357,\"y\":326},{\"x\":1357,\"y\":350},{\"x\":1260,\"y\":348}],\"text\":\"Feature\"},{\"boundingBox\":[{\"x\":874,\"y\":365},{\"x\":956,\"y\":366},{\"x\":956,\"y\":392},{\"x\":874,\"y\":389}],\"text\":\"Image\"},{\"boundingBox\":[{\"x\":1040,\"y\":361},{\"x\":1172,\"y\":363},{\"x\":1170,\"y\":388},{\"x\":1039,\"y\":386}],\"text\":\"Extraction\"},{\"boundingBox\":[{\"x\":1267,\"y\":362},{\"x\":1345,\"y\":363},{\"x\":1345,\"y\":388},{\"x\":1267,\"y\":388}],\"text\":\"Space\"},{\"boundingBox\":[{\"x\":873,\"y\":468},{\"x\":967,\"y\":473},{\"x\":966,\"y\":498},{\"x\":873,\"y\":494}],\"text\":\"Testing\"},{\"boundingBox\":[{\"x\":1070,\"y\":469},{\"x\":1166,\"y\":470},{\"x\":1166,\"y\":494},{\"x\":1070,\"y\":492}],\"text\":\"Feature\"},{\"boundingBox\":[{\"x\":1260,\"y\":468},{\"x\":1355,\"y\":471},{\"x\":1355,\"y\":494},{\"x\":1260,\"y\":492}],\"text\":\"Feature\"},{\"boundingBox\":[{\"x\":877,\"y\":505},{\"x\":961,\"y\":509},{\"x\":960,\"y\":535},{\"x\":876,\"y\":531}],\"text\":\"Image\"},{\"boundingBox\":[{\"x\":1055,\"y\":506},{\"x\":1184,\"y\":508},{\"x\":1183,\"y\":532},{\"x\":1053,\"y\":530}],\"text\":\"Extraction\"},{\"boundingBox\":[{\"x\":1247,\"y\":505},{\"x\":1369,\"y\":507},{\"x\":1368,\"y\":535},{\"x\":1247,\"y\":530}],\"text\":\"Macthing\"},{\"boundingBox\":[{\"x\":881,\"y\":552},{\"x\":971,\"y\":553},{\"x\":971,\"y\":580},{\"x\":882,\"y\":579}],\"text\":\"Online\"},{\"boundingBox\":[{\"x\":976,\"y\":553},{\"x\":1094,\"y\":554},{\"x\":1094,\"y\":582},{\"x\":976,\"y\":580}],\"text\":\"Learning\"}]}", "{\"language\":\"en\",\"text\":\"X1 K(x ,x) K(x ,x) Sgn Otrain y train Xtrain ,x)\",\"lines\":[{\"boundingBox\":[{\"x\":20,\"y\":34},{\"x\":57,\"y\":34},{\"x\":57,\"y\":71},{\"x\":19,\"y\":71}],\"text\":\"X1\"},{\"boundingBox\":[{\"x\":370,\"y\":31},{\"x\":527,\"y\":34},{\"x\":526,\"y\":74},{\"x\":370,\"y\":70}],\"text\":\"K(x ,x)\"},{\"boundingBox\":[{\"x\":369,\"y\":196},{\"x\":523,\"y\":199},{\"x\":521,\"y\":241},{\"x\":369,\"y\":235}],\"text\":\"K(x ,x)\"},{\"boundingBox\":[{\"x\":899,\"y\":201},{\"x\":971,\"y\":204},{\"x\":970,\"y\":237},{\"x\":898,\"y\":235}],\"text\":\"Sgn\"},{\"boundingBox\":[{\"x\":616,\"y\":301},{\"x\":752,\"y\":230},{\"x\":768,\"y\":260},{\"x\":631,\"y\":332}],\"text\":\"Otrain y train\"},{\"boundingBox\":[{\"x\":4,\"y\":373},{\"x\":80,\"y\":383},{\"x\":79,\"y\":409},{\"x\":3,\"y\":403}],\"text\":\"Xtrain\"},{\"boundingBox\":[{\"x\":489,\"y\":385},{\"x\":539,\"y\":381},{\"x\":541,\"y\":418},{\"x\":491,\"y\":421}],\"text\":\",x)\"}],\"words\":[{\"boundingBox\":[{\"x\":19,\"y\":34},{\"x\":56,\"y\":34},{\"x\":56,\"y\":71},{\"x\":19,\"y\":71}],\"text\":\"X1\"},{\"boundingBox\":[{\"x\":370,\"y\":33},{\"x\":451,\"y\":33},{\"x\":452,\"y\":72},{\"x\":372,\"y\":71}],\"text\":\"K(x\"},{\"boundingBox\":[{\"x\":466,\"y\":33},{\"x\":524,\"y\":36},{\"x\":525,\"y\":74},{\"x\":467,\"y\":73}],\"text\":\",x)\"},{\"boundingBox\":[{\"x\":369,\"y\":197},{\"x\":454,\"y\":197},{\"x\":454,\"y\":237},{\"x\":369,\"y\":236}],\"text\":\"K(x\"},{\"boundingBox\":[{\"x\":467,\"y\":198},{\"x\":521,\"y\":202},{\"x\":520,\"y\":242},{\"x\":467,\"y\":238}],\"text\":\",x)\"},{\"boundingBox\":[{\"x\":899,\"y\":201},{\"x\":969,\"y\":203},{\"x\":968,\"y\":237},{\"x\":898,\"y\":234}],\"text\":\"Sgn\"},{\"boundingBox\":[{\"x\":622,\"y\":300},{\"x\":681,\"y\":267},{\"x\":696,\"y\":297},{\"x\":637,\"y\":329}],\"text\":\"Otrain\"},{\"boundingBox\":[{\"x\":686,\"y\":264},{\"x\":692,\"y\":261},{\"x\":707,\"y\":292},{\"x\":701,\"y\":294}],\"text\":\"y\"},{\"boundingBox\":[{\"x\":698,\"y\":258},{\"x\":752,\"y\":231},{\"x\":767,\"y\":262},{\"x\":712,\"y\":289}],\"text\":\"train\"},{\"boundingBox\":[{\"x\":10,\"y\":374},{\"x\":81,\"y\":381},{\"x\":78,\"y\":410},{\"x\":7,\"y\":403}],\"text\":\"Xtrain\"},{\"boundingBox\":[{\"x\":489,\"y\":384},{\"x\":538,\"y\":381},{\"x\":540,\"y\":417},{\"x\":491,\"y\":421}],\"text\":\",x)\"}]}", "{\"language\":\"en\",\"text\":\"Start Import Image No Does it include faces yes SVM Classifier detection Output results for face detection End\",\"lines\":[{\"boundingBox\":[{\"x\":404,\"y\":32},{\"x\":531,\"y\":33},{\"x\":529,\"y\":79},{\"x\":403,\"y\":77}],\"text\":\"Start\"},{\"boundingBox\":[{\"x\":287,\"y\":264},{\"x\":644,\"y\":268},{\"x\":643,\"y\":321},{\"x\":287,\"y\":316}],\"text\":\"Import Image\"},{\"boundingBox\":[{\"x\":16,\"y\":494},{\"x\":96,\"y\":494},{\"x\":95,\"y\":542},{\"x\":15,\"y\":542}],\"text\":\"No\"},{\"boundingBox\":[{\"x\":198,\"y\":554},{\"x\":734,\"y\":554},{\"x\":734,\"y\":610},{\"x\":198,\"y\":609}],\"text\":\"Does it include faces\"},{\"boundingBox\":[{\"x\":533,\"y\":742},{\"x\":626,\"y\":737},{\"x\":628,\"y\":784},{\"x\":534,\"y\":788}],\"text\":\"yes\"},{\"boundingBox\":[{\"x\":137,\"y\":899},{\"x\":796,\"y\":900},{\"x\":796,\"y\":956},{\"x\":137,\"y\":954}],\"text\":\"SVM Classifier detection\"},{\"boundingBox\":[{\"x\":239,\"y\":1202},{\"x\":698,\"y\":1196},{\"x\":699,\"y\":1251},{\"x\":239,\"y\":1256}],\"text\":\"Output results for\"},{\"boundingBox\":[{\"x\":287,\"y\":1272},{\"x\":649,\"y\":1274},{\"x\":649,\"y\":1328},{\"x\":287,\"y\":1326}],\"text\":\"face detection\"},{\"boundingBox\":[{\"x\":415,\"y\":1498},{\"x\":519,\"y\":1497},{\"x\":520,\"y\":1546},{\"x\":415,\"y\":1545}],\"text\":\"End\"}],\"words\":[{\"boundingBox\":[{\"x\":403,\"y\":32},{\"x\":529,\"y\":33},{\"x\":528,\"y\":79},{\"x\":403,\"y\":77}],\"text\":\"Start\"},{\"boundingBox\":[{\"x\":288,\"y\":264},{\"x\":466,\"y\":266},{\"x\":465,\"y\":320},{\"x\":288,\"y\":318}],\"text\":\"Import\"},{\"boundingBox\":[{\"x\":476,\"y\":266},{\"x\":644,\"y\":269},{\"x\":642,\"y\":321},{\"x\":475,\"y\":320}],\"text\":\"Image\"},{\"boundingBox\":[{\"x\":15,\"y\":494},{\"x\":95,\"y\":494},{\"x\":95,\"y\":542},{\"x\":15,\"y\":542}],\"text\":\"No\"},{\"boundingBox\":[{\"x\":199,\"y\":556},{\"x\":327,\"y\":555},{\"x\":328,\"y\":609},{\"x\":199,\"y\":607}],\"text\":\"Does\"},{\"boundingBox\":[{\"x\":337,\"y\":555},{\"x\":381,\"y\":555},{\"x\":382,\"y\":609},{\"x\":338,\"y\":609}],\"text\":\"it\"},{\"boundingBox\":[{\"x\":391,\"y\":555},{\"x\":587,\"y\":554},{\"x\":587,\"y\":611},{\"x\":392,\"y\":609}],\"text\":\"include\"},{\"boundingBox\":[{\"x\":597,\"y\":554},{\"x\":733,\"y\":554},{\"x\":732,\"y\":611},{\"x\":597,\"y\":611}],\"text\":\"faces\"},{\"boundingBox\":[{\"x\":535,\"y\":741},{\"x\":626,\"y\":737},{\"x\":628,\"y\":783},{\"x\":537,\"y\":788}],\"text\":\"yes\"},{\"boundingBox\":[{\"x\":138,\"y\":903},{\"x\":266,\"y\":901},{\"x\":265,\"y\":955},{\"x\":137,\"y\":954}],\"text\":\"SVM\"},{\"boundingBox\":[{\"x\":292,\"y\":900},{\"x\":538,\"y\":900},{\"x\":538,\"y\":956},{\"x\":292,\"y\":955}],\"text\":\"Classifier\"},{\"boundingBox\":[{\"x\":548,\"y\":900},{\"x\":793,\"y\":904},{\"x\":794,\"y\":957},{\"x\":548,\"y\":956}],\"text\":\"detection\"},{\"boundingBox\":[{\"x\":241,\"y\":1203},{\"x\":414,\"y\":1200},{\"x\":413,\"y\":1256},{\"x\":239,\"y\":1255}],\"text\":\"Output\"},{\"boundingBox\":[{\"x\":425,\"y\":1200},{\"x\":605,\"y\":1197},{\"x\":604,\"y\":1253},{\"x\":423,\"y\":1256}],\"text\":\"results\"},{\"boundingBox\":[{\"x\":615,\"y\":1197},{\"x\":697,\"y\":1196},{\"x\":696,\"y\":1249},{\"x\":614,\"y\":1252}],\"text\":\"for\"},{\"boundingBox\":[{\"x\":289,\"y\":1273},{\"x\":393,\"y\":1273},{\"x\":391,\"y\":1328},{\"x\":287,\"y\":1327}],\"text\":\"face\"},{\"boundingBox\":[{\"x\":403,\"y\":1273},{\"x\":646,\"y\":1276},{\"x\":647,\"y\":1328},{\"x\":402,\"y\":1328}],\"text\":\"detection\"},{\"boundingBox\":[{\"x\":415,\"y\":1497},{\"x\":520,\"y\":1497},{\"x\":520,\"y\":1546},{\"x\":415,\"y\":1546}],\"text\":\"End\"}]}", "{\"language\":\"en\",\"text\":\"Image input Image input template input Detection extreme extracti on local characteristic region characteristic set Location the Key points of stability characteristicmatching Describing local characteristic region Dislodge the Wrong image matching characteristic set The input of matching image\",\"lines\":[{\"boundingBox\":[{\"x\":160,\"y\":101},{\"x\":355,\"y\":103},{\"x\":354,\"y\":141},{\"x\":160,\"y\":139}],\"text\":\"Image input\"},{\"boundingBox\":[{\"x\":675,\"y\":183},{\"x\":869,\"y\":184},{\"x\":868,\"y\":222},{\"x\":674,\"y\":220}],\"text\":\"Image input\"},{\"boundingBox\":[{\"x\":1120,\"y\":184},{\"x\":1350,\"y\":183},{\"x\":1350,\"y\":220},{\"x\":1120,\"y\":221}],\"text\":\"template input\"},{\"boundingBox\":[{\"x\":108,\"y\":259},{\"x\":410,\"y\":261},{\"x\":410,\"y\":296},{\"x\":108,\"y\":294}],\"text\":\"Detection extreme\"},{\"boundingBox\":[{\"x\":661,\"y\":304},{\"x\":907,\"y\":302},{\"x\":907,\"y\":336},{\"x\":661,\"y\":337}],\"text\":\"extracti on local\"},{\"boundingBox\":[{\"x\":621,\"y\":349},{\"x\":952,\"y\":352},{\"x\":952,\"y\":385},{\"x\":621,\"y\":382}],\"text\":\"characteristic region\"},{\"boundingBox\":[{\"x\":1101,\"y\":336},{\"x\":1380,\"y\":336},{\"x\":1380,\"y\":369},{\"x\":1101,\"y\":370}],\"text\":\"characteristic set\"},{\"boundingBox\":[{\"x\":63,\"y\":431},{\"x\":449,\"y\":432},{\"x\":449,\"y\":472},{\"x\":63,\"y\":469}],\"text\":\"Location the Key points\"},{\"boundingBox\":[{\"x\":173,\"y\":480},{\"x\":347,\"y\":480},{\"x\":346,\"y\":520},{\"x\":173,\"y\":518}],\"text\":\"of stability\"},{\"boundingBox\":[{\"x\":825,\"y\":542},{\"x\":1192,\"y\":542},{\"x\":1192,\"y\":582},{\"x\":825,\"y\":580}],\"text\":\"characteristicmatching\"},{\"boundingBox\":[{\"x\":119,\"y\":620},{\"x\":395,\"y\":620},{\"x\":395,\"y\":665},{\"x\":120,\"y\":666}],\"text\":\"Describing local\"},{\"boundingBox\":[{\"x\":95,\"y\":672},{\"x\":427,\"y\":674},{\"x\":427,\"y\":711},{\"x\":95,\"y\":708}],\"text\":\"characteristic region\"},{\"boundingBox\":[{\"x\":844,\"y\":671},{\"x\":1170,\"y\":672},{\"x\":1170,\"y\":713},{\"x\":844,\"y\":710}],\"text\":\"Dislodge the Wrong\"},{\"boundingBox\":[{\"x\":875,\"y\":718},{\"x\":1136,\"y\":718},{\"x\":1136,\"y\":758},{\"x\":875,\"y\":758}],\"text\":\"image matching\"},{\"boundingBox\":[{\"x\":124,\"y\":847},{\"x\":403,\"y\":847},{\"x\":403,\"y\":884},{\"x\":124,\"y\":884}],\"text\":\"characteristic set\"},{\"boundingBox\":[{\"x\":914,\"y\":849},{\"x\":1113,\"y\":850},{\"x\":1113,\"y\":886},{\"x\":914,\"y\":885}],\"text\":\"The input of\"},{\"boundingBox\":[{\"x\":884,\"y\":901},{\"x\":1148,\"y\":902},{\"x\":1148,\"y\":937},{\"x\":884,\"y\":935}],\"text\":\"matching image\"}],\"words\":[{\"boundingBox\":[{\"x\":161,\"y\":102},{\"x\":260,\"y\":103},{\"x\":261,\"y\":141},{\"x\":162,\"y\":138}],\"text\":\"Image\"},{\"boundingBox\":[{\"x\":268,\"y\":103},{\"x\":356,\"y\":105},{\"x\":355,\"y\":139},{\"x\":268,\"y\":141}],\"text\":\"input\"},{\"boundingBox\":[{\"x\":676,\"y\":183},{\"x\":777,\"y\":185},{\"x\":776,\"y\":222},{\"x\":675,\"y\":219}],\"text\":\"Image\"},{\"boundingBox\":[{\"x\":784,\"y\":185},{\"x\":868,\"y\":185},{\"x\":868,\"y\":221},{\"x\":783,\"y\":222}],\"text\":\"input\"},{\"boundingBox\":[{\"x\":1121,\"y\":185},{\"x\":1257,\"y\":184},{\"x\":1256,\"y\":221},{\"x\":1121,\"y\":219}],\"text\":\"template\"},{\"boundingBox\":[{\"x\":1263,\"y\":184},{\"x\":1349,\"y\":184},{\"x\":1349,\"y\":219},{\"x\":1263,\"y\":221}],\"text\":\"input\"},{\"boundingBox\":[{\"x\":109,\"y\":259},{\"x\":269,\"y\":262},{\"x\":268,\"y\":296},{\"x\":108,\"y\":295}],\"text\":\"Detection\"},{\"boundingBox\":[{\"x\":276,\"y\":262},{\"x\":410,\"y\":264},{\"x\":410,\"y\":296},{\"x\":276,\"y\":296}],\"text\":\"extreme\"},{\"boundingBox\":[{\"x\":661,\"y\":306},{\"x\":778,\"y\":303},{\"x\":778,\"y\":337},{\"x\":662,\"y\":337}],\"text\":\"extracti\"},{\"boundingBox\":[{\"x\":784,\"y\":303},{\"x\":819,\"y\":303},{\"x\":819,\"y\":337},{\"x\":784,\"y\":337}],\"text\":\"on\"},{\"boundingBox\":[{\"x\":826,\"y\":303},{\"x\":907,\"y\":302},{\"x\":907,\"y\":337},{\"x\":826,\"y\":337}],\"text\":\"local\"},{\"boundingBox\":[{\"x\":622,\"y\":350},{\"x\":842,\"y\":351},{\"x\":842,\"y\":385},{\"x\":623,\"y\":382}],\"text\":\"characteristic\"},{\"boundingBox\":[{\"x\":848,\"y\":351},{\"x\":953,\"y\":353},{\"x\":952,\"y\":384},{\"x\":848,\"y\":385}],\"text\":\"region\"},{\"boundingBox\":[{\"x\":1102,\"y\":337},{\"x\":1321,\"y\":337},{\"x\":1321,\"y\":370},{\"x\":1102,\"y\":371}],\"text\":\"characteristic\"},{\"boundingBox\":[{\"x\":1328,\"y\":337},{\"x\":1380,\"y\":337},{\"x\":1380,\"y\":370},{\"x\":1328,\"y\":370}],\"text\":\"set\"},{\"boundingBox\":[{\"x\":63,\"y\":432},{\"x\":207,\"y\":431},{\"x\":207,\"y\":471},{\"x\":64,\"y\":468}],\"text\":\"Location\"},{\"boundingBox\":[{\"x\":214,\"y\":431},{\"x\":263,\"y\":431},{\"x\":263,\"y\":471},{\"x\":214,\"y\":471}],\"text\":\"the\"},{\"boundingBox\":[{\"x\":270,\"y\":431},{\"x\":341,\"y\":432},{\"x\":341,\"y\":472},{\"x\":270,\"y\":471}],\"text\":\"Key\"},{\"boundingBox\":[{\"x\":348,\"y\":432},{\"x\":449,\"y\":433},{\"x\":449,\"y\":471},{\"x\":348,\"y\":472}],\"text\":\"points\"},{\"boundingBox\":[{\"x\":174,\"y\":480},{\"x\":207,\"y\":481},{\"x\":208,\"y\":519},{\"x\":176,\"y\":519}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":214,\"y\":481},{\"x\":347,\"y\":480},{\"x\":346,\"y\":521},{\"x\":216,\"y\":519}],\"text\":\"stability\"},{\"boundingBox\":[{\"x\":825,\"y\":545},{\"x\":1192,\"y\":544},{\"x\":1191,\"y\":583},{\"x\":826,\"y\":578}],\"text\":\"characteristicmatching\"},{\"boundingBox\":[{\"x\":132,\"y\":621},{\"x\":293,\"y\":620},{\"x\":293,\"y\":665},{\"x\":132,\"y\":667}],\"text\":\"Describing\"},{\"boundingBox\":[{\"x\":302,\"y\":620},{\"x\":396,\"y\":621},{\"x\":395,\"y\":666},{\"x\":302,\"y\":665}],\"text\":\"local\"},{\"boundingBox\":[{\"x\":96,\"y\":673},{\"x\":314,\"y\":674},{\"x\":314,\"y\":711},{\"x\":96,\"y\":709}],\"text\":\"characteristic\"},{\"boundingBox\":[{\"x\":321,\"y\":674},{\"x\":427,\"y\":676},{\"x\":427,\"y\":711},{\"x\":321,\"y\":711}],\"text\":\"region\"},{\"boundingBox\":[{\"x\":844,\"y\":672},{\"x\":984,\"y\":671},{\"x\":984,\"y\":711},{\"x\":846,\"y\":710}],\"text\":\"Dislodge\"},{\"boundingBox\":[{\"x\":991,\"y\":671},{\"x\":1050,\"y\":672},{\"x\":1050,\"y\":712},{\"x\":992,\"y\":711}],\"text\":\"the\"},{\"boundingBox\":[{\"x\":1057,\"y\":672},{\"x\":1170,\"y\":673},{\"x\":1169,\"y\":714},{\"x\":1057,\"y\":712}],\"text\":\"Wrong\"},{\"boundingBox\":[{\"x\":876,\"y\":723},{\"x\":976,\"y\":722},{\"x\":977,\"y\":755},{\"x\":877,\"y\":755}],\"text\":\"image\"},{\"boundingBox\":[{\"x\":983,\"y\":721},{\"x\":1137,\"y\":719},{\"x\":1137,\"y\":759},{\"x\":983,\"y\":755}],\"text\":\"matching\"},{\"boundingBox\":[{\"x\":125,\"y\":849},{\"x\":343,\"y\":848},{\"x\":344,\"y\":883},{\"x\":128,\"y\":885}],\"text\":\"characteristic\"},{\"boundingBox\":[{\"x\":350,\"y\":848},{\"x\":403,\"y\":849},{\"x\":403,\"y\":884},{\"x\":351,\"y\":883}],\"text\":\"set\"},{\"boundingBox\":[{\"x\":916,\"y\":850},{\"x\":968,\"y\":851},{\"x\":967,\"y\":886},{\"x\":915,\"y\":885}],\"text\":\"The\"},{\"boundingBox\":[{\"x\":975,\"y\":851},{\"x\":1067,\"y\":851},{\"x\":1066,\"y\":887},{\"x\":974,\"y\":886}],\"text\":\"input\"},{\"boundingBox\":[{\"x\":1074,\"y\":851},{\"x\":1113,\"y\":850},{\"x\":1112,\"y\":887},{\"x\":1073,\"y\":887}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":884,\"y\":902},{\"x\":1037,\"y\":902},{\"x\":1037,\"y\":938},{\"x\":885,\"y\":936}],\"text\":\"matching\"},{\"boundingBox\":[{\"x\":1044,\"y\":902},{\"x\":1149,\"y\":904},{\"x\":1148,\"y\":938},{\"x\":1044,\"y\":938}],\"text\":\"image\"}]}", "{\"language\":\"en\",\"text\":\"Octave5 80 Octave4 40 Octave3 20 Octave2 Octave1\",\"lines\":[{\"boundingBox\":[{\"x\":741,\"y\":12},{\"x\":894,\"y\":11},{\"x\":895,\"y\":43},{\"x\":742,\"y\":46}],\"text\":\"Octave5\"},{\"boundingBox\":[{\"x\":0,\"y\":99},{\"x\":55,\"y\":98},{\"x\":57,\"y\":129},{\"x\":0,\"y\":133}],\"text\":\"80\"},{\"boundingBox\":[{\"x\":741,\"y\":93},{\"x\":894,\"y\":93},{\"x\":895,\"y\":131},{\"x\":741,\"y\":132}],\"text\":\"Octave4\"},{\"boundingBox\":[{\"x\":0,\"y\":170},{\"x\":55,\"y\":167},{\"x\":58,\"y\":199},{\"x\":0,\"y\":205}],\"text\":\"40\"},{\"boundingBox\":[{\"x\":743,\"y\":173},{\"x\":894,\"y\":173},{\"x\":895,\"y\":205},{\"x\":743,\"y\":208}],\"text\":\"Octave3\"},{\"boundingBox\":[{\"x\":9,\"y\":326},{\"x\":57,\"y\":326},{\"x\":57,\"y\":364},{\"x\":9,\"y\":364}],\"text\":\"20\"},{\"boundingBox\":[{\"x\":745,\"y\":303},{\"x\":893,\"y\":303},{\"x\":893,\"y\":343},{\"x\":745,\"y\":343}],\"text\":\"Octave2\"},{\"boundingBox\":[{\"x\":746,\"y\":696},{\"x\":891,\"y\":696},{\"x\":891,\"y\":730},{\"x\":746,\"y\":731}],\"text\":\"Octave1\"}],\"words\":[{\"boundingBox\":[{\"x\":742,\"y\":12},{\"x\":894,\"y\":11},{\"x\":894,\"y\":40},{\"x\":742,\"y\":45}],\"text\":\"Octave5\"},{\"boundingBox\":[{\"x\":0,\"y\":99},{\"x\":53,\"y\":98},{\"x\":55,\"y\":130},{\"x\":0,\"y\":132}],\"text\":\"80\"},{\"boundingBox\":[{\"x\":742,\"y\":94},{\"x\":895,\"y\":93},{\"x\":895,\"y\":132},{\"x\":741,\"y\":132}],\"text\":\"Octave4\"},{\"boundingBox\":[{\"x\":0,\"y\":170},{\"x\":54,\"y\":167},{\"x\":56,\"y\":200},{\"x\":0,\"y\":204}],\"text\":\"40\"},{\"boundingBox\":[{\"x\":744,\"y\":174},{\"x\":895,\"y\":173},{\"x\":895,\"y\":203},{\"x\":743,\"y\":207}],\"text\":\"Octave3\"},{\"boundingBox\":[{\"x\":9,\"y\":326},{\"x\":55,\"y\":326},{\"x\":55,\"y\":364},{\"x\":9,\"y\":364}],\"text\":\"20\"},{\"boundingBox\":[{\"x\":746,\"y\":304},{\"x\":893,\"y\":303},{\"x\":893,\"y\":343},{\"x\":746,\"y\":343}],\"text\":\"Octave2\"},{\"boundingBox\":[{\"x\":747,\"y\":697},{\"x\":891,\"y\":696},{\"x\":888,\"y\":731},{\"x\":747,\"y\":732}],\"text\":\"Octave1\"}]}", "{\"language\":\"en\",\"text\":\"02 0.1 -0.1 02 -03 Laplacian - DOG -0.4\",\"lines\":[{\"boundingBox\":[{\"x\":16,\"y\":35},{\"x\":68,\"y\":35},{\"x\":68,\"y\":72},{\"x\":16,\"y\":72}],\"text\":\"02\"},{\"boundingBox\":[{\"x\":16,\"y\":191},{\"x\":64,\"y\":191},{\"x\":66,\"y\":224},{\"x\":16,\"y\":225}],\"text\":\"0.1\"},{\"boundingBox\":[{\"x\":10,\"y\":500},{\"x\":64,\"y\":501},{\"x\":63,\"y\":535},{\"x\":9,\"y\":533}],\"text\":\"-0.1\"},{\"boundingBox\":[{\"x\":13,\"y\":654},{\"x\":77,\"y\":655},{\"x\":77,\"y\":689},{\"x\":12,\"y\":689}],\"text\":\"02\"},{\"boundingBox\":[{\"x\":11,\"y\":808},{\"x\":66,\"y\":809},{\"x\":66,\"y\":845},{\"x\":11,\"y\":844}],\"text\":\"-03\"},{\"boundingBox\":[{\"x\":996,\"y\":873},{\"x\":1191,\"y\":871},{\"x\":1192,\"y\":922},{\"x\":996,\"y\":925}],\"text\":\"Laplacian\"},{\"boundingBox\":[{\"x\":936,\"y\":919},{\"x\":997,\"y\":920},{\"x\":993,\"y\":960},{\"x\":935,\"y\":957}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":1003,\"y\":924},{\"x\":1097,\"y\":924},{\"x\":1096,\"y\":960},{\"x\":1003,\"y\":960}],\"text\":\"DOG\"},{\"boundingBox\":[{\"x\":7,\"y\":963},{\"x\":66,\"y\":964},{\"x\":65,\"y\":999},{\"x\":7,\"y\":998}],\"text\":\"-0.4\"}],\"words\":[{\"boundingBox\":[{\"x\":16,\"y\":35},{\"x\":68,\"y\":35},{\"x\":68,\"y\":72},{\"x\":16,\"y\":72}],\"text\":\"02\"},{\"boundingBox\":[{\"x\":16,\"y\":191},{\"x\":66,\"y\":191},{\"x\":66,\"y\":224},{\"x\":16,\"y\":225}],\"text\":\"0.1\"},{\"boundingBox\":[{\"x\":9,\"y\":500},{\"x\":63,\"y\":501},{\"x\":62,\"y\":535},{\"x\":9,\"y\":533}],\"text\":\"-0.1\"},{\"boundingBox\":[{\"x\":14,\"y\":654},{\"x\":75,\"y\":654},{\"x\":75,\"y\":689},{\"x\":14,\"y\":688}],\"text\":\"02\"},{\"boundingBox\":[{\"x\":11,\"y\":808},{\"x\":64,\"y\":809},{\"x\":63,\"y\":845},{\"x\":11,\"y\":844}],\"text\":\"-03\"},{\"boundingBox\":[{\"x\":1000,\"y\":874},{\"x\":1191,\"y\":872},{\"x\":1191,\"y\":924},{\"x\":1000,\"y\":925}],\"text\":\"Laplacian\"},{\"boundingBox\":[{\"x\":936,\"y\":919},{\"x\":968,\"y\":919},{\"x\":967,\"y\":959},{\"x\":935,\"y\":958}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":1005,\"y\":924},{\"x\":1094,\"y\":924},{\"x\":1094,\"y\":960},{\"x\":1005,\"y\":960}],\"text\":\"DOG\"},{\"boundingBox\":[{\"x\":7,\"y\":963},{\"x\":65,\"y\":964},{\"x\":65,\"y\":999},{\"x\":7,\"y\":998}],\"text\":\"-0.4\"}]}", "{\"language\":\"en\",\"text\":\"Scale (next octave) Gaussian DOG\",\"lines\":[{\"boundingBox\":[{\"x\":0,\"y\":92},{\"x\":206,\"y\":91},{\"x\":207,\"y\":126},{\"x\":0,\"y\":127}],\"text\":\"Scale (next\"},{\"boundingBox\":[{\"x\":37,\"y\":147},{\"x\":164,\"y\":146},{\"x\":165,\"y\":181},{\"x\":37,\"y\":182}],\"text\":\"octave)\"},{\"boundingBox\":[{\"x\":251,\"y\":810},{\"x\":402,\"y\":810},{\"x\":402,\"y\":841},{\"x\":251,\"y\":841}],\"text\":\"Gaussian\"},{\"boundingBox\":[{\"x\":955,\"y\":805},{\"x\":1033,\"y\":805},{\"x\":1032,\"y\":835},{\"x\":954,\"y\":836}],\"text\":\"DOG\"}],\"words\":[{\"boundingBox\":[{\"x\":0,\"y\":95},{\"x\":96,\"y\":92},{\"x\":96,\"y\":127},{\"x\":0,\"y\":127}],\"text\":\"Scale\"},{\"boundingBox\":[{\"x\":106,\"y\":92},{\"x\":206,\"y\":91},{\"x\":206,\"y\":127},{\"x\":106,\"y\":127}],\"text\":\"(next\"},{\"boundingBox\":[{\"x\":37,\"y\":150},{\"x\":165,\"y\":146},{\"x\":164,\"y\":182},{\"x\":40,\"y\":181}],\"text\":\"octave)\"},{\"boundingBox\":[{\"x\":252,\"y\":811},{\"x\":402,\"y\":811},{\"x\":402,\"y\":842},{\"x\":251,\"y\":842}],\"text\":\"Gaussian\"},{\"boundingBox\":[{\"x\":954,\"y\":805},{\"x\":1032,\"y\":805},{\"x\":1033,\"y\":835},{\"x\":954,\"y\":836}],\"text\":\"DOG\"}]}", "{\"language\":\"en\",\"text\":\"Scale\",\"lines\":[{\"boundingBox\":[{\"x\":689,\"y\":411},{\"x\":894,\"y\":411},{\"x\":893,\"y\":472},{\"x\":688,\"y\":470}],\"text\":\"Scale\"}],\"words\":[{\"boundingBox\":[{\"x\":693,\"y\":413},{\"x\":894,\"y\":412},{\"x\":894,\"y\":473},{\"x\":689,\"y\":469}],\"text\":\"Scale\"}]}", "{\"language\":\"en\",\"text\":\"The main direction\",\"lines\":[{\"boundingBox\":[{\"x\":929,\"y\":2},{\"x\":1368,\"y\":3},{\"x\":1367,\"y\":39},{\"x\":929,\"y\":38}],\"text\":\"The main direction\"}],\"words\":[{\"boundingBox\":[{\"x\":930,\"y\":3},{\"x\":1010,\"y\":3},{\"x\":1010,\"y\":39},{\"x\":930,\"y\":38}],\"text\":\"The\"},{\"boundingBox\":[{\"x\":1022,\"y\":3},{\"x\":1129,\"y\":3},{\"x\":1130,\"y\":40},{\"x\":1022,\"y\":39}],\"text\":\"main\"},{\"boundingBox\":[{\"x\":1146,\"y\":3},{\"x\":1368,\"y\":4},{\"x\":1368,\"y\":39},{\"x\":1146,\"y\":40}],\"text\":\"direction\"}]}", "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}", "{\"language\":\"en\",\"text\":\"50 100 150 200 100 200 300 400 500 600\",\"lines\":[{\"boundingBox\":[{\"x\":17,\"y\":87},{\"x\":52,\"y\":87},{\"x\":52,\"y\":111},{\"x\":18,\"y\":112}],\"text\":\"50\"},{\"boundingBox\":[{\"x\":3,\"y\":177},{\"x\":52,\"y\":176},{\"x\":52,\"y\":202},{\"x\":4,\"y\":202}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":5,\"y\":267},{\"x\":50,\"y\":267},{\"x\":50,\"y\":294},{\"x\":5,\"y\":294}],\"text\":\"150\"},{\"boundingBox\":[{\"x\":2,\"y\":360},{\"x\":52,\"y\":360},{\"x\":52,\"y\":386},{\"x\":3,\"y\":386}],\"text\":\"200\"},{\"boundingBox\":[{\"x\":210,\"y\":465},{\"x\":261,\"y\":465},{\"x\":262,\"y\":490},{\"x\":211,\"y\":490}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":383,\"y\":465},{\"x\":434,\"y\":465},{\"x\":434,\"y\":490},{\"x\":385,\"y\":491}],\"text\":\"200\"},{\"boundingBox\":[{\"x\":556,\"y\":464},{\"x\":609,\"y\":464},{\"x\":610,\"y\":490},{\"x\":556,\"y\":491}],\"text\":\"300\"},{\"boundingBox\":[{\"x\":732,\"y\":464},{\"x\":785,\"y\":464},{\"x\":784,\"y\":491},{\"x\":733,\"y\":490}],\"text\":\"400\"},{\"boundingBox\":[{\"x\":906,\"y\":466},{\"x\":957,\"y\":465},{\"x\":957,\"y\":490},{\"x\":906,\"y\":490}],\"text\":\"500\"},{\"boundingBox\":[{\"x\":1080,\"y\":466},{\"x\":1130,\"y\":465},{\"x\":1131,\"y\":489},{\"x\":1080,\"y\":490}],\"text\":\"600\"}],\"words\":[{\"boundingBox\":[{\"x\":17,\"y\":87},{\"x\":52,\"y\":87},{\"x\":52,\"y\":111},{\"x\":17,\"y\":112}],\"text\":\"50\"},{\"boundingBox\":[{\"x\":3,\"y\":176},{\"x\":51,\"y\":176},{\"x\":52,\"y\":202},{\"x\":3,\"y\":202}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":5,\"y\":267},{\"x\":50,\"y\":267},{\"x\":50,\"y\":294},{\"x\":5,\"y\":294}],\"text\":\"150\"},{\"boundingBox\":[{\"x\":2,\"y\":360},{\"x\":51,\"y\":360},{\"x\":51,\"y\":386},{\"x\":2,\"y\":386}],\"text\":\"200\"},{\"boundingBox\":[{\"x\":212,\"y\":465},{\"x\":262,\"y\":465},{\"x\":262,\"y\":490},{\"x\":212,\"y\":490}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":384,\"y\":465},{\"x\":433,\"y\":465},{\"x\":433,\"y\":490},{\"x\":385,\"y\":491}],\"text\":\"200\"},{\"boundingBox\":[{\"x\":556,\"y\":464},{\"x\":610,\"y\":464},{\"x\":610,\"y\":490},{\"x\":556,\"y\":491}],\"text\":\"300\"},{\"boundingBox\":[{\"x\":732,\"y\":464},{\"x\":784,\"y\":464},{\"x\":784,\"y\":491},{\"x\":732,\"y\":490}],\"text\":\"400\"},{\"boundingBox\":[{\"x\":906,\"y\":465},{\"x\":956,\"y\":465},{\"x\":956,\"y\":490},{\"x\":906,\"y\":490}],\"text\":\"500\"},{\"boundingBox\":[{\"x\":1080,\"y\":465},{\"x\":1129,\"y\":465},{\"x\":1130,\"y\":489},{\"x\":1080,\"y\":490}],\"text\":\"600\"}]}", "{\"language\":\"en\",\"text\":\"50 100 150 200 100 200 300 400 500 600\",\"lines\":[{\"boundingBox\":[{\"x\":17,\"y\":86},{\"x\":50,\"y\":85},{\"x\":51,\"y\":112},{\"x\":17,\"y\":112}],\"text\":\"50\"},{\"boundingBox\":[{\"x\":4,\"y\":176},{\"x\":50,\"y\":176},{\"x\":50,\"y\":202},{\"x\":4,\"y\":203}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":0,\"y\":270},{\"x\":50,\"y\":267},{\"x\":50,\"y\":292},{\"x\":0,\"y\":291}],\"text\":\"150\"},{\"boundingBox\":[{\"x\":0,\"y\":362},{\"x\":51,\"y\":360},{\"x\":51,\"y\":386},{\"x\":0,\"y\":385}],\"text\":\"200\"},{\"boundingBox\":[{\"x\":209,\"y\":466},{\"x\":261,\"y\":465},{\"x\":261,\"y\":489},{\"x\":210,\"y\":490}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":381,\"y\":466},{\"x\":434,\"y\":465},{\"x\":434,\"y\":490},{\"x\":382,\"y\":491}],\"text\":\"200\"},{\"boundingBox\":[{\"x\":555,\"y\":465},{\"x\":608,\"y\":464},{\"x\":608,\"y\":491},{\"x\":555,\"y\":491}],\"text\":\"300\"},{\"boundingBox\":[{\"x\":733,\"y\":466},{\"x\":783,\"y\":466},{\"x\":782,\"y\":490},{\"x\":734,\"y\":489}],\"text\":\"400\"},{\"boundingBox\":[{\"x\":905,\"y\":467},{\"x\":957,\"y\":465},{\"x\":957,\"y\":489},{\"x\":906,\"y\":490}],\"text\":\"500\"},{\"boundingBox\":[{\"x\":1080,\"y\":466},{\"x\":1132,\"y\":466},{\"x\":1132,\"y\":489},{\"x\":1081,\"y\":490}],\"text\":\"600\"}],\"words\":[{\"boundingBox\":[{\"x\":17,\"y\":85},{\"x\":49,\"y\":85},{\"x\":49,\"y\":112},{\"x\":17,\"y\":112}],\"text\":\"50\"},{\"boundingBox\":[{\"x\":4,\"y\":176},{\"x\":49,\"y\":176},{\"x\":49,\"y\":203},{\"x\":4,\"y\":203}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":0,\"y\":268},{\"x\":49,\"y\":267},{\"x\":50,\"y\":292},{\"x\":0,\"y\":293}],\"text\":\"150\"},{\"boundingBox\":[{\"x\":0,\"y\":360},{\"x\":50,\"y\":360},{\"x\":50,\"y\":386},{\"x\":0,\"y\":386}],\"text\":\"200\"},{\"boundingBox\":[{\"x\":209,\"y\":466},{\"x\":260,\"y\":465},{\"x\":260,\"y\":489},{\"x\":209,\"y\":490}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":382,\"y\":466},{\"x\":432,\"y\":465},{\"x\":433,\"y\":490},{\"x\":383,\"y\":491}],\"text\":\"200\"},{\"boundingBox\":[{\"x\":555,\"y\":464},{\"x\":607,\"y\":464},{\"x\":607,\"y\":491},{\"x\":555,\"y\":491}],\"text\":\"300\"},{\"boundingBox\":[{\"x\":733,\"y\":466},{\"x\":783,\"y\":466},{\"x\":782,\"y\":490},{\"x\":733,\"y\":489}],\"text\":\"400\"},{\"boundingBox\":[{\"x\":905,\"y\":466},{\"x\":955,\"y\":465},{\"x\":956,\"y\":489},{\"x\":905,\"y\":490}],\"text\":\"500\"},{\"boundingBox\":[{\"x\":1080,\"y\":466},{\"x\":1131,\"y\":466},{\"x\":1131,\"y\":489},{\"x\":1080,\"y\":490}],\"text\":\"600\"}]}", "{\"language\":\"en\",\"text\":\"50 100 150 200 100 200 300 400 500 600\",\"lines\":[{\"boundingBox\":[{\"x\":19,\"y\":81},{\"x\":56,\"y\":80},{\"x\":55,\"y\":107},{\"x\":19,\"y\":107}],\"text\":\"50\"},{\"boundingBox\":[{\"x\":2,\"y\":164},{\"x\":54,\"y\":163},{\"x\":54,\"y\":190},{\"x\":2,\"y\":191}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":4,\"y\":249},{\"x\":54,\"y\":249},{\"x\":53,\"y\":275},{\"x\":5,\"y\":273}],\"text\":\"150\"},{\"boundingBox\":[{\"x\":1,\"y\":334},{\"x\":53,\"y\":333},{\"x\":54,\"y\":361},{\"x\":1,\"y\":361}],\"text\":\"200\"},{\"boundingBox\":[{\"x\":212,\"y\":434},{\"x\":263,\"y\":434},{\"x\":264,\"y\":457},{\"x\":212,\"y\":457}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":385,\"y\":432},{\"x\":436,\"y\":431},{\"x\":437,\"y\":459},{\"x\":386,\"y\":459}],\"text\":\"200\"},{\"boundingBox\":[{\"x\":559,\"y\":434},{\"x\":606,\"y\":434},{\"x\":607,\"y\":457},{\"x\":558,\"y\":458}],\"text\":\"300\"},{\"boundingBox\":[{\"x\":733,\"y\":434},{\"x\":783,\"y\":434},{\"x\":783,\"y\":457},{\"x\":733,\"y\":457}],\"text\":\"400\"},{\"boundingBox\":[{\"x\":906,\"y\":433},{\"x\":956,\"y\":434},{\"x\":957,\"y\":457},{\"x\":905,\"y\":457}],\"text\":\"500\"},{\"boundingBox\":[{\"x\":1080,\"y\":434},{\"x\":1130,\"y\":434},{\"x\":1131,\"y\":457},{\"x\":1080,\"y\":458}],\"text\":\"600\"}],\"words\":[{\"boundingBox\":[{\"x\":19,\"y\":80},{\"x\":55,\"y\":80},{\"x\":55,\"y\":107},{\"x\":19,\"y\":107}],\"text\":\"50\"},{\"boundingBox\":[{\"x\":2,\"y\":164},{\"x\":52,\"y\":163},{\"x\":52,\"y\":190},{\"x\":2,\"y\":191}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":4,\"y\":249},{\"x\":53,\"y\":249},{\"x\":52,\"y\":275},{\"x\":4,\"y\":274}],\"text\":\"150\"},{\"boundingBox\":[{\"x\":3,\"y\":333},{\"x\":53,\"y\":333},{\"x\":53,\"y\":360},{\"x\":3,\"y\":361}],\"text\":\"200\"},{\"boundingBox\":[{\"x\":212,\"y\":434},{\"x\":263,\"y\":434},{\"x\":263,\"y\":457},{\"x\":212,\"y\":457}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":385,\"y\":431},{\"x\":435,\"y\":431},{\"x\":435,\"y\":459},{\"x\":385,\"y\":459}],\"text\":\"200\"},{\"boundingBox\":[{\"x\":558,\"y\":434},{\"x\":606,\"y\":434},{\"x\":606,\"y\":458},{\"x\":558,\"y\":458}],\"text\":\"300\"},{\"boundingBox\":[{\"x\":733,\"y\":434},{\"x\":782,\"y\":434},{\"x\":782,\"y\":457},{\"x\":733,\"y\":457}],\"text\":\"400\"},{\"boundingBox\":[{\"x\":905,\"y\":433},{\"x\":956,\"y\":433},{\"x\":956,\"y\":457},{\"x\":905,\"y\":456}],\"text\":\"500\"},{\"boundingBox\":[{\"x\":1080,\"y\":434},{\"x\":1129,\"y\":434},{\"x\":1130,\"y\":458},{\"x\":1080,\"y\":458}],\"text\":\"600\"}]}", "{\"language\":\"en\",\"text\":\"50 100 150 200\",\"lines\":[{\"boundingBox\":[{\"x\":17,\"y\":86},{\"x\":52,\"y\":85},{\"x\":52,\"y\":112},{\"x\":17,\"y\":113}],\"text\":\"50\"},{\"boundingBox\":[{\"x\":2,\"y\":176},{\"x\":52,\"y\":176},{\"x\":52,\"y\":202},{\"x\":2,\"y\":202}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":1,\"y\":267},{\"x\":52,\"y\":267},{\"x\":52,\"y\":293},{\"x\":3,\"y\":293}],\"text\":\"150\"},{\"boundingBox\":[{\"x\":1,\"y\":360},{\"x\":52,\"y\":360},{\"x\":52,\"y\":387},{\"x\":2,\"y\":387}],\"text\":\"200\"}],\"words\":[{\"boundingBox\":[{\"x\":17,\"y\":86},{\"x\":50,\"y\":85},{\"x\":51,\"y\":112},{\"x\":17,\"y\":113}],\"text\":\"50\"},{\"boundingBox\":[{\"x\":2,\"y\":176},{\"x\":51,\"y\":176},{\"x\":51,\"y\":202},{\"x\":2,\"y\":202}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":1,\"y\":267},{\"x\":51,\"y\":267},{\"x\":51,\"y\":293},{\"x\":1,\"y\":293}],\"text\":\"150\"},{\"boundingBox\":[{\"x\":1,\"y\":360},{\"x\":51,\"y\":360},{\"x\":51,\"y\":387},{\"x\":1,\"y\":387}],\"text\":\"200\"}]}", "{\"language\":\"en\",\"text\":\"Start Input the training face sample Image normalization. Calculate the average face Extract local features SDM Iterative Solution Generate face detection model End\",\"lines\":[{\"boundingBox\":[{\"x\":393,\"y\":27},{\"x\":499,\"y\":29},{\"x\":498,\"y\":69},{\"x\":392,\"y\":68}],\"text\":\"Start\"},{\"boundingBox\":[{\"x\":119,\"y\":211},{\"x\":764,\"y\":211},{\"x\":764,\"y\":262},{\"x\":119,\"y\":262}],\"text\":\"Input the training face sample\"},{\"boundingBox\":[{\"x\":211,\"y\":440},{\"x\":668,\"y\":438},{\"x\":668,\"y\":486},{\"x\":211,\"y\":488}],\"text\":\"Image normalization.\"},{\"boundingBox\":[{\"x\":159,\"y\":502},{\"x\":729,\"y\":505},{\"x\":729,\"y\":553},{\"x\":159,\"y\":551}],\"text\":\"Calculate the average face\"},{\"boundingBox\":[{\"x\":213,\"y\":715},{\"x\":674,\"y\":716},{\"x\":674,\"y\":763},{\"x\":213,\"y\":762}],\"text\":\"Extract local features\"},{\"boundingBox\":[{\"x\":189,\"y\":936},{\"x\":699,\"y\":936},{\"x\":699,\"y\":984},{\"x\":189,\"y\":982}],\"text\":\"SDM Iterative Solution\"},{\"boundingBox\":[{\"x\":100,\"y\":1141},{\"x\":782,\"y\":1141},{\"x\":782,\"y\":1187},{\"x\":100,\"y\":1188}],\"text\":\"Generate face detection model\"},{\"boundingBox\":[{\"x\":398,\"y\":1332},{\"x\":489,\"y\":1333},{\"x\":488,\"y\":1375},{\"x\":398,\"y\":1376}],\"text\":\"End\"}],\"words\":[{\"boundingBox\":[{\"x\":392,\"y\":27},{\"x\":499,\"y\":28},{\"x\":498,\"y\":69},{\"x\":392,\"y\":67}],\"text\":\"Start\"},{\"boundingBox\":[{\"x\":121,\"y\":213},{\"x\":233,\"y\":212},{\"x\":232,\"y\":262},{\"x\":120,\"y\":262}],\"text\":\"Input\"},{\"boundingBox\":[{\"x\":242,\"y\":212},{\"x\":310,\"y\":212},{\"x\":309,\"y\":263},{\"x\":242,\"y\":262}],\"text\":\"the\"},{\"boundingBox\":[{\"x\":319,\"y\":212},{\"x\":493,\"y\":212},{\"x\":493,\"y\":263},{\"x\":319,\"y\":263}],\"text\":\"training\"},{\"boundingBox\":[{\"x\":502,\"y\":212},{\"x\":592,\"y\":212},{\"x\":592,\"y\":263},{\"x\":502,\"y\":263}],\"text\":\"face\"},{\"boundingBox\":[{\"x\":602,\"y\":212},{\"x\":762,\"y\":212},{\"x\":763,\"y\":263},{\"x\":602,\"y\":263}],\"text\":\"sample\"},{\"boundingBox\":[{\"x\":212,\"y\":441},{\"x\":347,\"y\":441},{\"x\":346,\"y\":488},{\"x\":212,\"y\":488}],\"text\":\"Image\"},{\"boundingBox\":[{\"x\":356,\"y\":441},{\"x\":667,\"y\":439},{\"x\":667,\"y\":486},{\"x\":355,\"y\":488}],\"text\":\"normalization.\"},{\"boundingBox\":[{\"x\":160,\"y\":503},{\"x\":361,\"y\":504},{\"x\":361,\"y\":553},{\"x\":159,\"y\":549}],\"text\":\"Calculate\"},{\"boundingBox\":[{\"x\":371,\"y\":504},{\"x\":441,\"y\":504},{\"x\":441,\"y\":553},{\"x\":370,\"y\":553}],\"text\":\"the\"},{\"boundingBox\":[{\"x\":450,\"y\":504},{\"x\":621,\"y\":505},{\"x\":621,\"y\":553},{\"x\":450,\"y\":553}],\"text\":\"average\"},{\"boundingBox\":[{\"x\":630,\"y\":505},{\"x\":728,\"y\":505},{\"x\":728,\"y\":551},{\"x\":630,\"y\":553}],\"text\":\"face\"},{\"boundingBox\":[{\"x\":215,\"y\":719},{\"x\":371,\"y\":716},{\"x\":371,\"y\":762},{\"x\":214,\"y\":762}],\"text\":\"Extract\"},{\"boundingBox\":[{\"x\":380,\"y\":716},{\"x\":492,\"y\":716},{\"x\":491,\"y\":763},{\"x\":379,\"y\":762}],\"text\":\"local\"},{\"boundingBox\":[{\"x\":500,\"y\":716},{\"x\":674,\"y\":720},{\"x\":673,\"y\":764},{\"x\":499,\"y\":763}],\"text\":\"features\"},{\"boundingBox\":[{\"x\":189,\"y\":937},{\"x\":299,\"y\":936},{\"x\":299,\"y\":983},{\"x\":189,\"y\":982}],\"text\":\"SDM\"},{\"boundingBox\":[{\"x\":317,\"y\":936},{\"x\":500,\"y\":936},{\"x\":499,\"y\":984},{\"x\":316,\"y\":983}],\"text\":\"Iterative\"},{\"boundingBox\":[{\"x\":509,\"y\":936},{\"x\":698,\"y\":937},{\"x\":697,\"y\":985},{\"x\":508,\"y\":984}],\"text\":\"Solution\"},{\"boundingBox\":[{\"x\":101,\"y\":1144},{\"x\":304,\"y\":1142},{\"x\":303,\"y\":1189},{\"x\":101,\"y\":1188}],\"text\":\"Generate\"},{\"boundingBox\":[{\"x\":312,\"y\":1142},{\"x\":408,\"y\":1141},{\"x\":407,\"y\":1189},{\"x\":312,\"y\":1189}],\"text\":\"face\"},{\"boundingBox\":[{\"x\":416,\"y\":1141},{\"x\":627,\"y\":1141},{\"x\":627,\"y\":1189},{\"x\":416,\"y\":1189}],\"text\":\"detection\"},{\"boundingBox\":[{\"x\":636,\"y\":1141},{\"x\":781,\"y\":1142},{\"x\":780,\"y\":1188},{\"x\":636,\"y\":1189}],\"text\":\"model\"},{\"boundingBox\":[{\"x\":398,\"y\":1332},{\"x\":489,\"y\":1332},{\"x\":489,\"y\":1376},{\"x\":398,\"y\":1376}],\"text\":\"End\"}]}", "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}", "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}", "{\"language\":\"en\",\"text\":\"w P x World coordinate systems U C O Z plane-coordinate system R, T X Y Camera coordinate systems\",\"lines\":[{\"boundingBox\":[{\"x\":1056,\"y\":14},{\"x\":1087,\"y\":14},{\"x\":1086,\"y\":43},{\"x\":1055,\"y\":43}],\"text\":\"w\"},{\"boundingBox\":[{\"x\":910,\"y\":53},{\"x\":930,\"y\":49},{\"x\":933,\"y\":68},{\"x\":913,\"y\":72}],\"text\":\"P\"},{\"boundingBox\":[{\"x\":550,\"y\":79},{\"x\":569,\"y\":78},{\"x\":570,\"y\":97},{\"x\":551,\"y\":99}],\"text\":\"x\"},{\"boundingBox\":[{\"x\":1085,\"y\":191},{\"x\":1419,\"y\":193},{\"x\":1418,\"y\":222},{\"x\":1085,\"y\":220}],\"text\":\"World coordinate systems\"},{\"boundingBox\":[{\"x\":974,\"y\":243},{\"x\":994,\"y\":248},{\"x\":989,\"y\":272},{\"x\":969,\"y\":267}],\"text\":\"U\"},{\"boundingBox\":[{\"x\":621,\"y\":302},{\"x\":641,\"y\":303},{\"x\":641,\"y\":322},{\"x\":621,\"y\":321}],\"text\":\"C\"},{\"boundingBox\":[{\"x\":141,\"y\":405},{\"x\":138,\"y\":383},{\"x\":153,\"y\":379},{\"x\":156,\"y\":402}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":340,\"y\":375},{\"x\":360,\"y\":374},{\"x\":360,\"y\":396},{\"x\":340,\"y\":397}],\"text\":\"Z\"},{\"boundingBox\":[{\"x\":454,\"y\":425},{\"x\":768,\"y\":425},{\"x\":768,\"y\":455},{\"x\":454,\"y\":456}],\"text\":\"plane-coordinate system\"},{\"boundingBox\":[{\"x\":876,\"y\":438},{\"x\":924,\"y\":436},{\"x\":926,\"y\":468},{\"x\":877,\"y\":470}],\"text\":\"R, T\"},{\"boundingBox\":[{\"x\":286,\"y\":511},{\"x\":289,\"y\":477},{\"x\":309,\"y\":478},{\"x\":306,\"y\":512}],\"text\":\"X\"},{\"boundingBox\":[{\"x\":185,\"y\":508},{\"x\":209,\"y\":509},{\"x\":208,\"y\":534},{\"x\":184,\"y\":533}],\"text\":\"Y\"},{\"boundingBox\":[{\"x\":0,\"y\":551},{\"x\":354,\"y\":553},{\"x\":353,\"y\":581},{\"x\":0,\"y\":580}],\"text\":\"Camera coordinate systems\"}],\"words\":[{\"boundingBox\":[{\"x\":1055,\"y\":14},{\"x\":1082,\"y\":14},{\"x\":1082,\"y\":43},{\"x\":1055,\"y\":43}],\"text\":\"w\"},{\"boundingBox\":[{\"x\":910,\"y\":53},{\"x\":929,\"y\":49},{\"x\":933,\"y\":68},{\"x\":914,\"y\":72}],\"text\":\"P\"},{\"boundingBox\":[{\"x\":552,\"y\":79},{\"x\":568,\"y\":78},{\"x\":569,\"y\":97},{\"x\":554,\"y\":99}],\"text\":\"x\"},{\"boundingBox\":[{\"x\":1086,\"y\":191},{\"x\":1165,\"y\":192},{\"x\":1164,\"y\":221},{\"x\":1086,\"y\":219}],\"text\":\"World\"},{\"boundingBox\":[{\"x\":1170,\"y\":192},{\"x\":1306,\"y\":193},{\"x\":1306,\"y\":222},{\"x\":1169,\"y\":221}],\"text\":\"coordinate\"},{\"boundingBox\":[{\"x\":1312,\"y\":193},{\"x\":1419,\"y\":195},{\"x\":1419,\"y\":220},{\"x\":1312,\"y\":222}],\"text\":\"systems\"},{\"boundingBox\":[{\"x\":973,\"y\":243},{\"x\":993,\"y\":248},{\"x\":987,\"y\":271},{\"x\":969,\"y\":266}],\"text\":\"U\"},{\"boundingBox\":[{\"x\":624,\"y\":302},{\"x\":640,\"y\":303},{\"x\":639,\"y\":322},{\"x\":623,\"y\":321}],\"text\":\"C\"},{\"boundingBox\":[{\"x\":141,\"y\":405},{\"x\":139,\"y\":391},{\"x\":154,\"y\":389},{\"x\":156,\"y\":403}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":342,\"y\":375},{\"x\":360,\"y\":374},{\"x\":361,\"y\":395},{\"x\":343,\"y\":396}],\"text\":\"Z\"},{\"boundingBox\":[{\"x\":455,\"y\":427},{\"x\":668,\"y\":426},{\"x\":668,\"y\":454},{\"x\":455,\"y\":457}],\"text\":\"plane-coordinate\"},{\"boundingBox\":[{\"x\":674,\"y\":426},{\"x\":768,\"y\":428},{\"x\":768,\"y\":454},{\"x\":674,\"y\":454}],\"text\":\"system\"},{\"boundingBox\":[{\"x\":876,\"y\":437},{\"x\":902,\"y\":436},{\"x\":903,\"y\":469},{\"x\":877,\"y\":470}],\"text\":\"R,\"},{\"boundingBox\":[{\"x\":909,\"y\":436},{\"x\":924,\"y\":436},{\"x\":925,\"y\":468},{\"x\":910,\"y\":469}],\"text\":\"T\"},{\"boundingBox\":[{\"x\":286,\"y\":508},{\"x\":288,\"y\":488},{\"x\":308,\"y\":490},{\"x\":306,\"y\":510}],\"text\":\"X\"},{\"boundingBox\":[{\"x\":192,\"y\":508},{\"x\":208,\"y\":509},{\"x\":207,\"y\":534},{\"x\":191,\"y\":533}],\"text\":\"Y\"},{\"boundingBox\":[{\"x\":1,\"y\":552},{\"x\":97,\"y\":552},{\"x\":97,\"y\":581},{\"x\":2,\"y\":581}],\"text\":\"Camera\"},{\"boundingBox\":[{\"x\":103,\"y\":552},{\"x\":240,\"y\":553},{\"x\":240,\"y\":581},{\"x\":103,\"y\":581}],\"text\":\"coordinate\"},{\"boundingBox\":[{\"x\":246,\"y\":553},{\"x\":353,\"y\":556},{\"x\":353,\"y\":581},{\"x\":246,\"y\":581}],\"text\":\"systems\"}]}", "{\"language\":\"en\",\"text\":\"C\",\"lines\":[{\"boundingBox\":[{\"x\":236,\"y\":159},{\"x\":274,\"y\":160},{\"x\":273,\"y\":204},{\"x\":236,\"y\":204}],\"text\":\"C\"}],\"words\":[{\"boundingBox\":[{\"x\":236,\"y\":159},{\"x\":272,\"y\":159},{\"x\":271,\"y\":204},{\"x\":236,\"y\":203}],\"text\":\"C\"}]}", "{\"language\":\"en\",\"text\":\"B\",\"lines\":[{\"boundingBox\":[{\"x\":813,\"y\":392},{\"x\":843,\"y\":390},{\"x\":844,\"y\":425},{\"x\":814,\"y\":426}],\"text\":\"B\"}],\"words\":[{\"boundingBox\":[{\"x\":813,\"y\":391},{\"x\":840,\"y\":390},{\"x\":842,\"y\":425},{\"x\":814,\"y\":426}],\"text\":\"B\"}]}", "{\"language\":\"en\",\"text\":\"Real world coordinates Virtual world coordinates actual environment Glasses model Scene acquisition Tracking registry Model generation system system Virtual reality synthesis system Coordinates of real Coordinates of the world cameras virtual world camera display system User UI\",\"lines\":[{\"boundingBox\":[{\"x\":18,\"y\":24},{\"x\":333,\"y\":24},{\"x\":333,\"y\":53},{\"x\":18,\"y\":53}],\"text\":\"Real world coordinates\"},{\"boundingBox\":[{\"x\":758,\"y\":23},{\"x\":1106,\"y\":24},{\"x\":1106,\"y\":53},{\"x\":758,\"y\":52}],\"text\":\"Virtual world coordinates\"},{\"boundingBox\":[{\"x\":42,\"y\":144},{\"x\":307,\"y\":144},{\"x\":307,\"y\":173},{\"x\":42,\"y\":173}],\"text\":\"actual environment\"},{\"boundingBox\":[{\"x\":833,\"y\":143},{\"x\":1033,\"y\":143},{\"x\":1033,\"y\":172},{\"x\":833,\"y\":172}],\"text\":\"Glasses model\"},{\"boundingBox\":[{\"x\":53,\"y\":244},{\"x\":295,\"y\":246},{\"x\":295,\"y\":279},{\"x\":52,\"y\":276}],\"text\":\"Scene acquisition\"},{\"boundingBox\":[{\"x\":411,\"y\":265},{\"x\":648,\"y\":267},{\"x\":647,\"y\":299},{\"x\":411,\"y\":296}],\"text\":\"Tracking registry\"},{\"boundingBox\":[{\"x\":762,\"y\":265},{\"x\":1105,\"y\":267},{\"x\":1105,\"y\":297},{\"x\":762,\"y\":295}],\"text\":\"Model generation system\"},{\"boundingBox\":[{\"x\":122,\"y\":291},{\"x\":223,\"y\":291},{\"x\":223,\"y\":313},{\"x\":122,\"y\":313}],\"text\":\"system\"},{\"boundingBox\":[{\"x\":339,\"y\":374},{\"x\":766,\"y\":375},{\"x\":766,\"y\":407},{\"x\":339,\"y\":405}],\"text\":\"Virtual reality synthesis system\"},{\"boundingBox\":[{\"x\":28,\"y\":408},{\"x\":291,\"y\":408},{\"x\":291,\"y\":440},{\"x\":28,\"y\":441}],\"text\":\"Coordinates of real\"},{\"boundingBox\":[{\"x\":839,\"y\":406},{\"x\":1095,\"y\":405},{\"x\":1095,\"y\":437},{\"x\":839,\"y\":437}],\"text\":\"Coordinates of the\"},{\"boundingBox\":[{\"x\":62,\"y\":452},{\"x\":261,\"y\":452},{\"x\":261,\"y\":480},{\"x\":62,\"y\":479}],\"text\":\"world cameras\"},{\"boundingBox\":[{\"x\":826,\"y\":448},{\"x\":1109,\"y\":448},{\"x\":1109,\"y\":477},{\"x\":826,\"y\":476}],\"text\":\"virtual world camera\"},{\"boundingBox\":[{\"x\":455,\"y\":492},{\"x\":659,\"y\":493},{\"x\":659,\"y\":525},{\"x\":455,\"y\":524}],\"text\":\"display system\"},{\"boundingBox\":[{\"x\":521,\"y\":613},{\"x\":587,\"y\":615},{\"x\":587,\"y\":641},{\"x\":520,\"y\":640}],\"text\":\"User\"},{\"boundingBox\":[{\"x\":840,\"y\":612},{\"x\":878,\"y\":612},{\"x\":877,\"y\":639},{\"x\":840,\"y\":638}],\"text\":\"UI\"}],\"words\":[{\"boundingBox\":[{\"x\":19,\"y\":25},{\"x\":80,\"y\":24},{\"x\":79,\"y\":54},{\"x\":19,\"y\":53}],\"text\":\"Real\"},{\"boundingBox\":[{\"x\":85,\"y\":24},{\"x\":167,\"y\":24},{\"x\":167,\"y\":54},{\"x\":85,\"y\":54}],\"text\":\"world\"},{\"boundingBox\":[{\"x\":172,\"y\":24},{\"x\":333,\"y\":25},{\"x\":333,\"y\":54},{\"x\":172,\"y\":54}],\"text\":\"coordinates\"},{\"boundingBox\":[{\"x\":761,\"y\":24},{\"x\":854,\"y\":24},{\"x\":853,\"y\":53},{\"x\":760,\"y\":53}],\"text\":\"Virtual\"},{\"boundingBox\":[{\"x\":859,\"y\":24},{\"x\":941,\"y\":24},{\"x\":941,\"y\":54},{\"x\":859,\"y\":53}],\"text\":\"world\"},{\"boundingBox\":[{\"x\":946,\"y\":24},{\"x\":1105,\"y\":25},{\"x\":1106,\"y\":54},{\"x\":946,\"y\":54}],\"text\":\"coordinates\"},{\"boundingBox\":[{\"x\":43,\"y\":147},{\"x\":126,\"y\":145},{\"x\":126,\"y\":173},{\"x\":43,\"y\":173}],\"text\":\"actual\"},{\"boundingBox\":[{\"x\":131,\"y\":145},{\"x\":307,\"y\":147},{\"x\":307,\"y\":174},{\"x\":131,\"y\":173}],\"text\":\"environment\"},{\"boundingBox\":[{\"x\":834,\"y\":143},{\"x\":936,\"y\":144},{\"x\":936,\"y\":173},{\"x\":834,\"y\":172}],\"text\":\"Glasses\"},{\"boundingBox\":[{\"x\":941,\"y\":144},{\"x\":1033,\"y\":143},{\"x\":1032,\"y\":173},{\"x\":941,\"y\":173}],\"text\":\"model\"},{\"boundingBox\":[{\"x\":54,\"y\":244},{\"x\":134,\"y\":245},{\"x\":134,\"y\":278},{\"x\":53,\"y\":277}],\"text\":\"Scene\"},{\"boundingBox\":[{\"x\":140,\"y\":245},{\"x\":295,\"y\":247},{\"x\":295,\"y\":280},{\"x\":140,\"y\":279}],\"text\":\"acquisition\"},{\"boundingBox\":[{\"x\":414,\"y\":266},{\"x\":533,\"y\":266},{\"x\":532,\"y\":298},{\"x\":414,\"y\":295}],\"text\":\"Tracking\"},{\"boundingBox\":[{\"x\":539,\"y\":266},{\"x\":647,\"y\":268},{\"x\":645,\"y\":300},{\"x\":538,\"y\":298}],\"text\":\"registry\"},{\"boundingBox\":[{\"x\":763,\"y\":266},{\"x\":851,\"y\":266},{\"x\":851,\"y\":296},{\"x\":763,\"y\":295}],\"text\":\"Model\"},{\"boundingBox\":[{\"x\":857,\"y\":266},{\"x\":1001,\"y\":267},{\"x\":1000,\"y\":297},{\"x\":857,\"y\":296}],\"text\":\"generation\"},{\"boundingBox\":[{\"x\":1006,\"y\":267},{\"x\":1102,\"y\":269},{\"x\":1102,\"y\":296},{\"x\":1006,\"y\":297}],\"text\":\"system\"},{\"boundingBox\":[{\"x\":124,\"y\":291},{\"x\":215,\"y\":292},{\"x\":214,\"y\":314},{\"x\":125,\"y\":313}],\"text\":\"system\"},{\"boundingBox\":[{\"x\":341,\"y\":375},{\"x\":433,\"y\":375},{\"x\":433,\"y\":406},{\"x\":342,\"y\":405}],\"text\":\"Virtual\"},{\"boundingBox\":[{\"x\":439,\"y\":374},{\"x\":527,\"y\":375},{\"x\":527,\"y\":407},{\"x\":439,\"y\":406}],\"text\":\"reality\"},{\"boundingBox\":[{\"x\":533,\"y\":375},{\"x\":660,\"y\":376},{\"x\":659,\"y\":407},{\"x\":533,\"y\":407}],\"text\":\"synthesis\"},{\"boundingBox\":[{\"x\":666,\"y\":376},{\"x\":763,\"y\":378},{\"x\":763,\"y\":406},{\"x\":665,\"y\":407}],\"text\":\"system\"},{\"boundingBox\":[{\"x\":29,\"y\":409},{\"x\":193,\"y\":409},{\"x\":194,\"y\":441},{\"x\":29,\"y\":441}],\"text\":\"Coordinates\"},{\"boundingBox\":[{\"x\":200,\"y\":409},{\"x\":229,\"y\":409},{\"x\":230,\"y\":441},{\"x\":200,\"y\":441}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":236,\"y\":409},{\"x\":290,\"y\":408},{\"x\":291,\"y\":441},{\"x\":236,\"y\":441}],\"text\":\"real\"},{\"boundingBox\":[{\"x\":840,\"y\":407},{\"x\":1006,\"y\":406},{\"x\":1006,\"y\":438},{\"x\":840,\"y\":438}],\"text\":\"Coordinates\"},{\"boundingBox\":[{\"x\":1012,\"y\":406},{\"x\":1043,\"y\":406},{\"x\":1042,\"y\":438},{\"x\":1012,\"y\":438}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":1049,\"y\":406},{\"x\":1095,\"y\":406},{\"x\":1095,\"y\":438},{\"x\":1048,\"y\":438}],\"text\":\"the\"},{\"boundingBox\":[{\"x\":62,\"y\":453},{\"x\":141,\"y\":452},{\"x\":141,\"y\":480},{\"x\":64,\"y\":480}],\"text\":\"world\"},{\"boundingBox\":[{\"x\":146,\"y\":452},{\"x\":262,\"y\":454},{\"x\":262,\"y\":480},{\"x\":147,\"y\":480}],\"text\":\"cameras\"},{\"boundingBox\":[{\"x\":827,\"y\":450},{\"x\":916,\"y\":449},{\"x\":916,\"y\":477},{\"x\":827,\"y\":477}],\"text\":\"virtual\"},{\"boundingBox\":[{\"x\":921,\"y\":449},{\"x\":1004,\"y\":449},{\"x\":1004,\"y\":477},{\"x\":922,\"y\":477}],\"text\":\"world\"},{\"boundingBox\":[{\"x\":1009,\"y\":449},{\"x\":1110,\"y\":451},{\"x\":1110,\"y\":478},{\"x\":1009,\"y\":477}],\"text\":\"camera\"},{\"boundingBox\":[{\"x\":456,\"y\":492},{\"x\":552,\"y\":494},{\"x\":552,\"y\":525},{\"x\":455,\"y\":525}],\"text\":\"display\"},{\"boundingBox\":[{\"x\":559,\"y\":494},{\"x\":660,\"y\":495},{\"x\":660,\"y\":524},{\"x\":558,\"y\":525}],\"text\":\"system\"},{\"boundingBox\":[{\"x\":520,\"y\":613},{\"x\":587,\"y\":614},{\"x\":586,\"y\":641},{\"x\":520,\"y\":639}],\"text\":\"User\"},{\"boundingBox\":[{\"x\":840,\"y\":612},{\"x\":878,\"y\":612},{\"x\":877,\"y\":639},{\"x\":840,\"y\":638}],\"text\":\"UI\"}]}", "{\"language\":\"en\",\"text\":\"Augmented reality glasses sales system Commodity User Collection shopping Order discount photo setting browsing of goods trolley coupon wall up commodity list Glasses photogr uploading sharing Show photos comments sharing Try on browsing screening try-on aphing of trying on and likes quickly commodity Product modification Address Order statistics and deletion management Generated\",\"lines\":[{\"boundingBox\":[{\"x\":891,\"y\":19},{\"x\":1455,\"y\":19},{\"x\":1455,\"y\":55},{\"x\":891,\"y\":56}],\"text\":\"Augmented reality glasses sales system\"},{\"boundingBox\":[{\"x\":434,\"y\":231},{\"x\":611,\"y\":232},{\"x\":610,\"y\":267},{\"x\":434,\"y\":265}],\"text\":\"Commodity\"},{\"boundingBox\":[{\"x\":659,\"y\":229},{\"x\":728,\"y\":231},{\"x\":726,\"y\":259},{\"x\":658,\"y\":256}],\"text\":\"User\"},{\"boundingBox\":[{\"x\":790,\"y\":228},{\"x\":948,\"y\":229},{\"x\":948,\"y\":259},{\"x\":790,\"y\":258}],\"text\":\"Collection\"},{\"boundingBox\":[{\"x\":1035,\"y\":230},{\"x\":1173,\"y\":231},{\"x\":1172,\"y\":264},{\"x\":1034,\"y\":262}],\"text\":\"shopping\"},{\"boundingBox\":[{\"x\":1226,\"y\":229},{\"x\":1318,\"y\":230},{\"x\":1318,\"y\":258},{\"x\":1226,\"y\":258}],\"text\":\"Order\"},{\"boundingBox\":[{\"x\":1359,\"y\":229},{\"x\":1491,\"y\":230},{\"x\":1490,\"y\":260},{\"x\":1359,\"y\":258}],\"text\":\"discount\"},{\"boundingBox\":[{\"x\":1526,\"y\":231},{\"x\":1613,\"y\":229},{\"x\":1613,\"y\":261},{\"x\":1526,\"y\":262}],\"text\":\"photo\"},{\"boundingBox\":[{\"x\":1669,\"y\":232},{\"x\":1772,\"y\":233},{\"x\":1771,\"y\":263},{\"x\":1669,\"y\":261}],\"text\":\"setting\"},{\"boundingBox\":[{\"x\":449,\"y\":274},{\"x\":589,\"y\":278},{\"x\":588,\"y\":310},{\"x\":449,\"y\":305}],\"text\":\"browsing\"},{\"boundingBox\":[{\"x\":802,\"y\":271},{\"x\":932,\"y\":270},{\"x\":932,\"y\":304},{\"x\":802,\"y\":305}],\"text\":\"of goods\"},{\"boundingBox\":[{\"x\":1054,\"y\":270},{\"x\":1152,\"y\":272},{\"x\":1151,\"y\":305},{\"x\":1053,\"y\":303}],\"text\":\"trolley\"},{\"boundingBox\":[{\"x\":1370,\"y\":275},{\"x\":1478,\"y\":275},{\"x\":1478,\"y\":305},{\"x\":1370,\"y\":305}],\"text\":\"coupon\"},{\"boundingBox\":[{\"x\":1537,\"y\":273},{\"x\":1604,\"y\":271},{\"x\":1603,\"y\":300},{\"x\":1538,\"y\":301}],\"text\":\"wall\"},{\"boundingBox\":[{\"x\":1700,\"y\":275},{\"x\":1741,\"y\":278},{\"x\":1738,\"y\":306},{\"x\":1697,\"y\":303}],\"text\":\"up\"},{\"boundingBox\":[{\"x\":19,\"y\":444},{\"x\":183,\"y\":444},{\"x\":183,\"y\":473},{\"x\":19,\"y\":473}],\"text\":\"commodity\"},{\"boundingBox\":[{\"x\":266,\"y\":440},{\"x\":315,\"y\":442},{\"x\":315,\"y\":470},{\"x\":265,\"y\":469}],\"text\":\"list\"},{\"boundingBox\":[{\"x\":401,\"y\":441},{\"x\":515,\"y\":443},{\"x\":514,\"y\":473},{\"x\":401,\"y\":471}],\"text\":\"Glasses\"},{\"boundingBox\":[{\"x\":562,\"y\":443},{\"x\":677,\"y\":444},{\"x\":677,\"y\":476},{\"x\":562,\"y\":475}],\"text\":\"photogr\"},{\"boundingBox\":[{\"x\":712,\"y\":438},{\"x\":865,\"y\":438},{\"x\":865,\"y\":481},{\"x\":712,\"y\":482}],\"text\":\"uploading\"},{\"boundingBox\":[{\"x\":893,\"y\":443},{\"x\":1004,\"y\":446},{\"x\":1004,\"y\":477},{\"x\":892,\"y\":474}],\"text\":\"sharing\"},{\"boundingBox\":[{\"x\":1152,\"y\":434},{\"x\":1344,\"y\":435},{\"x\":1344,\"y\":468},{\"x\":1152,\"y\":467}],\"text\":\"Show photos\"},{\"boundingBox\":[{\"x\":1431,\"y\":439},{\"x\":1582,\"y\":440},{\"x\":1582,\"y\":464},{\"x\":1431,\"y\":464}],\"text\":\"comments\"},{\"boundingBox\":[{\"x\":1629,\"y\":434},{\"x\":1741,\"y\":437},{\"x\":1740,\"y\":469},{\"x\":1628,\"y\":464}],\"text\":\"sharing\"},{\"boundingBox\":[{\"x\":1802,\"y\":429},{\"x\":1900,\"y\":429},{\"x\":1900,\"y\":472},{\"x\":1803,\"y\":472}],\"text\":\"Try on\"},{\"boundingBox\":[{\"x\":33,\"y\":485},{\"x\":175,\"y\":487},{\"x\":174,\"y\":519},{\"x\":32,\"y\":516}],\"text\":\"browsing\"},{\"boundingBox\":[{\"x\":216,\"y\":487},{\"x\":360,\"y\":488},{\"x\":360,\"y\":518},{\"x\":215,\"y\":516}],\"text\":\"screening\"},{\"boundingBox\":[{\"x\":411,\"y\":489},{\"x\":501,\"y\":488},{\"x\":501,\"y\":516},{\"x\":411,\"y\":516}],\"text\":\"try-on\"},{\"boundingBox\":[{\"x\":570,\"y\":485},{\"x\":670,\"y\":486},{\"x\":669,\"y\":519},{\"x\":570,\"y\":519}],\"text\":\"aphing\"},{\"boundingBox\":[{\"x\":1161,\"y\":475},{\"x\":1331,\"y\":477},{\"x\":1331,\"y\":512},{\"x\":1161,\"y\":511}],\"text\":\"of trying on\"},{\"boundingBox\":[{\"x\":1440,\"y\":476},{\"x\":1571,\"y\":476},{\"x\":1572,\"y\":506},{\"x\":1440,\"y\":507}],\"text\":\"and likes\"},{\"boundingBox\":[{\"x\":1794,\"y\":476},{\"x\":1904,\"y\":476},{\"x\":1904,\"y\":510},{\"x\":1794,\"y\":510}],\"text\":\"quickly\"},{\"boundingBox\":[{\"x\":576,\"y\":660},{\"x\":742,\"y\":660},{\"x\":742,\"y\":692},{\"x\":576,\"y\":692}],\"text\":\"commodity\"},{\"boundingBox\":[{\"x\":792,\"y\":657},{\"x\":1095,\"y\":658},{\"x\":1095,\"y\":690},{\"x\":792,\"y\":690}],\"text\":\"Product modification\"},{\"boundingBox\":[{\"x\":1201,\"y\":658},{\"x\":1324,\"y\":659},{\"x\":1324,\"y\":690},{\"x\":1201,\"y\":689}],\"text\":\"Address\"},{\"boundingBox\":[{\"x\":1447,\"y\":658},{\"x\":1536,\"y\":660},{\"x\":1536,\"y\":690},{\"x\":1446,\"y\":688}],\"text\":\"Order\"},{\"boundingBox\":[{\"x\":593,\"y\":703},{\"x\":723,\"y\":703},{\"x\":723,\"y\":733},{\"x\":593,\"y\":733}],\"text\":\"statistics\"},{\"boundingBox\":[{\"x\":854,\"y\":701},{\"x\":1033,\"y\":701},{\"x\":1033,\"y\":733},{\"x\":854,\"y\":733}],\"text\":\"and deletion\"},{\"boundingBox\":[{\"x\":1169,\"y\":706},{\"x\":1356,\"y\":705},{\"x\":1356,\"y\":733},{\"x\":1169,\"y\":734}],\"text\":\"management\"},{\"boundingBox\":[{\"x\":1414,\"y\":699},{\"x\":1562,\"y\":699},{\"x\":1562,\"y\":732},{\"x\":1414,\"y\":732}],\"text\":\"Generated\"}],\"words\":[{\"boundingBox\":[{\"x\":892,\"y\":20},{\"x\":1054,\"y\":19},{\"x\":1054,\"y\":56},{\"x\":892,\"y\":56}],\"text\":\"Augmented\"},{\"boundingBox\":[{\"x\":1061,\"y\":19},{\"x\":1156,\"y\":19},{\"x\":1156,\"y\":56},{\"x\":1061,\"y\":56}],\"text\":\"reality\"},{\"boundingBox\":[{\"x\":1163,\"y\":19},{\"x\":1263,\"y\":19},{\"x\":1263,\"y\":56},{\"x\":1163,\"y\":56}],\"text\":\"glasses\"},{\"boundingBox\":[{\"x\":1270,\"y\":19},{\"x\":1342,\"y\":20},{\"x\":1341,\"y\":56},{\"x\":1270,\"y\":56}],\"text\":\"sales\"},{\"boundingBox\":[{\"x\":1349,\"y\":20},{\"x\":1456,\"y\":21},{\"x\":1455,\"y\":55},{\"x\":1348,\"y\":56}],\"text\":\"system\"},{\"boundingBox\":[{\"x\":435,\"y\":232},{\"x\":610,\"y\":234},{\"x\":608,\"y\":268},{\"x\":435,\"y\":265}],\"text\":\"Commodity\"},{\"boundingBox\":[{\"x\":659,\"y\":229},{\"x\":726,\"y\":231},{\"x\":725,\"y\":259},{\"x\":658,\"y\":256}],\"text\":\"User\"},{\"boundingBox\":[{\"x\":791,\"y\":229},{\"x\":948,\"y\":230},{\"x\":949,\"y\":260},{\"x\":791,\"y\":259}],\"text\":\"Collection\"},{\"boundingBox\":[{\"x\":1035,\"y\":230},{\"x\":1174,\"y\":232},{\"x\":1172,\"y\":265},{\"x\":1034,\"y\":261}],\"text\":\"shopping\"},{\"boundingBox\":[{\"x\":1228,\"y\":230},{\"x\":1319,\"y\":232},{\"x\":1318,\"y\":259},{\"x\":1227,\"y\":258}],\"text\":\"Order\"},{\"boundingBox\":[{\"x\":1360,\"y\":230},{\"x\":1492,\"y\":232},{\"x\":1491,\"y\":260},{\"x\":1360,\"y\":259}],\"text\":\"discount\"},{\"boundingBox\":[{\"x\":1526,\"y\":230},{\"x\":1611,\"y\":229},{\"x\":1611,\"y\":261},{\"x\":1526,\"y\":262}],\"text\":\"photo\"},{\"boundingBox\":[{\"x\":1669,\"y\":233},{\"x\":1772,\"y\":233},{\"x\":1770,\"y\":264},{\"x\":1670,\"y\":260}],\"text\":\"setting\"},{\"boundingBox\":[{\"x\":450,\"y\":275},{\"x\":589,\"y\":279},{\"x\":588,\"y\":311},{\"x\":449,\"y\":306}],\"text\":\"browsing\"},{\"boundingBox\":[{\"x\":803,\"y\":273},{\"x\":833,\"y\":272},{\"x\":834,\"y\":306},{\"x\":803,\"y\":306}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":839,\"y\":272},{\"x\":930,\"y\":271},{\"x\":931,\"y\":305},{\"x\":840,\"y\":306}],\"text\":\"goods\"},{\"boundingBox\":[{\"x\":1055,\"y\":270},{\"x\":1152,\"y\":273},{\"x\":1149,\"y\":306},{\"x\":1054,\"y\":302}],\"text\":\"trolley\"},{\"boundingBox\":[{\"x\":1372,\"y\":276},{\"x\":1477,\"y\":276},{\"x\":1477,\"y\":306},{\"x\":1371,\"y\":304}],\"text\":\"coupon\"},{\"boundingBox\":[{\"x\":1537,\"y\":272},{\"x\":1602,\"y\":271},{\"x\":1603,\"y\":300},{\"x\":1537,\"y\":301}],\"text\":\"wall\"},{\"boundingBox\":[{\"x\":1701,\"y\":275},{\"x\":1740,\"y\":278},{\"x\":1738,\"y\":306},{\"x\":1699,\"y\":303}],\"text\":\"up\"},{\"boundingBox\":[{\"x\":20,\"y\":445},{\"x\":184,\"y\":444},{\"x\":182,\"y\":474},{\"x\":19,\"y\":473}],\"text\":\"commodity\"},{\"boundingBox\":[{\"x\":265,\"y\":440},{\"x\":313,\"y\":441},{\"x\":312,\"y\":470},{\"x\":265,\"y\":468}],\"text\":\"list\"},{\"boundingBox\":[{\"x\":401,\"y\":441},{\"x\":515,\"y\":443},{\"x\":514,\"y\":474},{\"x\":401,\"y\":472}],\"text\":\"Glasses\"},{\"boundingBox\":[{\"x\":562,\"y\":443},{\"x\":677,\"y\":445},{\"x\":676,\"y\":477},{\"x\":563,\"y\":476}],\"text\":\"photogr\"},{\"boundingBox\":[{\"x\":713,\"y\":439},{\"x\":866,\"y\":438},{\"x\":866,\"y\":483},{\"x\":713,\"y\":483}],\"text\":\"uploading\"},{\"boundingBox\":[{\"x\":894,\"y\":443},{\"x\":1005,\"y\":446},{\"x\":1003,\"y\":478},{\"x\":893,\"y\":474}],\"text\":\"sharing\"},{\"boundingBox\":[{\"x\":1152,\"y\":435},{\"x\":1233,\"y\":435},{\"x\":1233,\"y\":469},{\"x\":1152,\"y\":467}],\"text\":\"Show\"},{\"boundingBox\":[{\"x\":1239,\"y\":435},{\"x\":1343,\"y\":437},{\"x\":1342,\"y\":468},{\"x\":1239,\"y\":469}],\"text\":\"photos\"},{\"boundingBox\":[{\"x\":1432,\"y\":440},{\"x\":1581,\"y\":440},{\"x\":1581,\"y\":465},{\"x\":1431,\"y\":464}],\"text\":\"comments\"},{\"boundingBox\":[{\"x\":1629,\"y\":435},{\"x\":1741,\"y\":439},{\"x\":1740,\"y\":470},{\"x\":1629,\"y\":465}],\"text\":\"sharing\"},{\"boundingBox\":[{\"x\":1802,\"y\":429},{\"x\":1848,\"y\":429},{\"x\":1848,\"y\":472},{\"x\":1802,\"y\":472}],\"text\":\"Try\"},{\"boundingBox\":[{\"x\":1856,\"y\":429},{\"x\":1899,\"y\":429},{\"x\":1899,\"y\":472},{\"x\":1856,\"y\":472}],\"text\":\"on\"},{\"boundingBox\":[{\"x\":33,\"y\":485},{\"x\":175,\"y\":487},{\"x\":173,\"y\":520},{\"x\":32,\"y\":515}],\"text\":\"browsing\"},{\"boundingBox\":[{\"x\":216,\"y\":487},{\"x\":360,\"y\":488},{\"x\":359,\"y\":519},{\"x\":216,\"y\":515}],\"text\":\"screening\"},{\"boundingBox\":[{\"x\":411,\"y\":489},{\"x\":501,\"y\":489},{\"x\":501,\"y\":516},{\"x\":411,\"y\":517}],\"text\":\"try-on\"},{\"boundingBox\":[{\"x\":570,\"y\":486},{\"x\":670,\"y\":486},{\"x\":669,\"y\":520},{\"x\":570,\"y\":520}],\"text\":\"aphing\"},{\"boundingBox\":[{\"x\":1162,\"y\":475},{\"x\":1188,\"y\":476},{\"x\":1187,\"y\":512},{\"x\":1161,\"y\":512}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":1195,\"y\":476},{\"x\":1284,\"y\":478},{\"x\":1283,\"y\":512},{\"x\":1195,\"y\":512}],\"text\":\"trying\"},{\"boundingBox\":[{\"x\":1291,\"y\":479},{\"x\":1332,\"y\":479},{\"x\":1331,\"y\":512},{\"x\":1290,\"y\":512}],\"text\":\"on\"},{\"boundingBox\":[{\"x\":1441,\"y\":478},{\"x\":1493,\"y\":477},{\"x\":1494,\"y\":506},{\"x\":1441,\"y\":508}],\"text\":\"and\"},{\"boundingBox\":[{\"x\":1499,\"y\":477},{\"x\":1571,\"y\":477},{\"x\":1571,\"y\":507},{\"x\":1499,\"y\":506}],\"text\":\"likes\"},{\"boundingBox\":[{\"x\":1795,\"y\":477},{\"x\":1904,\"y\":477},{\"x\":1903,\"y\":511},{\"x\":1794,\"y\":510}],\"text\":\"quickly\"},{\"boundingBox\":[{\"x\":577,\"y\":662},{\"x\":742,\"y\":661},{\"x\":741,\"y\":693},{\"x\":577,\"y\":691}],\"text\":\"commodity\"},{\"boundingBox\":[{\"x\":793,\"y\":659},{\"x\":901,\"y\":658},{\"x\":901,\"y\":691},{\"x\":793,\"y\":690}],\"text\":\"Product\"},{\"boundingBox\":[{\"x\":907,\"y\":658},{\"x\":1094,\"y\":659},{\"x\":1094,\"y\":691},{\"x\":907,\"y\":691}],\"text\":\"modification\"},{\"boundingBox\":[{\"x\":1203,\"y\":659},{\"x\":1324,\"y\":661},{\"x\":1323,\"y\":690},{\"x\":1202,\"y\":690}],\"text\":\"Address\"},{\"boundingBox\":[{\"x\":1446,\"y\":658},{\"x\":1536,\"y\":660},{\"x\":1535,\"y\":690},{\"x\":1446,\"y\":688}],\"text\":\"Order\"},{\"boundingBox\":[{\"x\":593,\"y\":704},{\"x\":724,\"y\":704},{\"x\":724,\"y\":734},{\"x\":593,\"y\":734}],\"text\":\"statistics\"},{\"boundingBox\":[{\"x\":854,\"y\":703},{\"x\":906,\"y\":702},{\"x\":906,\"y\":733},{\"x\":855,\"y\":733}],\"text\":\"and\"},{\"boundingBox\":[{\"x\":912,\"y\":702},{\"x\":1032,\"y\":701},{\"x\":1033,\"y\":734},{\"x\":912,\"y\":733}],\"text\":\"deletion\"},{\"boundingBox\":[{\"x\":1170,\"y\":708},{\"x\":1356,\"y\":706},{\"x\":1356,\"y\":734},{\"x\":1170,\"y\":733}],\"text\":\"management\"},{\"boundingBox\":[{\"x\":1415,\"y\":701},{\"x\":1560,\"y\":700},{\"x\":1561,\"y\":733},{\"x\":1414,\"y\":731}],\"text\":\"Generated\"}]}", "{\"language\":\"en\",\"text\":\"Start Import Image Input the training face sample Is there a face? Image normalization, Calculate the average face SVM classitier SIFT Extract local features Is there a face? SDM Iterative Solution Feature extraction Face Alignment Feature point Face rotation Calculate the perspective relationship The lens selection Angle between 2D and 3D coordinates library Shoot the The glasses 3d effect Transparency perspective picking try-on effect overlap the face design treatment transformation out a lens End\",\"lines\":[{\"boundingBox\":[{\"x\":320,\"y\":17},{\"x\":390,\"y\":18},{\"x\":390,\"y\":47},{\"x\":319,\"y\":46}],\"text\":\"Start\"},{\"boundingBox\":[{\"x\":254,\"y\":140},{\"x\":451,\"y\":142},{\"x\":450,\"y\":175},{\"x\":254,\"y\":172}],\"text\":\"Import Image\"},{\"boundingBox\":[{\"x\":696,\"y\":122},{\"x\":944,\"y\":123},{\"x\":943,\"y\":160},{\"x\":696,\"y\":158}],\"text\":\"Input the training\"},{\"boundingBox\":[{\"x\":737,\"y\":164},{\"x\":906,\"y\":167},{\"x\":905,\"y\":200},{\"x\":737,\"y\":197}],\"text\":\"face sample\"},{\"boundingBox\":[{\"x\":238,\"y\":301},{\"x\":452,\"y\":302},{\"x\":451,\"y\":335},{\"x\":238,\"y\":333}],\"text\":\"Is there a face?\"},{\"boundingBox\":[{\"x\":671,\"y\":283},{\"x\":970,\"y\":283},{\"x\":970,\"y\":318},{\"x\":671,\"y\":318}],\"text\":\"Image normalization,\"},{\"boundingBox\":[{\"x\":634,\"y\":322},{\"x\":1007,\"y\":325},{\"x\":1007,\"y\":359},{\"x\":633,\"y\":357}],\"text\":\"Calculate the average face\"},{\"boundingBox\":[{\"x\":246,\"y\":443},{\"x\":464,\"y\":444},{\"x\":464,\"y\":477},{\"x\":246,\"y\":476}],\"text\":\"SVM classitier\"},{\"boundingBox\":[{\"x\":627,\"y\":458},{\"x\":1012,\"y\":461},{\"x\":1012,\"y\":493},{\"x\":627,\"y\":490}],\"text\":\"SIFT Extract local features\"},{\"boundingBox\":[{\"x\":217,\"y\":577},{\"x\":450,\"y\":579},{\"x\":450,\"y\":612},{\"x\":217,\"y\":610}],\"text\":\"Is there a face?\"},{\"boundingBox\":[{\"x\":650,\"y\":594},{\"x\":985,\"y\":595},{\"x\":985,\"y\":627},{\"x\":650,\"y\":625}],\"text\":\"SDM Iterative Solution\"},{\"boundingBox\":[{\"x\":222,\"y\":713},{\"x\":486,\"y\":715},{\"x\":486,\"y\":745},{\"x\":222,\"y\":743}],\"text\":\"Feature extraction\"},{\"boundingBox\":[{\"x\":700,\"y\":713},{\"x\":931,\"y\":715},{\"x\":931,\"y\":747},{\"x\":699,\"y\":745}],\"text\":\"Face Alignment\"},{\"boundingBox\":[{\"x\":76,\"y\":880},{\"x\":277,\"y\":880},{\"x\":277,\"y\":914},{\"x\":76,\"y\":914}],\"text\":\"Feature point\"},{\"boundingBox\":[{\"x\":397,\"y\":886},{\"x\":583,\"y\":886},{\"x\":583,\"y\":916},{\"x\":397,\"y\":915}],\"text\":\"Face rotation\"},{\"boundingBox\":[{\"x\":671,\"y\":880},{\"x\":1193,\"y\":880},{\"x\":1193,\"y\":917},{\"x\":671,\"y\":915}],\"text\":\"Calculate the perspective relationship\"},{\"boundingBox\":[{\"x\":1253,\"y\":890},{\"x\":1377,\"y\":891},{\"x\":1377,\"y\":922},{\"x\":1253,\"y\":920}],\"text\":\"The lens\"},{\"boundingBox\":[{\"x\":111,\"y\":925},{\"x\":240,\"y\":923},{\"x\":240,\"y\":951},{\"x\":112,\"y\":954}],\"text\":\"selection\"},{\"boundingBox\":[{\"x\":448,\"y\":927},{\"x\":533,\"y\":929},{\"x\":531,\"y\":958},{\"x\":446,\"y\":958}],\"text\":\"Angle\"},{\"boundingBox\":[{\"x\":708,\"y\":922},{\"x\":1156,\"y\":923},{\"x\":1155,\"y\":958},{\"x\":708,\"y\":956}],\"text\":\"between 2D and 3D coordinates\"},{\"boundingBox\":[{\"x\":1268,\"y\":931},{\"x\":1362,\"y\":932},{\"x\":1362,\"y\":963},{\"x\":1268,\"y\":961}],\"text\":\"library\"},{\"boundingBox\":[{\"x\":38,\"y\":1043},{\"x\":176,\"y\":1046},{\"x\":175,\"y\":1077},{\"x\":37,\"y\":1075}],\"text\":\"Shoot the\"},{\"boundingBox\":[{\"x\":275,\"y\":1044},{\"x\":441,\"y\":1046},{\"x\":440,\"y\":1080},{\"x\":275,\"y\":1077}],\"text\":\"The glasses\"},{\"boundingBox\":[{\"x\":519,\"y\":1042},{\"x\":647,\"y\":1044},{\"x\":646,\"y\":1075},{\"x\":518,\"y\":1072}],\"text\":\"3d effect\"},{\"boundingBox\":[{\"x\":707,\"y\":1044},{\"x\":892,\"y\":1048},{\"x\":892,\"y\":1079},{\"x\":707,\"y\":1074}],\"text\":\"Transparency\"},{\"boundingBox\":[{\"x\":998,\"y\":1046},{\"x\":1163,\"y\":1044},{\"x\":1163,\"y\":1075},{\"x\":999,\"y\":1079}],\"text\":\"perspective\"},{\"boundingBox\":[{\"x\":1264,\"y\":1048},{\"x\":1377,\"y\":1049},{\"x\":1377,\"y\":1082},{\"x\":1264,\"y\":1080}],\"text\":\"picking\"},{\"boundingBox\":[{\"x\":16,\"y\":1089},{\"x\":198,\"y\":1087},{\"x\":199,\"y\":1117},{\"x\":17,\"y\":1120}],\"text\":\"try-on effect\"},{\"boundingBox\":[{\"x\":245,\"y\":1088},{\"x\":472,\"y\":1087},{\"x\":472,\"y\":1119},{\"x\":245,\"y\":1119}],\"text\":\"overlap the face\"},{\"boundingBox\":[{\"x\":535,\"y\":1084},{\"x\":632,\"y\":1087},{\"x\":630,\"y\":1119},{\"x\":534,\"y\":1117}],\"text\":\"design\"},{\"boundingBox\":[{\"x\":730,\"y\":1089},{\"x\":868,\"y\":1089},{\"x\":868,\"y\":1117},{\"x\":730,\"y\":1117}],\"text\":\"treatment\"},{\"boundingBox\":[{\"x\":976,\"y\":1087},{\"x\":1185,\"y\":1085},{\"x\":1186,\"y\":1116},{\"x\":976,\"y\":1118}],\"text\":\"transformation\"},{\"boundingBox\":[{\"x\":1243,\"y\":1088},{\"x\":1385,\"y\":1087},{\"x\":1386,\"y\":1116},{\"x\":1243,\"y\":1117}],\"text\":\"out a lens\"},{\"boundingBox\":[{\"x\":76,\"y\":1208},{\"x\":137,\"y\":1208},{\"x\":137,\"y\":1236},{\"x\":76,\"y\":1237}],\"text\":\"End\"}],\"words\":[{\"boundingBox\":[{\"x\":319,\"y\":17},{\"x\":389,\"y\":18},{\"x\":388,\"y\":47},{\"x\":319,\"y\":46}],\"text\":\"Start\"},{\"boundingBox\":[{\"x\":254,\"y\":140},{\"x\":351,\"y\":142},{\"x\":351,\"y\":175},{\"x\":255,\"y\":172}],\"text\":\"Import\"},{\"boundingBox\":[{\"x\":357,\"y\":142},{\"x\":450,\"y\":143},{\"x\":449,\"y\":176},{\"x\":357,\"y\":175}],\"text\":\"Image\"},{\"boundingBox\":[{\"x\":696,\"y\":124},{\"x\":770,\"y\":123},{\"x\":770,\"y\":158},{\"x\":696,\"y\":157}],\"text\":\"Input\"},{\"boundingBox\":[{\"x\":777,\"y\":123},{\"x\":820,\"y\":123},{\"x\":819,\"y\":158},{\"x\":776,\"y\":158}],\"text\":\"the\"},{\"boundingBox\":[{\"x\":827,\"y\":123},{\"x\":943,\"y\":124},{\"x\":941,\"y\":161},{\"x\":826,\"y\":158}],\"text\":\"training\"},{\"boundingBox\":[{\"x\":738,\"y\":165},{\"x\":793,\"y\":165},{\"x\":793,\"y\":199},{\"x\":738,\"y\":196}],\"text\":\"face\"},{\"boundingBox\":[{\"x\":800,\"y\":165},{\"x\":905,\"y\":168},{\"x\":902,\"y\":199},{\"x\":799,\"y\":199}],\"text\":\"sample\"},{\"boundingBox\":[{\"x\":239,\"y\":302},{\"x\":264,\"y\":302},{\"x\":264,\"y\":333},{\"x\":238,\"y\":332}],\"text\":\"Is\"},{\"boundingBox\":[{\"x\":270,\"y\":302},{\"x\":341,\"y\":303},{\"x\":340,\"y\":335},{\"x\":270,\"y\":333}],\"text\":\"there\"},{\"boundingBox\":[{\"x\":347,\"y\":303},{\"x\":368,\"y\":303},{\"x\":367,\"y\":335},{\"x\":346,\"y\":335}],\"text\":\"a\"},{\"boundingBox\":[{\"x\":374,\"y\":303},{\"x\":452,\"y\":303},{\"x\":452,\"y\":335},{\"x\":373,\"y\":335}],\"text\":\"face?\"},{\"boundingBox\":[{\"x\":672,\"y\":285},{\"x\":755,\"y\":284},{\"x\":754,\"y\":319},{\"x\":672,\"y\":317}],\"text\":\"Image\"},{\"boundingBox\":[{\"x\":761,\"y\":284},{\"x\":970,\"y\":284},{\"x\":969,\"y\":318},{\"x\":761,\"y\":319}],\"text\":\"normalization,\"},{\"boundingBox\":[{\"x\":635,\"y\":323},{\"x\":766,\"y\":324},{\"x\":765,\"y\":359},{\"x\":634,\"y\":356}],\"text\":\"Calculate\"},{\"boundingBox\":[{\"x\":772,\"y\":324},{\"x\":818,\"y\":325},{\"x\":818,\"y\":359},{\"x\":772,\"y\":359}],\"text\":\"the\"},{\"boundingBox\":[{\"x\":825,\"y\":325},{\"x\":936,\"y\":325},{\"x\":936,\"y\":359},{\"x\":824,\"y\":359}],\"text\":\"average\"},{\"boundingBox\":[{\"x\":942,\"y\":325},{\"x\":1008,\"y\":326},{\"x\":1008,\"y\":358},{\"x\":942,\"y\":359}],\"text\":\"face\"},{\"boundingBox\":[{\"x\":247,\"y\":444},{\"x\":321,\"y\":445},{\"x\":320,\"y\":476},{\"x\":246,\"y\":476}],\"text\":\"SVM\"},{\"boundingBox\":[{\"x\":330,\"y\":445},{\"x\":465,\"y\":445},{\"x\":464,\"y\":478},{\"x\":329,\"y\":476}],\"text\":\"classitier\"},{\"boundingBox\":[{\"x\":628,\"y\":459},{\"x\":700,\"y\":459},{\"x\":700,\"y\":491},{\"x\":628,\"y\":491}],\"text\":\"SIFT\"},{\"boundingBox\":[{\"x\":707,\"y\":459},{\"x\":808,\"y\":460},{\"x\":808,\"y\":492},{\"x\":706,\"y\":491}],\"text\":\"Extract\"},{\"boundingBox\":[{\"x\":815,\"y\":460},{\"x\":885,\"y\":460},{\"x\":884,\"y\":492},{\"x\":814,\"y\":492}],\"text\":\"local\"},{\"boundingBox\":[{\"x\":892,\"y\":460},{\"x\":1012,\"y\":461},{\"x\":1010,\"y\":494},{\"x\":890,\"y\":492}],\"text\":\"features\"},{\"boundingBox\":[{\"x\":236,\"y\":578},{\"x\":261,\"y\":578},{\"x\":260,\"y\":611},{\"x\":234,\"y\":610}],\"text\":\"Is\"},{\"boundingBox\":[{\"x\":268,\"y\":578},{\"x\":339,\"y\":580},{\"x\":338,\"y\":612},{\"x\":266,\"y\":611}],\"text\":\"there\"},{\"boundingBox\":[{\"x\":346,\"y\":580},{\"x\":365,\"y\":580},{\"x\":364,\"y\":612},{\"x\":345,\"y\":612}],\"text\":\"a\"},{\"boundingBox\":[{\"x\":371,\"y\":580},{\"x\":451,\"y\":580},{\"x\":451,\"y\":611},{\"x\":370,\"y\":612}],\"text\":\"face?\"},{\"boundingBox\":[{\"x\":650,\"y\":594},{\"x\":727,\"y\":595},{\"x\":728,\"y\":626},{\"x\":651,\"y\":625}],\"text\":\"SDM\"},{\"boundingBox\":[{\"x\":736,\"y\":595},{\"x\":853,\"y\":595},{\"x\":854,\"y\":627},{\"x\":736,\"y\":626}],\"text\":\"Iterative\"},{\"boundingBox\":[{\"x\":859,\"y\":595},{\"x\":983,\"y\":596},{\"x\":984,\"y\":627},{\"x\":860,\"y\":627}],\"text\":\"Solution\"},{\"boundingBox\":[{\"x\":224,\"y\":713},{\"x\":330,\"y\":715},{\"x\":329,\"y\":745},{\"x\":222,\"y\":744}],\"text\":\"Feature\"},{\"boundingBox\":[{\"x\":336,\"y\":715},{\"x\":485,\"y\":715},{\"x\":486,\"y\":746},{\"x\":335,\"y\":745}],\"text\":\"extraction\"},{\"boundingBox\":[{\"x\":701,\"y\":713},{\"x\":770,\"y\":714},{\"x\":769,\"y\":746},{\"x\":700,\"y\":744}],\"text\":\"Face\"},{\"boundingBox\":[{\"x\":776,\"y\":714},{\"x\":930,\"y\":716},{\"x\":930,\"y\":747},{\"x\":775,\"y\":747}],\"text\":\"Alignment\"},{\"boundingBox\":[{\"x\":77,\"y\":881},{\"x\":183,\"y\":881},{\"x\":184,\"y\":915},{\"x\":77,\"y\":913}],\"text\":\"Feature\"},{\"boundingBox\":[{\"x\":190,\"y\":881},{\"x\":277,\"y\":880},{\"x\":278,\"y\":914},{\"x\":190,\"y\":915}],\"text\":\"point\"},{\"boundingBox\":[{\"x\":399,\"y\":887},{\"x\":463,\"y\":886},{\"x\":461,\"y\":916},{\"x\":397,\"y\":916}],\"text\":\"Face\"},{\"boundingBox\":[{\"x\":468,\"y\":886},{\"x\":583,\"y\":887},{\"x\":582,\"y\":917},{\"x\":467,\"y\":916}],\"text\":\"rotation\"},{\"boundingBox\":[{\"x\":672,\"y\":881},{\"x\":800,\"y\":880},{\"x\":800,\"y\":916},{\"x\":672,\"y\":914}],\"text\":\"Calculate\"},{\"boundingBox\":[{\"x\":807,\"y\":880},{\"x\":852,\"y\":880},{\"x\":851,\"y\":916},{\"x\":806,\"y\":916}],\"text\":\"the\"},{\"boundingBox\":[{\"x\":858,\"y\":880},{\"x\":1019,\"y\":880},{\"x\":1018,\"y\":917},{\"x\":858,\"y\":916}],\"text\":\"perspective\"},{\"boundingBox\":[{\"x\":1025,\"y\":880},{\"x\":1194,\"y\":881},{\"x\":1193,\"y\":917},{\"x\":1025,\"y\":917}],\"text\":\"relationship\"},{\"boundingBox\":[{\"x\":1255,\"y\":891},{\"x\":1310,\"y\":891},{\"x\":1311,\"y\":922},{\"x\":1256,\"y\":921}],\"text\":\"The\"},{\"boundingBox\":[{\"x\":1316,\"y\":891},{\"x\":1377,\"y\":892},{\"x\":1378,\"y\":922},{\"x\":1317,\"y\":922}],\"text\":\"lens\"},{\"boundingBox\":[{\"x\":112,\"y\":927},{\"x\":240,\"y\":924},{\"x\":240,\"y\":952},{\"x\":112,\"y\":954}],\"text\":\"selection\"},{\"boundingBox\":[{\"x\":446,\"y\":927},{\"x\":533,\"y\":928},{\"x\":532,\"y\":959},{\"x\":446,\"y\":958}],\"text\":\"Angle\"},{\"boundingBox\":[{\"x\":708,\"y\":924},{\"x\":826,\"y\":923},{\"x\":826,\"y\":956},{\"x\":710,\"y\":957}],\"text\":\"between\"},{\"boundingBox\":[{\"x\":832,\"y\":923},{\"x\":875,\"y\":923},{\"x\":875,\"y\":956},{\"x\":833,\"y\":956}],\"text\":\"2D\"},{\"boundingBox\":[{\"x\":881,\"y\":923},{\"x\":934,\"y\":923},{\"x\":935,\"y\":956},{\"x\":882,\"y\":956}],\"text\":\"and\"},{\"boundingBox\":[{\"x\":941,\"y\":923},{\"x\":983,\"y\":923},{\"x\":983,\"y\":957},{\"x\":941,\"y\":956}],\"text\":\"3D\"},{\"boundingBox\":[{\"x\":990,\"y\":923},{\"x\":1156,\"y\":925},{\"x\":1155,\"y\":959},{\"x\":990,\"y\":957}],\"text\":\"coordinates\"},{\"boundingBox\":[{\"x\":1269,\"y\":932},{\"x\":1362,\"y\":932},{\"x\":1361,\"y\":964},{\"x\":1268,\"y\":961}],\"text\":\"library\"},{\"boundingBox\":[{\"x\":39,\"y\":1044},{\"x\":121,\"y\":1046},{\"x\":120,\"y\":1077},{\"x\":38,\"y\":1075}],\"text\":\"Shoot\"},{\"boundingBox\":[{\"x\":127,\"y\":1046},{\"x\":177,\"y\":1046},{\"x\":176,\"y\":1078},{\"x\":126,\"y\":1077}],\"text\":\"the\"},{\"boundingBox\":[{\"x\":276,\"y\":1045},{\"x\":330,\"y\":1046},{\"x\":330,\"y\":1079},{\"x\":276,\"y\":1077}],\"text\":\"The\"},{\"boundingBox\":[{\"x\":336,\"y\":1047},{\"x\":441,\"y\":1047},{\"x\":440,\"y\":1079},{\"x\":336,\"y\":1079}],\"text\":\"glasses\"},{\"boundingBox\":[{\"x\":520,\"y\":1042},{\"x\":554,\"y\":1043},{\"x\":553,\"y\":1074},{\"x\":519,\"y\":1072}],\"text\":\"3d\"},{\"boundingBox\":[{\"x\":560,\"y\":1044},{\"x\":645,\"y\":1044},{\"x\":645,\"y\":1075},{\"x\":559,\"y\":1074}],\"text\":\"effect\"},{\"boundingBox\":[{\"x\":707,\"y\":1044},{\"x\":893,\"y\":1048},{\"x\":891,\"y\":1079},{\"x\":708,\"y\":1075}],\"text\":\"Transparency\"},{\"boundingBox\":[{\"x\":999,\"y\":1048},{\"x\":1163,\"y\":1045},{\"x\":1163,\"y\":1076},{\"x\":999,\"y\":1080}],\"text\":\"perspective\"},{\"boundingBox\":[{\"x\":1265,\"y\":1049},{\"x\":1378,\"y\":1052},{\"x\":1377,\"y\":1082},{\"x\":1265,\"y\":1081}],\"text\":\"picking\"},{\"boundingBox\":[{\"x\":17,\"y\":1089},{\"x\":104,\"y\":1089},{\"x\":104,\"y\":1120},{\"x\":18,\"y\":1120}],\"text\":\"try-on\"},{\"boundingBox\":[{\"x\":110,\"y\":1089},{\"x\":198,\"y\":1087},{\"x\":197,\"y\":1117},{\"x\":110,\"y\":1120}],\"text\":\"effect\"},{\"boundingBox\":[{\"x\":246,\"y\":1088},{\"x\":348,\"y\":1088},{\"x\":347,\"y\":1120},{\"x\":245,\"y\":1120}],\"text\":\"overlap\"},{\"boundingBox\":[{\"x\":355,\"y\":1088},{\"x\":401,\"y\":1088},{\"x\":400,\"y\":1120},{\"x\":354,\"y\":1120}],\"text\":\"the\"},{\"boundingBox\":[{\"x\":407,\"y\":1088},{\"x\":472,\"y\":1088},{\"x\":471,\"y\":1120},{\"x\":406,\"y\":1120}],\"text\":\"face\"},{\"boundingBox\":[{\"x\":534,\"y\":1084},{\"x\":631,\"y\":1086},{\"x\":630,\"y\":1119},{\"x\":534,\"y\":1116}],\"text\":\"design\"},{\"boundingBox\":[{\"x\":730,\"y\":1089},{\"x\":868,\"y\":1089},{\"x\":869,\"y\":1118},{\"x\":730,\"y\":1117}],\"text\":\"treatment\"},{\"boundingBox\":[{\"x\":977,\"y\":1088},{\"x\":1185,\"y\":1085},{\"x\":1185,\"y\":1117},{\"x\":976,\"y\":1117}],\"text\":\"transformation\"},{\"boundingBox\":[{\"x\":1246,\"y\":1089},{\"x\":1293,\"y\":1089},{\"x\":1293,\"y\":1117},{\"x\":1246,\"y\":1116}],\"text\":\"out\"},{\"boundingBox\":[{\"x\":1298,\"y\":1089},{\"x\":1318,\"y\":1088},{\"x\":1318,\"y\":1117},{\"x\":1298,\"y\":1117}],\"text\":\"a\"},{\"boundingBox\":[{\"x\":1323,\"y\":1088},{\"x\":1385,\"y\":1087},{\"x\":1384,\"y\":1116},{\"x\":1323,\"y\":1117}],\"text\":\"lens\"},{\"boundingBox\":[{\"x\":76,\"y\":1208},{\"x\":136,\"y\":1208},{\"x\":136,\"y\":1237},{\"x\":76,\"y\":1237}],\"text\":\"End\"}]}", "{\"language\":\"en\",\"text\":\"Published online: 27 November 2018\",\"lines\":[{\"boundingBox\":[{\"x\":0,\"y\":16},{\"x\":1062,\"y\":17},{\"x\":1062,\"y\":69},{\"x\":0,\"y\":69}],\"text\":\"Published online: 27 November 2018\"}],\"words\":[{\"boundingBox\":[{\"x\":1,\"y\":17},{\"x\":281,\"y\":17},{\"x\":280,\"y\":70},{\"x\":0,\"y\":70}],\"text\":\"Published\"},{\"boundingBox\":[{\"x\":291,\"y\":17},{\"x\":500,\"y\":17},{\"x\":499,\"y\":70},{\"x\":291,\"y\":70}],\"text\":\"online:\"},{\"boundingBox\":[{\"x\":510,\"y\":17},{\"x\":585,\"y\":17},{\"x\":585,\"y\":70},{\"x\":510,\"y\":70}],\"text\":\"27\"},{\"boundingBox\":[{\"x\":595,\"y\":17},{\"x\":903,\"y\":18},{\"x\":903,\"y\":70},{\"x\":595,\"y\":70}],\"text\":\"November\"},{\"boundingBox\":[{\"x\":913,\"y\":18},{\"x\":1060,\"y\":18},{\"x\":1060,\"y\":70},{\"x\":913,\"y\":70}],\"text\":\"2018\"}]}", "{\"language\":\"en\",\"text\":\"-108 PmBY 19:18 @¥( 100%* 15:48 < # AGLASSES = GLASSES Rayban xfett RB33 ... . - ...... *FEWQrb3025 112 ... ¥ 450 ¥ 560 2 3 5 6 7 8 O i - & @ .? ! ABC space Done V ¥ 1010\",\"lines\":[{\"boundingBox\":[{\"x\":465,\"y\":6},{\"x\":549,\"y\":6},{\"x\":549,\"y\":19},{\"x\":465,\"y\":19}],\"text\":\"-108 PmBY\"},{\"boundingBox\":[{\"x\":1145,\"y\":2},{\"x\":1175,\"y\":2},{\"x\":1175,\"y\":14},{\"x\":1145,\"y\":14}],\"text\":\"19:18\"},{\"boundingBox\":[{\"x\":1218,\"y\":2},{\"x\":1321,\"y\":2},{\"x\":1321,\"y\":17},{\"x\":1218,\"y\":16}],\"text\":\"@¥( 100%*\"},{\"boundingBox\":[{\"x\":1686,\"y\":2},{\"x\":1710,\"y\":2},{\"x\":1711,\"y\":13},{\"x\":1686,\"y\":12}],\"text\":\"15:48\"},{\"boundingBox\":[{\"x\":4,\"y\":23},{\"x\":25,\"y\":22},{\"x\":26,\"y\":42},{\"x\":5,\"y\":43}],\"text\":\"<\"},{\"boundingBox\":[{\"x\":92,\"y\":25},{\"x\":199,\"y\":24},{\"x\":199,\"y\":41},{\"x\":92,\"y\":41}],\"text\":\"# AGLASSES\"},{\"boundingBox\":[{\"x\":471,\"y\":31},{\"x\":492,\"y\":30},{\"x\":493,\"y\":44},{\"x\":472,\"y\":45}],\"text\":\"=\"},{\"boundingBox\":[{\"x\":586,\"y\":28},{\"x\":676,\"y\":28},{\"x\":676,\"y\":46},{\"x\":586,\"y\":46}],\"text\":\"GLASSES\"},{\"boundingBox\":[{\"x\":1588,\"y\":102},{\"x\":1719,\"y\":102},{\"x\":1719,\"y\":116},{\"x\":1588,\"y\":116}],\"text\":\"Rayban xfett RB33 ...\"},{\"boundingBox\":[{\"x\":485,\"y\":192},{\"x\":508,\"y\":191},{\"x\":508,\"y\":208},{\"x\":485,\"y\":209}],\"text\":\".\"},{\"boundingBox\":[{\"x\":1645,\"y\":191},{\"x\":1669,\"y\":192},{\"x\":1668,\"y\":210},{\"x\":1644,\"y\":209}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":124,\"y\":221},{\"x\":170,\"y\":220},{\"x\":170,\"y\":231},{\"x\":124,\"y\":231}],\"text\":\"......\"},{\"boundingBox\":[{\"x\":478,\"y\":252},{\"x\":605,\"y\":253},{\"x\":605,\"y\":266},{\"x\":478,\"y\":266}],\"text\":\"*FEWQrb3025 112 ...\"},{\"boundingBox\":[{\"x\":710,\"y\":240},{\"x\":774,\"y\":238},{\"x\":775,\"y\":260},{\"x\":710,\"y\":262}],\"text\":\"¥ 450\"},{\"boundingBox\":[{\"x\":1735,\"y\":325},{\"x\":1808,\"y\":325},{\"x\":1808,\"y\":341},{\"x\":1735,\"y\":340}],\"text\":\"¥ 560\"},{\"boundingBox\":[{\"x\":37,\"y\":374},{\"x\":52,\"y\":374},{\"x\":51,\"y\":388},{\"x\":37,\"y\":389}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":65,\"y\":375},{\"x\":78,\"y\":375},{\"x\":78,\"y\":387},{\"x\":64,\"y\":388}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":127,\"y\":375},{\"x\":138,\"y\":375},{\"x\":138,\"y\":387},{\"x\":126,\"y\":387}],\"text\":\"5\"},{\"boundingBox\":[{\"x\":156,\"y\":376},{\"x\":167,\"y\":375},{\"x\":167,\"y\":387},{\"x\":156,\"y\":388}],\"text\":\"6\"},{\"boundingBox\":[{\"x\":185,\"y\":375},{\"x\":198,\"y\":375},{\"x\":198,\"y\":387},{\"x\":185,\"y\":387}],\"text\":\"7\"},{\"boundingBox\":[{\"x\":214,\"y\":375},{\"x\":226,\"y\":375},{\"x\":226,\"y\":387},{\"x\":214,\"y\":388}],\"text\":\"8\"},{\"boundingBox\":[{\"x\":272,\"y\":389},{\"x\":273,\"y\":373},{\"x\":285,\"y\":375},{\"x\":283,\"y\":390}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":1039,\"y\":387},{\"x\":1057,\"y\":388},{\"x\":1056,\"y\":407},{\"x\":1038,\"y\":406}],\"text\":\"i\"},{\"boundingBox\":[{\"x\":1816,\"y\":387},{\"x\":1840,\"y\":386},{\"x\":1841,\"y\":397},{\"x\":1816,\"y\":398}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":211,\"y\":416},{\"x\":228,\"y\":414},{\"x\":229,\"y\":433},{\"x\":212,\"y\":435}],\"text\":\"&\"},{\"boundingBox\":[{\"x\":240,\"y\":417},{\"x\":256,\"y\":417},{\"x\":256,\"y\":433},{\"x\":240,\"y\":433}],\"text\":\"@\"},{\"boundingBox\":[{\"x\":89,\"y\":451},{\"x\":250,\"y\":449},{\"x\":250,\"y\":483},{\"x\":89,\"y\":484}],\"text\":\".? !\"},{\"boundingBox\":[{\"x\":23,\"y\":501},{\"x\":49,\"y\":501},{\"x\":49,\"y\":513},{\"x\":23,\"y\":513}],\"text\":\"ABC\"},{\"boundingBox\":[{\"x\":126,\"y\":502},{\"x\":164,\"y\":502},{\"x\":164,\"y\":514},{\"x\":126,\"y\":514}],\"text\":\"space\"},{\"boundingBox\":[{\"x\":239,\"y\":501},{\"x\":274,\"y\":500},{\"x\":273,\"y\":513},{\"x\":238,\"y\":514}],\"text\":\"Done\"},{\"boundingBox\":[{\"x\":1550,\"y\":497},{\"x\":1571,\"y\":496},{\"x\":1571,\"y\":510},{\"x\":1550,\"y\":512}],\"text\":\"V\"},{\"boundingBox\":[{\"x\":1713,\"y\":491},{\"x\":1770,\"y\":491},{\"x\":1770,\"y\":505},{\"x\":1713,\"y\":506}],\"text\":\"¥ 1010\"}],\"words\":[{\"boundingBox\":[{\"x\":465,\"y\":8},{\"x\":490,\"y\":7},{\"x\":490,\"y\":19},{\"x\":465,\"y\":19}],\"text\":\"-108\"},{\"boundingBox\":[{\"x\":492,\"y\":7},{\"x\":550,\"y\":6},{\"x\":550,\"y\":20},{\"x\":492,\"y\":19}],\"text\":\"PmBY\"},{\"boundingBox\":[{\"x\":1146,\"y\":2},{\"x\":1175,\"y\":2},{\"x\":1175,\"y\":14},{\"x\":1146,\"y\":14}],\"text\":\"19:18\"},{\"boundingBox\":[{\"x\":1219,\"y\":2},{\"x\":1258,\"y\":2},{\"x\":1258,\"y\":15},{\"x\":1219,\"y\":16}],\"text\":\"@¥(\"},{\"boundingBox\":[{\"x\":1261,\"y\":2},{\"x\":1321,\"y\":3},{\"x\":1321,\"y\":18},{\"x\":1261,\"y\":15}],\"text\":\"100%*\"},{\"boundingBox\":[{\"x\":1686,\"y\":2},{\"x\":1711,\"y\":2},{\"x\":1711,\"y\":13},{\"x\":1686,\"y\":12}],\"text\":\"15:48\"},{\"boundingBox\":[{\"x\":8,\"y\":23},{\"x\":24,\"y\":22},{\"x\":25,\"y\":42},{\"x\":9,\"y\":43}],\"text\":\"<\"},{\"boundingBox\":[{\"x\":93,\"y\":25},{\"x\":108,\"y\":25},{\"x\":108,\"y\":42},{\"x\":93,\"y\":42}],\"text\":\"#\"},{\"boundingBox\":[{\"x\":111,\"y\":25},{\"x\":199,\"y\":25},{\"x\":200,\"y\":41},{\"x\":111,\"y\":42}],\"text\":\"AGLASSES\"},{\"boundingBox\":[{\"x\":471,\"y\":31},{\"x\":486,\"y\":30},{\"x\":487,\"y\":44},{\"x\":472,\"y\":45}],\"text\":\"=\"},{\"boundingBox\":[{\"x\":587,\"y\":29},{\"x\":676,\"y\":29},{\"x\":675,\"y\":46},{\"x\":587,\"y\":47}],\"text\":\"GLASSES\"},{\"boundingBox\":[{\"x\":1589,\"y\":105},{\"x\":1635,\"y\":103},{\"x\":1634,\"y\":117},{\"x\":1588,\"y\":116}],\"text\":\"Rayban\"},{\"boundingBox\":[{\"x\":1637,\"y\":103},{\"x\":1672,\"y\":103},{\"x\":1671,\"y\":117},{\"x\":1637,\"y\":117}],\"text\":\"xfett\"},{\"boundingBox\":[{\"x\":1674,\"y\":103},{\"x\":1702,\"y\":103},{\"x\":1702,\"y\":117},{\"x\":1674,\"y\":117}],\"text\":\"RB33\"},{\"boundingBox\":[{\"x\":1704,\"y\":103},{\"x\":1719,\"y\":104},{\"x\":1719,\"y\":117},{\"x\":1704,\"y\":117}],\"text\":\"...\"},{\"boundingBox\":[{\"x\":485,\"y\":192},{\"x\":502,\"y\":191},{\"x\":503,\"y\":208},{\"x\":486,\"y\":209}],\"text\":\".\"},{\"boundingBox\":[{\"x\":1646,\"y\":191},{\"x\":1664,\"y\":192},{\"x\":1664,\"y\":210},{\"x\":1646,\"y\":209}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":125,\"y\":221},{\"x\":170,\"y\":221},{\"x\":169,\"y\":231},{\"x\":124,\"y\":231}],\"text\":\"......\"},{\"boundingBox\":[{\"x\":479,\"y\":252},{\"x\":564,\"y\":253},{\"x\":564,\"y\":266},{\"x\":479,\"y\":267}],\"text\":\"*FEWQrb3025\"},{\"boundingBox\":[{\"x\":566,\"y\":253},{\"x\":585,\"y\":254},{\"x\":585,\"y\":266},{\"x\":566,\"y\":266}],\"text\":\"112\"},{\"boundingBox\":[{\"x\":588,\"y\":254},{\"x\":605,\"y\":254},{\"x\":605,\"y\":266},{\"x\":588,\"y\":266}],\"text\":\"...\"},{\"boundingBox\":[{\"x\":710,\"y\":241},{\"x\":721,\"y\":240},{\"x\":722,\"y\":262},{\"x\":711,\"y\":263}],\"text\":\"¥\"},{\"boundingBox\":[{\"x\":726,\"y\":240},{\"x\":775,\"y\":239},{\"x\":774,\"y\":261},{\"x\":726,\"y\":262}],\"text\":\"450\"},{\"boundingBox\":[{\"x\":1765,\"y\":325},{\"x\":1774,\"y\":325},{\"x\":1774,\"y\":341},{\"x\":1765,\"y\":341}],\"text\":\"¥\"},{\"boundingBox\":[{\"x\":1777,\"y\":325},{\"x\":1808,\"y\":326},{\"x\":1808,\"y\":341},{\"x\":1777,\"y\":341}],\"text\":\"560\"},{\"boundingBox\":[{\"x\":38,\"y\":374},{\"x\":51,\"y\":374},{\"x\":52,\"y\":388},{\"x\":39,\"y\":389}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":70,\"y\":375},{\"x\":77,\"y\":375},{\"x\":78,\"y\":387},{\"x\":71,\"y\":388}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":126,\"y\":375},{\"x\":138,\"y\":375},{\"x\":138,\"y\":387},{\"x\":126,\"y\":387}],\"text\":\"5\"},{\"boundingBox\":[{\"x\":156,\"y\":376},{\"x\":166,\"y\":375},{\"x\":167,\"y\":386},{\"x\":156,\"y\":387}],\"text\":\"6\"},{\"boundingBox\":[{\"x\":187,\"y\":375},{\"x\":198,\"y\":375},{\"x\":198,\"y\":387},{\"x\":187,\"y\":387}],\"text\":\"7\"},{\"boundingBox\":[{\"x\":214,\"y\":375},{\"x\":225,\"y\":375},{\"x\":225,\"y\":388},{\"x\":214,\"y\":388}],\"text\":\"8\"},{\"boundingBox\":[{\"x\":272,\"y\":388},{\"x\":273,\"y\":376},{\"x\":285,\"y\":377},{\"x\":283,\"y\":389}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":1043,\"y\":387},{\"x\":1057,\"y\":388},{\"x\":1056,\"y\":407},{\"x\":1042,\"y\":406}],\"text\":\"i\"},{\"boundingBox\":[{\"x\":1816,\"y\":386},{\"x\":1827,\"y\":386},{\"x\":1827,\"y\":398},{\"x\":1816,\"y\":398}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":211,\"y\":416},{\"x\":227,\"y\":414},{\"x\":229,\"y\":433},{\"x\":213,\"y\":435}],\"text\":\"&\"},{\"boundingBox\":[{\"x\":242,\"y\":417},{\"x\":256,\"y\":417},{\"x\":256,\"y\":433},{\"x\":242,\"y\":433}],\"text\":\"@\"},{\"boundingBox\":[{\"x\":95,\"y\":455},{\"x\":163,\"y\":452},{\"x\":163,\"y\":481},{\"x\":95,\"y\":481}],\"text\":\".?\"},{\"boundingBox\":[{\"x\":180,\"y\":451},{\"x\":205,\"y\":451},{\"x\":205,\"y\":482},{\"x\":180,\"y\":481}],\"text\":\"!\"},{\"boundingBox\":[{\"x\":23,\"y\":501},{\"x\":49,\"y\":501},{\"x\":49,\"y\":513},{\"x\":23,\"y\":513}],\"text\":\"ABC\"},{\"boundingBox\":[{\"x\":127,\"y\":503},{\"x\":164,\"y\":503},{\"x\":164,\"y\":514},{\"x\":127,\"y\":515}],\"text\":\"space\"},{\"boundingBox\":[{\"x\":239,\"y\":501},{\"x\":273,\"y\":500},{\"x\":273,\"y\":513},{\"x\":240,\"y\":514}],\"text\":\"Done\"},{\"boundingBox\":[{\"x\":1558,\"y\":496},{\"x\":1570,\"y\":496},{\"x\":1571,\"y\":509},{\"x\":1559,\"y\":510}],\"text\":\"V\"},{\"boundingBox\":[{\"x\":1718,\"y\":491},{\"x\":1729,\"y\":491},{\"x\":1729,\"y\":506},{\"x\":1718,\"y\":506}],\"text\":\"¥\"},{\"boundingBox\":[{\"x\":1732,\"y\":491},{\"x\":1770,\"y\":491},{\"x\":1770,\"y\":506},{\"x\":1732,\"y\":506}],\"text\":\"1010\"}]}" ] }, { "@search.score": 2.0897067, "content": "\nDetecting problematic transactions \nin a consumer‑to‑consumer e‑commerce \nnetwork\nShun Kodate1,2, Ryusuke Chiba3, Shunya Kimura3 and Naoki Masuda2,4,5* \n\nIntroduction\nIn tandem with the rapid growth of online and electronic transactions and communi-\ncations, fraud is expanding at a dramatic speed and penetrates our daily lives. Fraud \nincluding cybercrimes costs billions of dollars per year and threatens the security of our \nsociety (UK Parliament 2017; McAfee 2019). In particular, in the recent era where online \nactivity dominates, attacking a system is not too costly, whereas defending the system \nagainst fraud is costly (Anderson et al. 2013). The dimension of fraud is vast and ranges \nfrom credit card fraud, money laundering, computer intrusion, to plagiarism, to name a \nfew.\n\nAbstract \n\nProviders of online marketplaces are constantly combatting against problematic \ntransactions, such as selling illegal items and posting fictive items, exercised by some \nof their users. A typical approach to detect fraud activity has been to analyze registered \nuser profiles, user’s behavior, and texts attached to individual transactions and the user. \nHowever, this traditional approach may be limited because malicious users can easily \nconceal their information. Given this background, network indices have been exploited \nfor detecting frauds in various online transaction platforms. In the present study, we \nanalyzed networks of users of an online consumer-to-consumer marketplace in which \na seller and the corresponding buyer of a transaction are connected by a directed \nedge. We constructed egocentric networks of each of several hundreds of fraudulent \nusers and those of a similar number of normal users. We calculated eight local network \nindices based on up to connectivity between the neighbors of the focal node. Based \non the present descriptive analysis of these network indices, we fed twelve features \nthat we constructed from the eight network indices to random forest classifiers with \nthe aim of distinguishing between normal users and fraudulent users engaged in each \none of the four types of problematic transactions. We found that the classifier accu-\nrately distinguished the fraudulent users from normal users and that the classification \nperformance did not depend on the type of problematic transaction.\n\nKeywords: Network analysis, Machine learning, Fraud detection, Computational social \nscience\n\nOpen Access\n\n© The Author(s) 2020. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://\ncreat iveco mmons .org/licen ses/by/4.0/.\n\nRESEARCH\n\nKodate et al. Appl Netw Sci (2020) 5:90 \nhttps://doi.org/10.1007/s41109‑020‑00330‑x Applied Network Science\n\n*Correspondence: \nnaokimas@buffalo.edu \n4 Department \nof Mathematics, University \nat Buffalo, Buffalo, NY \n14260-2900, USA\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0003-1567-801X\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1007/s41109-020-00330-x&domain=pdf\n\n\nPage 2 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\nComputational and statistical methods for detecting and preventing fraud have been \ndeveloped and implemented for decades (Bolton and Hand 2002; Phua et  al. 2010; \nAbdallah et al. 2016; West and Bhattacharya 2016). Standard practice for fraud detec-\ntion is to employ statistical methods including the case of machine learning algorithms. \nIn particular, when both fraudulent and non-fraudulent samples are available, one can \nconstruct a classifier via supervised learning (Bolton and Hand 2002; Phua et al. 2010; \nAbdallah et al. 2016; West and Bhattacharya 2016). Exemplar features to be fed to such a \nstatistical classifier include the transaction amount, day of the week, item category, and \nuser’s address for detecting frauds in credit card systems, number of calls, call duration, \ncall type, and user’s age, gender, and geographical region in the case of telecommunica-\ntion, and user profiles and transaction history in the case of online auctions (Abdallah \net al. 2016).\n\nHowever, many of these features can be easily faked by advanced fraudsters (Akoglu \net al. 2015; Google LLC 2018). Furthermore, fraudulent users are adept at escaping the \neyes of the administrators or authorities that would detect the usage of particular words \nas a signature of anomalous behavior (Pu and Webb 2006; Hayes 2007; Bhowmick and \nHazarika 2016). For example, if the authority discovers that one jargon means a drug, \nthen fraudulent users may easily switch to another jargon to confuse the authority.\n\nNetwork analysis is an alternative way to construct features and is not new to fraud \ndetection techniques (Savage et al. 2014; Akoglu et al. 2015). The idea is to use connec-\ntivity between nodes, which are usually users or goods, in the given data and calculate \ngraph-theoretic quantities or scores that characterize nodes. These methods stand on \nthe expectation that anomalous users show connectivity patterns that are distinct from \nthose of normal users (Akoglu et al. 2015). Network analysis has been deployed for fraud \ndetection in insurance (Šubelj et  al. 2011), money laundering (Dreżewski et  al. 2015; \nColladon and Remondi 2017; Savage et al. 2017), health-care data (Liu et al. 2016), car-\nbooking (Shchur et al. 2018), a social security system (Van Vlasselaer et al. 2016), mobile \nadvertising (Hu et al. 2017), a mobile phone network (Ferrara et al. 2014), online social \nnetworks (Bhat and Abulaish 2013; Jiang et  al. 2014; Hooi et  al. 2016; Rasheed et  al. \n2018), online review forums (Akoglu et al. 2013; Liu et al. 2017; Wang et al. 2018), online \nauction or marketplaces (Chau et  al. 2006; Pandit et  al. 2007; Wang and Chiu 2008; \nBangcharoensap et  al. 2015; Yanchun et  al. 2011), credit card transactions (Van Vlas-\nselaer et al. 2015; Li et al. 2017), cryptocurrency transaction (Monamo et al. 2016), and \nvarious other fields (Akoglu et al. 2010). For example, fraudulent users and their accom-\nplices were shown to form approximately bipartite cores in a network of users to inflate \ntheir reputations in an online auction system (Chau et al. 2006). Then, the authors pro-\nposed an algorithm based on a belief propagation to detect such suspicious connectivity \npatterns. This method has been proven to be also effective on empirical data obtained \nfrom eBay (Pandit et al. 2007).\n\nIn the present study, we analyze a data set obtained from a large online consumer-to-\nconsumer (C2C) marketplace, Mercari, operating in Japan and the US. They are the larg-\nest C2C marketplace in Japan, in which, as of 2019, there are 13 million monthly active \nusers and 133 billion yen (approximately 1.2 billion USD) transactions per quarter year \n(Mercari 2019). Note that we analyze transaction frauds based on transaction networks \nof users, which contrasts with previous studies of online C2C marketplaces that looked \n\n\n\nPage 3 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\nat reputation frauds (Chau et al. 2006; Pandit et al. 2007; Wang and Chiu 2008; Yanchun \net  al. 2011). Many prior network-based fraud detection algorithms used global infor-\nmation about networks, such as connected components, communities, betweenness, \nk-cores, and that determined by belief propagation (Chau et al. 2006; Pandit et al. 2007; \nWang and Chiu 2008; Šubelj et  al. 2011; Akoglu et  al. 2013; Bhat and Abulaish 2013; \nFerrara et al. 2014; Jiang et al. 2014; Bangcharoensap et al. 2015; Dreżewski et al. 2015; \nVan Vlasselaer et al. 2015; Hooi et al. 2016; Liu et al. 2016; Van Vlasselaer et al. 2016; \nColladon and Remondi 2017; Hu et al. 2017; Li et al. 2017; Liu et al. 2017; Savage et al. \n2017; Shchur et al. 2018; Rasheed et al. 2018; Wang et al. 2018). Others used local infor-\nmation about the users’ network, such as the degree, the number of triangles, and the \nlocal clustering coefficient (Chau et al. 2006; Akoglu et al. 2010; Šubelj et al. 2011; Yan-\nchun et al. 2011; Bhat and Abulaish 2013; Bangcharoensap et al. 2015; Dreżewski et al. \n2015; Monamo et al. 2016; Van Vlasselaer et al. 2016; Colladon and Remondi 2017). We \nwill focus on local features of users, i.e., features of a node that can be calculated from \nthe connectivity of the user and the connectivity between neighbors of the user. This is \nbecause local features are easier and faster to calculate and thus practical for commercial \nimplementations.\n\nMaterials and methods\nData\n\nMercari is an online C2C marketplace service, where users trade various items among \nthemselves. The service is operating in Japan and the United States. In the present study, \nwe used the data obtained from the Japanese market between July 2013 and January \n2019. In addition to normal transactions, we focused on the following types of prob-\nlematic transactions: fictive, underwear, medicine, and weapon. Fictive transactions are \ndefined as selling non-existing items. Underwear refers to transactions of used under-\nwear; they are prohibited by the service from the perspective of morality and hygiene. \nMedicine refers to transactions of medicinal supplies, which are prohibited by the law. \nWeapon refers to transactions of weapons, which are prohibited by the service because \nthey may lead to crime. The number of sampled users of each type is shown in Table 1.\n\nNetwork analysis\n\nWe examine a directed and weighted network of users in which a user corresponds to a \nnode and a transaction between two users represents a directed edge. The weight of the \nedge is equal to the number of transactions between the seller and the buyer. We con-\nstructed egocentric networks of each of several hundreds of normal users and those of \nfraudulent users, i.e., those engaged in at least one problematic sell. Figure 1 shows the \negocentric networks of two normal users (Fig. 1a, b) and those of two fraudulent users \ninvolved in selling a fictive item (Fig. 1c, d). The egocentric network of either a normal or \nfraudulent user contained the nodes neighboring the focal user, edges between the focal \nuser and these neighbors, and edges between the pairs of these neighbors.\n\nWe calculated eight indices for each focal node. They are local indices in the mean-\ning that they require the information up to the connectivity among the neighbors of the \nfocal node.\n\n\n\nPage 4 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\nFive out of the eight indices use only the information about the connectivity of the focal \nnode. The degree ki of node vi is the number of its neighbors. The node strength  (Barrat \net al. 2004) (i.e., weighted degree) of node vi , denoted by si , is the number of transactions in \nwhich vi is involved. Using these two indices, we also considered the mean number of trans-\nactions per neighbor, i.e., si/ki , as a separate index. These three indices do not use informa-\ntion about the direction of edges.\n\nThe sell probability of node vi , denoted by SPi , uses the information about the direction of \nedges and defined as the proportion of the vi ’s neighbors for which vi acts as seller. Precisely, \nthe sell probability is given by\n\n(1)SPi =\nkouti\n\nk ini + kouti\n\n,\n\nFig. 1 Examples of egocentric networks. a, b Egocentric networks of arbitrarily selected two normal users. c, \nd Egocentric networks of arbitrarily selected two fraudulent users involved in selling a fictive item\n\n\n\n\n\nPage 5 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\nwhere k ini is vi ’s in-degree (i.e., the number of neighbors from whom vi bought at least \none item) and kouti is vi ’s out-degree (i.e., the number of neighbors to whom vi sold at \nleast one item). It should be noted that, if vi acted as both seller and buyer towards vj , the \ncontribution of vj to both in- and out-degree of vi is equal to one. Therefore, k ini + kouti is \nnot equal to ki in general.\n\nThe weighted version of the sell probability, denoted by WSPi , is defined as\n\nwhere sini is node vi ’s weighted in-degree (i.e., the number of buys) and souti is vi ’s weighted \nout-degree (i.e., the number of sells).\n\nThe other three indices are based on triangles that involve the focal node. The local \nclustering coefficient Ci quantifies the abundance of undirected and unweighted triangles \naround vi (Newman 2010). It is defined as the number of undirected and unweighted trian-\ngles including vi divided by ki(ki − 1)/2 . The local clustering coefficient Ci ranges between \n0 and 1.\n\nWe hypothesized that triangles contributing to an increase in the local clustering coef-\nficient are localized around particular neighbors of node vi . Such neighbors together with vi \nmay form an overlapping set of triangles, which may be regarded as a community (Radicchi \net al. 2004; Palla et al. 2005). Therefore, our hypothesis implies that the extent to which the \nfocal node is involved in communities should be different between normal and fraudulent \nusers. To quantify this concept, we introduce the so-called triangle congregation, denoted \nby mi . It is defined as the extent to which two triangles involving vi share another node and \nis given by\n\nwhere Tri = Ciki(ki − 1)/2 is the number of triangles involving vi . Note that mi ranges \nbetween 0 and 1.\n\nFrequencies of different directed three-node subnetworks, conventionally known as net-\nwork motifs (Milo et al. 2002), may distinguish between normal and fraudulent users. In \nparticular, among triangles composed of directed edges, we hypothesized that feedforward \ntriangles (Fig. 2a) should be natural and that cyclic triangles (Fig. 2b) are not. We hypoth-\nesized so because a natural interpretation of a feedforward triangle is that a node with out-\ndegree two tends to serve as seller while that with out-degree zero tends to serve as buyer \nand there are many such nodes that use the marketplace mostly as buyer or seller but not \nboth. In contrast, an abundance of cyclic triangles may imply that relatively many users use \nthe marketplace as both buyer and seller. We used the index called the cycle probability, \ndenoted by CYPi , which is defined by\n\nwhere FFi and CYi are the numbers of feedforward triangles and cyclic triangles to which \nnode vi belongs. The definition of FFi and CYi , and hence CYPi , is valid even when the \n\n(2)WSPi =\nsouti\n\nsini + souti\n\n,\n\n(3)mi =\n(Number of pairs of triangles involving vi that share another node)\n\nTri(Tri − 1)/2\n,\n\n(4)CYPi =\nCYi\n\nFFi + CYi\n,\n\n\n\nPage 6 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\ntriangles involving vi have bidirectional edges. In the case of Fig. 2c, for example, any of \nthe three nodes contains one feedforward triangle and one cyclic triangle. The other four \ncases in which bidirectional edges are involved in triangles are shown in Fig. 2d–g. In the \ncalculation of CYPi , we ignored the weights of edges.\n\nRandom forest classifier\n\nTo classify users into normal and fraudulent users based on their local network proper-\nties, we employed a random forest classifier (Breiman 2001; Breiman et al. 1984; Hastie \net  al. 2009) implemented in scikit-learn (Pedregosa et  al. 2011). It uses an ensemble \nlearning method that combines multiple classifiers, each of which is a decision tree, \nbuilt from training data and classifies test data avoiding overfitting. We combined 300 \ndecision-tree classifiers to construct a random forest classifier. Each decision tree is con-\nstructed on the basis of training samples that are randomly subsampled with replace-\nment from the set of all the training samples. To compute the best split of each node \nin a tree, one randomly samples the candidate features from the set of all the features. \nThe probability that a test sample is positive in a tree is estimated as follows. Consider \nthe terminal node in the tree that a test sample eventually reaches. The fraction of posi-\ntive training samples at the terminal node gives the probability that the test sample is \nclassified as positive. One minus the positive probability gives the negative probability \nestimated for the same test sample. The positive or negative probability for the random \nforest classifier is obtained as the average of single-tree positive or negative probability \nover all the 300 trees. A sample is classified as positive by the random forest classifier if \nthe positive probability is larger than 0.5, otherwise classified as negative.\n\nWe split samples of each type into two sets such that 75% and 25% of the samples of \neach type are assigned to the training and test samples, respectively. There were more \n\ncyclicfeedforward feedforward: 1\ncyclic: 1\n\nfeedforward: 2\ncyclic: 0\n\nfeedforward: 3\ncyclic: 1\n\nfeedforward: 6\ncyclic: 2\n\na b c d\n\nf g\n\nfeedforward: 2\ncyclic: 0\n\ne\n\nFig. 2 Directed triangle patterns and their count. a Feedforward triangle. b Cyclic triangle. c– g Five \nthree-node patterns that contain directed triangles and reciprocal edges. The numbers shown in the figure \nrepresent the number of feedforward or cyclic triangles to which each three-node pattern contributes\n\n\n\nPage 7 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\nnormal users than any type of fraudulent user. Therefore, to balance the number of \nthe negative (i.e., normal) and positive (i.e., fraudulent) samples, we uniformly ran-\ndomly subsampled the negative samples (i.e., under-sampling) such that the number \nof the samples is the same between the normal and fraudulent types in the training \nset. Based on the training sample constructed in this manner, we built each of the 300 \ndecision trees and hence a random forest classifier. Then, we examined the classifica-\ntion performance of the random forest classifier on the set of test samples.\n\nThe true positive rate, also called the recall, is defined as the proportion of the posi-\ntive samples (i.e., fraudulent users) that the random forest classifier correctly classifies \nas positive. The false positive rate is defined as the proportion of the negative samples \n(i.e., normal users) that are incorrectly classified as positive. The precision is defined \nas the proportion of the truly positive samples among those that are classified as posi-\ntive. The true positive rate, false positive rate, and precision range between 0 and 1.\n\nWe used the following two performance measures for the random forest classifier. \nTo draw the receiver operating characteristic (ROC) curve for a random forest clas-\nsifier, one first arranges the test samples in descending order of the estimated prob-\nability that they are positive. Then, one plots each test sample, with its false positive \nrate on the horizontal axis and the true positive rate on the vertical axis. By connect-\ning the test samples in a piecewise linear manner, one obtains the ROC curve. The \nprecision–recall (PR) curve is generated by plotting the samples in the same order in \n[0, 1]2 , with the recall on the horizontal axis and the precision on the vertical axis. For \nan accurate binary classifier, both ROC and PR curves visit near (x, y) = (0, 1) . There-\nfore, we quantify the performance of the classifier by the area under the curve (AUC) \nof each curve. The AUC ranges between 0 and 1, and a large value indicates a good \nperformance of the random forest classifier.\n\nTo calculate the importance of each feature in the random forest classifier, we \nused the permutation importance (Strobl et al. 2007; Altmann et al. 2010). With this \nmethod, the importance of a feature is given by the decrease in the performance of \nthe trained classifier when the feature is randomly permuted among the test samples. \nA large value indicates that the feature considerably contributes to the performance \nof the classifier. To calculate the permutation importance, we used the AUC value of \nthe ROC curve as the performance measure of a random forest classifier. We com-\nputed the permutation importance of each feature with ten different permutations \nand adopted the average over the ten permutations as the importance of the feature.\n\nWe optimized the parameters of the random forest classifier by a grid search with \n10-fold cross-validation on the training set. For the maximum depth of each tree (i.e., \nthe max_depth parameter in scikit-learn), we explored the integers between 3 and 10. \nFor the number of candidate features for each split (i.e., max_features), we explored \nthe integers between 3 and 6. For the minimum number of samples required at termi-\nnal nodes (i.e., min_samples_leaf ), we explored 1, 3, and 5. As mentioned above, the \nnumber of trees (i.e., n_estimators) was set to 300. The seed number for the random \nnumber generator (i.e., random_state) was set to 0. For the other hyperparameters, \nwe used the default values in scikit-learn version 0.22. In the parameter optimization, \nwe evaluated the performance of the random forest classifier with the AUC value of \nthe ROC curve measured on a single set of training and test samples.\n\n\n\nPage 8 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\nTo avoid sampling bias, we built 100 random forest classifiers, trained each classifier, \nand tested its performance on a randomly drawn set of train and test samples, whose \nsampling scheme was described above.\n\nResults\nDescriptive statistics\n\nThe survival probability of the degree (i.e., a fraction of nodes whose degree is larger \nthan a specified value) is shown in Fig. 3a for each user type. Approximately 60% of the \nnormal users have degree ki = 1 , whereas the fraction of the users with ki = 1 is approxi-\nmately equal to 2% or less for any type of fraudulent user (Table 1). Therefore, we expect \nthat whether ki = 1 or ki ≥ 2 gives useful information for distinguishing between normal \nand fraudulent users. The degree distribution at ki ≥ 2 may provide further information \nuseful for the classification. The survival probability of the degree distribution condi-\ntioned on ki ≥ 2 for the different types of users is shown in Fig. 3b. The figure suggests \nthat the degree distribution is systematically different between the normal and fraudu-\nlent users. However, we consider that the difference is not as clear-cut as that in the frac-\ntion of users having ki = 1 (Table 1).\n\nThe survival probability of the node strength (i.e., weighted degree) is shown in Fig. 3c \nfor each user type. As in the case for the unweighted degree, we found that many nor-\nmal users, but not fraudulent users, have si = 1 . In fact, the number of the normal users \nwith si = 1 is equal to those with ki = 1 (Table 1), implying that all normal users with \nki = 1 participated in just one transaction. In contrast, no user had si = 1 for any type \nof fraudulent user. The survival probability of the node strength conditioned on si ≥ 2 \napparently does not show a clear distinction between the normal and fraudulent users \n(Fig. 3d, Table 1).\n\na b\n\nc d\n\nFig. 3 Survival probability of the degree for each user type. a Degree (i.e., ki ) for all nodes. b Degree for the \nnodes with ki ≥ 2 . c Strength (i.e., si ) for all nodes. d Strength for the nodes with si ≥ 2\n\n\n\nPage 9 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\nThe distribution of the average number of transactions per edge, i.e., si/ki , is shown \nin Fig. 4a. We found that a majority of normal users have si/ki = 1 . This result indicates \nthat a large fraction of normal users is engaged in just one transaction per neighbor \n(Table 1). This result is consistent with the fact that approximately 60% of the normal \nusers have ki = si = 1 . In contrast, many of any type of fraudulent users have si/ki > 1 . \nHowever, they tend to have a smaller value of si/ki than the normal users. This differ-\nence is more noticeable when we discraded the users with si/ki = 1 (Fig. 4b, Table 1). \nTherefore, less frequent transactions with a specific neighbor seem to be a characteristic \nbehavior of fraudulent users.\n\nThe distribution of the unweighted sell probability for the different user types is \nshown in Fig.  5a. The distribution for the normal users is peaked around 0 and 1, \n\nTable 1 Properties of different types of users\n\nIn the first column, Mean ( A | B ), for example, represents the mean of A conditioned on B. Unless the first column mentions \nthe conditional mean, median, or the number of transactions, the numbers reported in the table represent the number of \nusers\n\nSeed user type Normal Fictive Underwear Medicine Weapon\n\nNumber of seed users 999 440 468 469 416\n\nNumber of transactions \ninvolving the seed user\n\n151,021 66,215 151,278 92,497 81,970\n\nTotal number of transactions 27,683,860 850,739 2,325,898 925,361 533,963\n\nki = 1 587 (58.8%) 8 (1.8%) 3 (0.6%) 2 (0.4%) 5 (1.2%)\n\nMean ( ki | ki ≥ 2) 195.0 138.3 297.8 184.2 179.7\n\nMedian ( ki | ki ≥ 2) 77.5 61.0 170.0 97.0 86.0\n\nsi = 1 587 (58.8%) 8 (1.8%) 3 (0.6%) 2 (0.4%) 5 (1.2%)\n\nMean ( si | si ≥ 2) 365.1 153.3 325.3 198.1 199.4\n\nMedian ( si | si ≥ 2) 89.0 66.5 175.0 100.0 90.0\n\nsi ≥ 2 412 432 465 467 411\n\nsi/ki = 1 97 (23.5%) 97 (22.5%) 86 (18.5%) 156 (33.4%) 121 (29.4%)\n\nMean ( si/ki | si/ki > 1) 1.413 1.135 1.055 1.066 1.092\n\nMedian ( si/ki | si/ki > 1) 1.124 1.059 1.03 1.031 1.055\n\nki ≥ 2 412 432 465 467 411\n\nSPi = 1 157 (38.1%) 15 (3.5%) 21 (4.5%) 16 (3.4%) 17 (4.1%)\n\nk\nout\ni\n\n= 1 118 (28.6%) 21 (4.9%) 2 (0.4%) 2 (0.4%) 9 (2.2%)\n\nsi ≥ 2 412 432 465 467 411\n\nWSPi = 1 157 (38.1%) 15 (3.5%) 21 (4.5%) 16 (3.4%) 17 (4.1%)\n\ns\nout\ni\n\n= 1 118 (28.6%) 14 (3.2%) 2 (0.4%) 2 (0.4%) 9 (2.2%)\n\nki ≥ 2 412 432 465 467 411\n\nCi = 0 118 (28.6%) 152 (35.2%) 108 (23.2%) 154 (33.0%) 128 (31.1%)\n\nMean ( Ci | Ci > 0) 8.554× 10\n−3\n\n8.348× 10\n−3 9.500× 10\n\n−4\n2.231× 10\n\n−3\n3.810× 10\n\n−3\n\nMedian ( Ci | Ci > 0) 2.411× 10\n−3\n\n2.039× 10\n−3 5.288× 10\n\n−4\n6.494× 10\n\n−4\n1.337× 10\n\n−3\n\nTri ≥ 2 262 241 317 251 244\n\nmi = 0 17 (6.5%) 27 (11.2%) 54 (17.0%) 44 (17.5%) 32 (13.1%)\n\nmi = 1 12 (4.6%) 9 (3.7%) 4 (1.3%) 6 (2.4%) 11 (4.5%)\n\nMean ( mi | mi > 0) 8.554× 10\n−3\n\n8.348× 10\n−3 9.500× 10\n\n−4\n2.231× 10\n\n−3\n3.810× 10\n\n−3\n\nMedian ( mi | mi > 0) 2.411× 10\n−3\n\n2.039× 10\n−3 5.288× 10\n\n−4\n6.494× 10\n\n−4\n1.337× 10\n\n−3\n\nFFi + CYi ≥ 1 294 280 357 313 283\n\nCYPi = 0 234 (79.6%) 188 (67.1%) 222 (62.2%) 227 (72.5%) 202 (71.4%)\n\nMean ( CYPi | CYPi > 0) 1.987× 10\n−2\n\n7.367× 10\n−2\n\n6.739× 10\n−2\n\n8.551× 10\n−2\n\n5.544× 10\n−2\n\nMedian ( CYPi | CYPi > 0) 1.521× 10\n−2\n\n4.481× 10\n−2\n\n3.396× 10\n−2\n\n3.822× 10\n−2\n\n3.618× 10\n−2\n\n\n\nPage 10 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\nindicating that a relatively large fraction of normal users is almost exclusive buyer or \nseller. Note that, by definition, the sell probability is at least 1/(k ini + kouti ) because our \nsamples are sellers. Therefore, a peak around the sell probability of zero implies that \nthe users probably have no or few sell transactions apart from the one sell transaction \nbased on which the users have been sampled as seller. In contrast, the distribution \nfor any fraudulent type is relatively flat. Figure  5b shows the relationships between \nthe unweighted sell probability and the degree. On the dashed line in Fig. 5b, the sell \nprobability is equal to 1/(k ini + kouti ) , indicating that the node has kouti = 1 , which is \nthe smallest possible out-degree. The users on this line were buyers in all but one \n\na b\n\nFig. 4 Survival probability of the average number of transactions per neighbor. a si/ki for all nodes. b si/ki for \nthe nodes with si/ki > 1\n\na b\n\nc d\n\nFig. 5 Sell probability for each user type. a Distribution of the unweighted sell probability. b Relationship \nbetween the degree and the unweighted sell probability. c Distribution of the weighted sell probability. d \nRelationship between the node strength and the weighted sell probability. The dashed lines in b, d indicate \n1/(k in\n\ni\n+ k\n\nout\ni\n\n) and 1/(sin\ni\n+ s\n\nout\ni\n\n) , respectively\n\n\n\nPage 11 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\ntransaction. Figure 5b indicates that a majority of such users are normal as opposed \nto fraudulent users, which is quantitatively confirmed in Table 1. We also found that \nmost of the normal users were either on the horizontal line with the sell probability \nof one (38.1% of the normal users with ki ≥ 2 ; see Table 1 for the corresponding frac-\ntions of normal users with ki = 1 ) or on the dashed line (28.6%). This is not the case \nfor any type of fraudulent user (Table 1).\n\nThe distribution of the weighted sell probability for the different user types and the \nrelationships between the weighted sell probability and the node strength are shown \nin Fig.  5c, d, respectively. The results are similar to the case of the unweighted sell \nprobability in two aspects. First, the normal users and the fraudulent users form dis-\ntinct frequency distributions (Fig. 5c). Second, most of the normal users are either on \nthe horizontal line with the weighted sell probability of one or on the dashed line with \nthe smallest possible weighted sell probability, i.e., 1/si (Fig. 5d, Table 1).\n\nThe survival probability of the local clustering coefficient is shown in Fig.  6a. It \nshould be noted that, in this analysis, we confined ourselves to the users with ki ≥ 2 \nbecause Ci is undefined when ki = 1 . We found that the number of users with Ci = 0 is \nnot considerably different between the normal and fraudulent users (also see Table 1). \nFigure  6b shows the survival probability of Ci conditioned on Ci > 0 . The normal \nusers tend to have a larger value of Ci than fraudulent users, whereas this tendency is \nnot strong (Table 1).\n\nThe survival probability of the triangle congregation is shown in Fig. 7a. Contrary to \nour hypothesis, there is no clear difference between the distribution of the normal and \nfraudulent users. The triangle congregation tends to be large when the node strength \nis small (Fig. 7b) and the local clustering coefficient is large (Fig. 7d). It depends little \non the weighted sell probability (Fig. 7c). However, we did not find clear differences in \nthe triangle congregation between the normal and fraudulent users (also see Table 1).\n\nThe survival probability of the cycle probability is shown in Fig. 8a. A large fraction \nof any type of users has CYPi = 0 (Table 1). When the users with CYPi = 0 are dis-\ncarded, the normal users tend to have a smaller value of CYPi than any type of fraudu-\nlent users (Fig. 8b, Table 1).\n\na b\n\nFig. 6 Local clustering coefficient for each user type. a Survival probability. b Survival probability conditioned \non Ci > 0\n\n\n\nPage 12 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\nClassification of users\n\nBased on the eight indices whose descriptive statistics were analyzed in the previ-\nous section, we defined 12 features and fed them to the random forest classifier. The \naim of the classifier is to distinguish between normal and fraudulent users. The first \nfeature is binary and whether the degree ki = 1 or ki ≥ 2 . The second feature is also \nbinary and whether the node strength si = 1 or si ≥ 2 . The third feature is si/ki , which \nis a real number greater than or equal to 1. The fourth feature is binary and whether the \nunweighted sell probability SPi = 1 or SPi < 1 . The fifth feature is binary and whether \n\na b\n\nc d\n\nFig. 7 Triangle congregation for each user type. a Survival probability. b Relationship between the triangle \ncongregation, mi , and the node strength. c Relationship between mi and the weighted sell probability. d \nRelationship between mi and the local clustering coefficient\n\na b\n\nFig. 8 Cycle probability for each user type. a Survival probability. b Survival probability conditioned on \nCYPi > 0\n\n\n\nPage 13 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\nSPi = 1/(k ini + kouti ) or SPi > 1/(k ini + kouti ) , i.e., whether kouti = 1 or kouti > 1 . The sixth \nfeature is SPi , which ranges between 0 and 1. The seventh feature is binary and whether \nthe weighted sell probability WSPi = 1 or WSPi < 1 . The eighth feature is binary and \nwhether WS", "metadata_storage_path": "aHR0cHM6Ly9zdG9yYWdlYWNjb3VudGxpZ2lyay5ibG9iLmNvcmUud2luZG93cy5uZXQvcGFwZXIvczQxMTA5LTAyMC0wMDMzMC14LnBkZg2", "metadata_author": " Shun Kodate ", "metadata_title": "Detecting problematic transactions in a consumer-to-consumer e-commerce network", "keyphrases": [ "Creative Commons Attribution 4.0 International License", "Computational social science Open Access", "other third party material", "eight local network indices", "various online transaction platforms", "Applied Network Science", "Creative Commons licence", "random forest classifiers", "Appl Netw Sci", "eight network indices", "present descriptive analysis", "credit card fraud", "Network analysis", "present study", "problematic transaction", "appropriate credit", "credit line", "online marketplaces", "Shun Kodate", "Ryusuke Chiba", "Shunya Kimura3", "Naoki Masuda", "rapid growth", "electronic transactions", "communi- cations", "dramatic speed", "daily lives", "UK Parliament", "recent era", "money laundering", "computer intrusion", "illegal items", "fictive items", "typical approach", "individual transactions", "traditional approach", "corresponding buyer", "several hundreds", "similar number", "focal node", "twelve features", "four types", "classification performance", "Machine learning", "author(s", "statutory regulation", "copyright holder", "iveco mmons", "RESEARCH Kodate", "Full list", "malicious users", "fraudulent users", "normal users", "online consumer", "intended use", "permitted use", "doi.org", "orcid.org", "Fraud detection", "consumer marketplace", "egocentric networks", "author information", "user profiles", "fraud activity", "Introduction", "tandem", "cybercrimes", "billions", "dollars", "year", "security", "society", "McAfee", "system", "Anderson", "dimension", "ranges", "plagiarism", "Abstract", "Providers", "behavior", "texts", "background", "frauds", "seller", "edge", "up", "connectivity", "neighbors", "aim", "Keywords", "article", "sharing", "adaptation", "distribution", "reproduction", "medium", "original", "source", "link", "changes", "images", "permission", "Correspondence", "naokimas", "buffalo", "4 Department", "Mathematics", "University", "USA", "creativecommons", "licenses", "crossmark", "dialog", "Page", "18Kodate", "13 million monthly active users", "credit card systems", "social security system", "various other fields", "online review forums", "large online consumer", "machine learning algorithms", "online social networks", "mobile phone network", "credit card transactions", "suspicious connectivity patterns", "online auction system", "fraud detec- tion", "fraud detection techniques", "online C2C marketplaces", "online auctions", "supervised learning", "telecommunica- tion", "transaction networks", "C2C) marketplace", "Standard practice", "transaction amount", "item category", "call duration", "call type", "geographical region", "transaction history", "advanced fraudsters", "Google LLC", "particular words", "anomalous behavior", "alternative way", "graph-theoretic quantities", "Dreżewski", "car- booking", "Van Vlasselaer", "cryptocurrency transaction", "accom- plices", "bipartite cores", "belief propagation", "133 billion yen", "1.2 billion USD", "quarter year", "previous studies", "anomalous users", "fraudulent samples", "health-care data", "empirical data", "data set", "statistical methods", "transaction frauds", "reputation frauds", "statistical classifier", "one jargon", "Exemplar features", "Computational", "decades", "Bolton", "Hand", "Phua", "Abdallah", "West", "Bhattacharya", "case", "day", "week", "address", "number", "calls", "age", "gender", "Akoglu", "eyes", "administrators", "authorities", "signature", "Webb", "Hayes", "Bhowmick", "Hazarika", "example", "authority", "drug", "idea", "nodes", "goods", "scores", "expectation", "insurance", "Šubelj", "Colladon", "Remondi", "Liu", "Shchur", "advertising", "Ferrara", "Abulaish", "Jiang", "Hooi", "Rasheed", "Wang", "Chau", "Pandit", "Chiu", "Bangcharoensap", "Yanchun", "Monamo", "reputations", "authors", "eBay", "Mercari", "Japan", "Many prior network-based fraud detection algorithms", "online C2C marketplace service", "one problematic sell", "local clustering coefficient", "local infor- mation", "two fraudulent users", "two normal users", "two indices", "sell probability", "local indices", "two users", "local features", "connected components", "Yan- chun", "commercial implementations", "various items", "United States", "Japanese market", "following types", "non-existing items", "medicinal supplies", "weighted network", "egocentric network", "eight indices", "trans- actions", "separate index", "three indices", "informa- tion", "users’ network", "fictive item", "node vi", "node strength", "methods Data", "normal transactions", "lematic transactions", "mean number", "focal user", "Fictive transactions", "directed edge", "networks", "global", "communities", "betweenness", "k-cores", "Bhat", "Savage", "Others", "degree", "triangles", "Materials", "July", "January", "addition", "underwear", "medicine", "weapon", "perspective", "morality", "hygiene", "law", "crime", "Table", "buyer", "Figure", "1a", "Fig.", "edges", "pairs", "information", "Barrat", "direction", "SPi", "local clustering coefficient Ci ranges", "Random forest classifier", "unweighted trian- gles", "other three indices", "one feedforward triangle", "one cyclic triangle", "one item", "triangle congregation", "other four", "k ini", "04 Q OS", "weighted version", "overlapping set", "three-node subnetworks", "work motifs", "natural interpretation", "three nodes", "unweighted triangles", "many users", "two triangles", "feedforward triangles", "cyclic triangles", "different directed", "particular neighbors", "Such neighbors", "bidirectional edges", "Fig. 2c", "degree zero", "proportion", "kouti", "Examples", "vj", "contribution", "ki", "sini", "buys", "souti", "sells", "abundance", "undirected", "Newman", "increase", "community", "Radicchi", "Palla", "hypothesis", "extent", "concept", "mi", "Note", "Frequencies", "marketplace", "contrast", "index", "cycle", "CYPi", "CYi", "definition", "calculation", "weights", "local network proper- ties", "random forest clas- sifier", "precision–recall (PR) curve", "random forest classifier", "ensemble learning method", "classifica- tion performance", "receiver operating characteristic", "two performance measures", "true positive rate", "false positive rate", "piecewise linear manner", "Directed triangle patterns", "same test sample", "tive training samples", "ROC) curve", "ROC curve", "two sets", "three-node patterns", "directed triangles", "same order", "single-tree positive", "Cyclic triangle", "precision range", "tive samples", "positive probability", "test data", "multiple classifiers", "decision-tree classifiers", "best split", "reciprocal edges", "fraudulent user", "fraudulent types", "descending order", "horizontal axis", "vertical axis", "test samples", "training data", "fraudulent) samples", "terminal node", "negative probability", "candidate features", "Feedforward triangle", "decision trees", "300 trees", "Breiman", "Hastie", "scikit-learn", "Pedregosa", "overfitting", "basis", "replace", "ment", "fraction", "average", "count", "Five", "numbers", "figure", "The", "2", "100 random forest classifiers", "accurate binary classifier", "random number generator", "ten different permutations", "ten permutations", "different types", "PR curves", "grid search", "10-fold cross-validation", "maximum depth", "max_depth parameter", "other hyperparameters", "default values", "parameter optimization", "sampling bias", "sampling scheme", "Descriptive statistics", "survival probability", "frac- tion", "one transaction", "clear distinction", "large value", "The AUC", "lent users", "mal users", "minimum number", "seed number", "single set", "degree distribution", "unweighted degree", "AUC value", "permutation importance", "nal nodes", "scikit-learn version", "Fig. 3a", "useful information", "Fig. 3c", "user type", "training set", "performance measure", "degree ki", "normal", "recall", "precision", "area", "good", "Strobl", "Altmann", "method", "decrease", "tree", "integers", "split", "max_features", "min_samples", "n_estimators", "Results", "specified", "classification", "difference", "many", "fact", "Normal Fictive Underwear Medicine Weapon", "unweighted sell probability", "less frequent transactions", "different user types", "one sell transaction", "Seed user type", "Survival probability", "c Strength", "d Strength", "large fraction", "smaller value", "characteristic behavior", "first column", "exclusive buyer", "seed users", "sell transactions", "Fig. 4a", "Fig. 4b", "Fig.  5a", "specific neighbor", "average number", "Total number", "Table 1 Properties", "conditional mean", "majority", "result", "ence", "median", "FFi", "samples", "peak", "smallest possible weighted sell probability", "smallest possible out-degree", "corresponding frac- tions", "tinct frequency distributions", "cycle probability", "Figure  5b", "dashed lines", "Figure 5b", "two aspects", "Figure  6b", "larger value", "clear difference", "descriptive statistics", "ous section", "horizontal line", "fraudulent type", "Fig.  6a", "Fig. 7a", "Fig. 8a", "relationships", "ini", "buyers", "one", "transactions", "neighbor", "si/ki", "results", "analysis", "tendency", "Classification", "12 features", "unweighted sell probability SPi", "Fig. 8 Cycle probability", "Fig. 7 Triangle congregation", "sell probability WSPi", "real number", "second feature", "third feature", "fourth feature", "fifth feature", "sixth feature", "seventh feature", "eighth feature", "c Relationship" ], "merged_content": "\nDetecting problematic transactions \nin a consumer‑to‑consumer e‑commerce \nnetwork\nShun Kodate1,2, Ryusuke Chiba3, Shunya Kimura3 and Naoki Masuda2,4,5* \n\nIntroduction\nIn tandem with the rapid growth of online and electronic transactions and communi-\ncations, fraud is expanding at a dramatic speed and penetrates our daily lives. Fraud \nincluding cybercrimes costs billions of dollars per year and threatens the security of our \nsociety (UK Parliament 2017; McAfee 2019). In particular, in the recent era where online \nactivity dominates, attacking a system is not too costly, whereas defending the system \nagainst fraud is costly (Anderson et al. 2013). The dimension of fraud is vast and ranges \nfrom credit card fraud, money laundering, computer intrusion, to plagiarism, to name a \nfew.\n\nAbstract \n\nProviders of online marketplaces are constantly combatting against problematic \ntransactions, such as selling illegal items and posting fictive items, exercised by some \nof their users. A typical approach to detect fraud activity has been to analyze registered \nuser profiles, user’s behavior, and texts attached to individual transactions and the user. \nHowever, this traditional approach may be limited because malicious users can easily \nconceal their information. Given this background, network indices have been exploited \nfor detecting frauds in various online transaction platforms. In the present study, we \nanalyzed networks of users of an online consumer-to-consumer marketplace in which \na seller and the corresponding buyer of a transaction are connected by a directed \nedge. We constructed egocentric networks of each of several hundreds of fraudulent \nusers and those of a similar number of normal users. We calculated eight local network \nindices based on up to connectivity between the neighbors of the focal node. Based \non the present descriptive analysis of these network indices, we fed twelve features \nthat we constructed from the eight network indices to random forest classifiers with \nthe aim of distinguishing between normal users and fraudulent users engaged in each \none of the four types of problematic transactions. We found that the classifier accu-\nrately distinguished the fraudulent users from normal users and that the classification \nperformance did not depend on the type of problematic transaction.\n\nKeywords: Network analysis, Machine learning, Fraud detection, Computational social \nscience\n\nOpen Access\n\n© The Author(s) 2020. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://\ncreat iveco mmons .org/licen ses/by/4.0/.\n\nRESEARCH\n\nKodate et al. Appl Netw Sci (2020) 5:90 \nhttps://doi.org/10.1007/s41109‑020‑00330‑x Applied Network Science\n\n*Correspondence: \nnaokimas@buffalo.edu \n4 Department \nof Mathematics, University \nat Buffalo, Buffalo, NY \n14260-2900, USA\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0003-1567-801X\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1007/s41109-020-00330-x&domain=pdf\n\n\nPage 2 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\nComputational and statistical methods for detecting and preventing fraud have been \ndeveloped and implemented for decades (Bolton and Hand 2002; Phua et  al. 2010; \nAbdallah et al. 2016; West and Bhattacharya 2016). Standard practice for fraud detec-\ntion is to employ statistical methods including the case of machine learning algorithms. \nIn particular, when both fraudulent and non-fraudulent samples are available, one can \nconstruct a classifier via supervised learning (Bolton and Hand 2002; Phua et al. 2010; \nAbdallah et al. 2016; West and Bhattacharya 2016). Exemplar features to be fed to such a \nstatistical classifier include the transaction amount, day of the week, item category, and \nuser’s address for detecting frauds in credit card systems, number of calls, call duration, \ncall type, and user’s age, gender, and geographical region in the case of telecommunica-\ntion, and user profiles and transaction history in the case of online auctions (Abdallah \net al. 2016).\n\nHowever, many of these features can be easily faked by advanced fraudsters (Akoglu \net al. 2015; Google LLC 2018). Furthermore, fraudulent users are adept at escaping the \neyes of the administrators or authorities that would detect the usage of particular words \nas a signature of anomalous behavior (Pu and Webb 2006; Hayes 2007; Bhowmick and \nHazarika 2016). For example, if the authority discovers that one jargon means a drug, \nthen fraudulent users may easily switch to another jargon to confuse the authority.\n\nNetwork analysis is an alternative way to construct features and is not new to fraud \ndetection techniques (Savage et al. 2014; Akoglu et al. 2015). The idea is to use connec-\ntivity between nodes, which are usually users or goods, in the given data and calculate \ngraph-theoretic quantities or scores that characterize nodes. These methods stand on \nthe expectation that anomalous users show connectivity patterns that are distinct from \nthose of normal users (Akoglu et al. 2015). Network analysis has been deployed for fraud \ndetection in insurance (Šubelj et  al. 2011), money laundering (Dreżewski et  al. 2015; \nColladon and Remondi 2017; Savage et al. 2017), health-care data (Liu et al. 2016), car-\nbooking (Shchur et al. 2018), a social security system (Van Vlasselaer et al. 2016), mobile \nadvertising (Hu et al. 2017), a mobile phone network (Ferrara et al. 2014), online social \nnetworks (Bhat and Abulaish 2013; Jiang et  al. 2014; Hooi et  al. 2016; Rasheed et  al. \n2018), online review forums (Akoglu et al. 2013; Liu et al. 2017; Wang et al. 2018), online \nauction or marketplaces (Chau et  al. 2006; Pandit et  al. 2007; Wang and Chiu 2008; \nBangcharoensap et  al. 2015; Yanchun et  al. 2011), credit card transactions (Van Vlas-\nselaer et al. 2015; Li et al. 2017), cryptocurrency transaction (Monamo et al. 2016), and \nvarious other fields (Akoglu et al. 2010). For example, fraudulent users and their accom-\nplices were shown to form approximately bipartite cores in a network of users to inflate \ntheir reputations in an online auction system (Chau et al. 2006). Then, the authors pro-\nposed an algorithm based on a belief propagation to detect such suspicious connectivity \npatterns. This method has been proven to be also effective on empirical data obtained \nfrom eBay (Pandit et al. 2007).\n\nIn the present study, we analyze a data set obtained from a large online consumer-to-\nconsumer (C2C) marketplace, Mercari, operating in Japan and the US. They are the larg-\nest C2C marketplace in Japan, in which, as of 2019, there are 13 million monthly active \nusers and 133 billion yen (approximately 1.2 billion USD) transactions per quarter year \n(Mercari 2019). Note that we analyze transaction frauds based on transaction networks \nof users, which contrasts with previous studies of online C2C marketplaces that looked \n\n\n\nPage 3 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\nat reputation frauds (Chau et al. 2006; Pandit et al. 2007; Wang and Chiu 2008; Yanchun \net  al. 2011). Many prior network-based fraud detection algorithms used global infor-\nmation about networks, such as connected components, communities, betweenness, \nk-cores, and that determined by belief propagation (Chau et al. 2006; Pandit et al. 2007; \nWang and Chiu 2008; Šubelj et  al. 2011; Akoglu et  al. 2013; Bhat and Abulaish 2013; \nFerrara et al. 2014; Jiang et al. 2014; Bangcharoensap et al. 2015; Dreżewski et al. 2015; \nVan Vlasselaer et al. 2015; Hooi et al. 2016; Liu et al. 2016; Van Vlasselaer et al. 2016; \nColladon and Remondi 2017; Hu et al. 2017; Li et al. 2017; Liu et al. 2017; Savage et al. \n2017; Shchur et al. 2018; Rasheed et al. 2018; Wang et al. 2018). Others used local infor-\nmation about the users’ network, such as the degree, the number of triangles, and the \nlocal clustering coefficient (Chau et al. 2006; Akoglu et al. 2010; Šubelj et al. 2011; Yan-\nchun et al. 2011; Bhat and Abulaish 2013; Bangcharoensap et al. 2015; Dreżewski et al. \n2015; Monamo et al. 2016; Van Vlasselaer et al. 2016; Colladon and Remondi 2017). We \nwill focus on local features of users, i.e., features of a node that can be calculated from \nthe connectivity of the user and the connectivity between neighbors of the user. This is \nbecause local features are easier and faster to calculate and thus practical for commercial \nimplementations.\n\nMaterials and methods\nData\n\nMercari is an online C2C marketplace service, where users trade various items among \nthemselves. The service is operating in Japan and the United States. In the present study, \nwe used the data obtained from the Japanese market between July 2013 and January \n2019. In addition to normal transactions, we focused on the following types of prob-\nlematic transactions: fictive, underwear, medicine, and weapon. Fictive transactions are \ndefined as selling non-existing items. Underwear refers to transactions of used under-\nwear; they are prohibited by the service from the perspective of morality and hygiene. \nMedicine refers to transactions of medicinal supplies, which are prohibited by the law. \nWeapon refers to transactions of weapons, which are prohibited by the service because \nthey may lead to crime. The number of sampled users of each type is shown in Table 1.\n\nNetwork analysis\n\nWe examine a directed and weighted network of users in which a user corresponds to a \nnode and a transaction between two users represents a directed edge. The weight of the \nedge is equal to the number of transactions between the seller and the buyer. We con-\nstructed egocentric networks of each of several hundreds of normal users and those of \nfraudulent users, i.e., those engaged in at least one problematic sell. Figure 1 shows the \negocentric networks of two normal users (Fig. 1a, b) and those of two fraudulent users \ninvolved in selling a fictive item (Fig. 1c, d). The egocentric network of either a normal or \nfraudulent user contained the nodes neighboring the focal user, edges between the focal \nuser and these neighbors, and edges between the pairs of these neighbors.\n\nWe calculated eight indices for each focal node. They are local indices in the mean-\ning that they require the information up to the connectivity among the neighbors of the \nfocal node.\n\n\n\nPage 4 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\nFive out of the eight indices use only the information about the connectivity of the focal \nnode. The degree ki of node vi is the number of its neighbors. The node strength  (Barrat \net al. 2004) (i.e., weighted degree) of node vi , denoted by si , is the number of transactions in \nwhich vi is involved. Using these two indices, we also considered the mean number of trans-\nactions per neighbor, i.e., si/ki , as a separate index. These three indices do not use informa-\ntion about the direction of edges.\n\nThe sell probability of node vi , denoted by SPi , uses the information about the direction of \nedges and defined as the proportion of the vi ’s neighbors for which vi acts as seller. Precisely, \nthe sell probability is given by\n\n(1)SPi =\nkouti\n\nk ini + kouti\n\n,\n\nFig. 1 Examples of egocentric networks. a, b Egocentric networks of arbitrarily selected two normal users. c, \nd Egocentric networks of arbitrarily selected two fraudulent users involved in selling a fictive item\n\n C a O d OF b 04 O 0+ 0 04 OF 04 04 Q OS OF O OF OF O 04 O 04 0 04 04 04 -O O FO \n\n\n\nPage 5 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\nwhere k ini is vi ’s in-degree (i.e., the number of neighbors from whom vi bought at least \none item) and kouti is vi ’s out-degree (i.e., the number of neighbors to whom vi sold at \nleast one item). It should be noted that, if vi acted as both seller and buyer towards vj , the \ncontribution of vj to both in- and out-degree of vi is equal to one. Therefore, k ini + kouti is \nnot equal to ki in general.\n\nThe weighted version of the sell probability, denoted by WSPi , is defined as\n\nwhere sini is node vi ’s weighted in-degree (i.e., the number of buys) and souti is vi ’s weighted \nout-degree (i.e., the number of sells).\n\nThe other three indices are based on triangles that involve the focal node. The local \nclustering coefficient Ci quantifies the abundance of undirected and unweighted triangles \naround vi (Newman 2010). It is defined as the number of undirected and unweighted trian-\ngles including vi divided by ki(ki − 1)/2 . The local clustering coefficient Ci ranges between \n0 and 1.\n\nWe hypothesized that triangles contributing to an increase in the local clustering coef-\nficient are localized around particular neighbors of node vi . Such neighbors together with vi \nmay form an overlapping set of triangles, which may be regarded as a community (Radicchi \net al. 2004; Palla et al. 2005). Therefore, our hypothesis implies that the extent to which the \nfocal node is involved in communities should be different between normal and fraudulent \nusers. To quantify this concept, we introduce the so-called triangle congregation, denoted \nby mi . It is defined as the extent to which two triangles involving vi share another node and \nis given by\n\nwhere Tri = Ciki(ki − 1)/2 is the number of triangles involving vi . Note that mi ranges \nbetween 0 and 1.\n\nFrequencies of different directed three-node subnetworks, conventionally known as net-\nwork motifs (Milo et al. 2002), may distinguish between normal and fraudulent users. In \nparticular, among triangles composed of directed edges, we hypothesized that feedforward \ntriangles (Fig. 2a) should be natural and that cyclic triangles (Fig. 2b) are not. We hypoth-\nesized so because a natural interpretation of a feedforward triangle is that a node with out-\ndegree two tends to serve as seller while that with out-degree zero tends to serve as buyer \nand there are many such nodes that use the marketplace mostly as buyer or seller but not \nboth. In contrast, an abundance of cyclic triangles may imply that relatively many users use \nthe marketplace as both buyer and seller. We used the index called the cycle probability, \ndenoted by CYPi , which is defined by\n\nwhere FFi and CYi are the numbers of feedforward triangles and cyclic triangles to which \nnode vi belongs. The definition of FFi and CYi , and hence CYPi , is valid even when the \n\n(2)WSPi =\nsouti\n\nsini + souti\n\n,\n\n(3)mi =\n(Number of pairs of triangles involving vi that share another node)\n\nTri(Tri − 1)/2\n,\n\n(4)CYPi =\nCYi\n\nFFi + CYi\n,\n\n\n\nPage 6 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\ntriangles involving vi have bidirectional edges. In the case of Fig. 2c, for example, any of \nthe three nodes contains one feedforward triangle and one cyclic triangle. The other four \ncases in which bidirectional edges are involved in triangles are shown in Fig. 2d–g. In the \ncalculation of CYPi , we ignored the weights of edges.\n\nRandom forest classifier\n\nTo classify users into normal and fraudulent users based on their local network proper-\nties, we employed a random forest classifier (Breiman 2001; Breiman et al. 1984; Hastie \net  al. 2009) implemented in scikit-learn (Pedregosa et  al. 2011). It uses an ensemble \nlearning method that combines multiple classifiers, each of which is a decision tree, \nbuilt from training data and classifies test data avoiding overfitting. We combined 300 \ndecision-tree classifiers to construct a random forest classifier. Each decision tree is con-\nstructed on the basis of training samples that are randomly subsampled with replace-\nment from the set of all the training samples. To compute the best split of each node \nin a tree, one randomly samples the candidate features from the set of all the features. \nThe probability that a test sample is positive in a tree is estimated as follows. Consider \nthe terminal node in the tree that a test sample eventually reaches. The fraction of posi-\ntive training samples at the terminal node gives the probability that the test sample is \nclassified as positive. One minus the positive probability gives the negative probability \nestimated for the same test sample. The positive or negative probability for the random \nforest classifier is obtained as the average of single-tree positive or negative probability \nover all the 300 trees. A sample is classified as positive by the random forest classifier if \nthe positive probability is larger than 0.5, otherwise classified as negative.\n\nWe split samples of each type into two sets such that 75% and 25% of the samples of \neach type are assigned to the training and test samples, respectively. There were more \n\ncyclicfeedforward feedforward: 1\ncyclic: 1\n\nfeedforward: 2\ncyclic: 0\n\nfeedforward: 3\ncyclic: 1\n\nfeedforward: 6\ncyclic: 2\n\na b c d\n\nf g\n\nfeedforward: 2\ncyclic: 0\n\ne\n\nFig. 2 Directed triangle patterns and their count. a Feedforward triangle. b Cyclic triangle. c– g Five \nthree-node patterns that contain directed triangles and reciprocal edges. The numbers shown in the figure \nrepresent the number of feedforward or cyclic triangles to which each three-node pattern contributes\n\n\n\nPage 7 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\nnormal users than any type of fraudulent user. Therefore, to balance the number of \nthe negative (i.e., normal) and positive (i.e., fraudulent) samples, we uniformly ran-\ndomly subsampled the negative samples (i.e., under-sampling) such that the number \nof the samples is the same between the normal and fraudulent types in the training \nset. Based on the training sample constructed in this manner, we built each of the 300 \ndecision trees and hence a random forest classifier. Then, we examined the classifica-\ntion performance of the random forest classifier on the set of test samples.\n\nThe true positive rate, also called the recall, is defined as the proportion of the posi-\ntive samples (i.e., fraudulent users) that the random forest classifier correctly classifies \nas positive. The false positive rate is defined as the proportion of the negative samples \n(i.e., normal users) that are incorrectly classified as positive. The precision is defined \nas the proportion of the truly positive samples among those that are classified as posi-\ntive. The true positive rate, false positive rate, and precision range between 0 and 1.\n\nWe used the following two performance measures for the random forest classifier. \nTo draw the receiver operating characteristic (ROC) curve for a random forest clas-\nsifier, one first arranges the test samples in descending order of the estimated prob-\nability that they are positive. Then, one plots each test sample, with its false positive \nrate on the horizontal axis and the true positive rate on the vertical axis. By connect-\ning the test samples in a piecewise linear manner, one obtains the ROC curve. The \nprecision–recall (PR) curve is generated by plotting the samples in the same order in \n[0, 1]2 , with the recall on the horizontal axis and the precision on the vertical axis. For \nan accurate binary classifier, both ROC and PR curves visit near (x, y) = (0, 1) . There-\nfore, we quantify the performance of the classifier by the area under the curve (AUC) \nof each curve. The AUC ranges between 0 and 1, and a large value indicates a good \nperformance of the random forest classifier.\n\nTo calculate the importance of each feature in the random forest classifier, we \nused the permutation importance (Strobl et al. 2007; Altmann et al. 2010). With this \nmethod, the importance of a feature is given by the decrease in the performance of \nthe trained classifier when the feature is randomly permuted among the test samples. \nA large value indicates that the feature considerably contributes to the performance \nof the classifier. To calculate the permutation importance, we used the AUC value of \nthe ROC curve as the performance measure of a random forest classifier. We com-\nputed the permutation importance of each feature with ten different permutations \nand adopted the average over the ten permutations as the importance of the feature.\n\nWe optimized the parameters of the random forest classifier by a grid search with \n10-fold cross-validation on the training set. For the maximum depth of each tree (i.e., \nthe max_depth parameter in scikit-learn), we explored the integers between 3 and 10. \nFor the number of candidate features for each split (i.e., max_features), we explored \nthe integers between 3 and 6. For the minimum number of samples required at termi-\nnal nodes (i.e., min_samples_leaf ), we explored 1, 3, and 5. As mentioned above, the \nnumber of trees (i.e., n_estimators) was set to 300. The seed number for the random \nnumber generator (i.e., random_state) was set to 0. For the other hyperparameters, \nwe used the default values in scikit-learn version 0.22. In the parameter optimization, \nwe evaluated the performance of the random forest classifier with the AUC value of \nthe ROC curve measured on a single set of training and test samples.\n\n\n\nPage 8 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\nTo avoid sampling bias, we built 100 random forest classifiers, trained each classifier, \nand tested its performance on a randomly drawn set of train and test samples, whose \nsampling scheme was described above.\n\nResults\nDescriptive statistics\n\nThe survival probability of the degree (i.e., a fraction of nodes whose degree is larger \nthan a specified value) is shown in Fig. 3a for each user type. Approximately 60% of the \nnormal users have degree ki = 1 , whereas the fraction of the users with ki = 1 is approxi-\nmately equal to 2% or less for any type of fraudulent user (Table 1). Therefore, we expect \nthat whether ki = 1 or ki ≥ 2 gives useful information for distinguishing between normal \nand fraudulent users. The degree distribution at ki ≥ 2 may provide further information \nuseful for the classification. The survival probability of the degree distribution condi-\ntioned on ki ≥ 2 for the different types of users is shown in Fig. 3b. The figure suggests \nthat the degree distribution is systematically different between the normal and fraudu-\nlent users. However, we consider that the difference is not as clear-cut as that in the frac-\ntion of users having ki = 1 (Table 1).\n\nThe survival probability of the node strength (i.e., weighted degree) is shown in Fig. 3c \nfor each user type. As in the case for the unweighted degree, we found that many nor-\nmal users, but not fraudulent users, have si = 1 . In fact, the number of the normal users \nwith si = 1 is equal to those with ki = 1 (Table 1), implying that all normal users with \nki = 1 participated in just one transaction. In contrast, no user had si = 1 for any type \nof fraudulent user. The survival probability of the node strength conditioned on si ≥ 2 \napparently does not show a clear distinction between the normal and fraudulent users \n(Fig. 3d, Table 1).\n\na b\n\nc d\n\nFig. 3 Survival probability of the degree for each user type. a Degree (i.e., ki ) for all nodes. b Degree for the \nnodes with ki ≥ 2 . c Strength (i.e., si ) for all nodes. d Strength for the nodes with si ≥ 2\n\n\n\nPage 9 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\nThe distribution of the average number of transactions per edge, i.e., si/ki , is shown \nin Fig. 4a. We found that a majority of normal users have si/ki = 1 . This result indicates \nthat a large fraction of normal users is engaged in just one transaction per neighbor \n(Table 1). This result is consistent with the fact that approximately 60% of the normal \nusers have ki = si = 1 . In contrast, many of any type of fraudulent users have si/ki > 1 . \nHowever, they tend to have a smaller value of si/ki than the normal users. This differ-\nence is more noticeable when we discraded the users with si/ki = 1 (Fig. 4b, Table 1). \nTherefore, less frequent transactions with a specific neighbor seem to be a characteristic \nbehavior of fraudulent users.\n\nThe distribution of the unweighted sell probability for the different user types is \nshown in Fig.  5a. The distribution for the normal users is peaked around 0 and 1, \n\nTable 1 Properties of different types of users\n\nIn the first column, Mean ( A | B ), for example, represents the mean of A conditioned on B. Unless the first column mentions \nthe conditional mean, median, or the number of transactions, the numbers reported in the table represent the number of \nusers\n\nSeed user type Normal Fictive Underwear Medicine Weapon\n\nNumber of seed users 999 440 468 469 416\n\nNumber of transactions \ninvolving the seed user\n\n151,021 66,215 151,278 92,497 81,970\n\nTotal number of transactions 27,683,860 850,739 2,325,898 925,361 533,963\n\nki = 1 587 (58.8%) 8 (1.8%) 3 (0.6%) 2 (0.4%) 5 (1.2%)\n\nMean ( ki | ki ≥ 2) 195.0 138.3 297.8 184.2 179.7\n\nMedian ( ki | ki ≥ 2) 77.5 61.0 170.0 97.0 86.0\n\nsi = 1 587 (58.8%) 8 (1.8%) 3 (0.6%) 2 (0.4%) 5 (1.2%)\n\nMean ( si | si ≥ 2) 365.1 153.3 325.3 198.1 199.4\n\nMedian ( si | si ≥ 2) 89.0 66.5 175.0 100.0 90.0\n\nsi ≥ 2 412 432 465 467 411\n\nsi/ki = 1 97 (23.5%) 97 (22.5%) 86 (18.5%) 156 (33.4%) 121 (29.4%)\n\nMean ( si/ki | si/ki > 1) 1.413 1.135 1.055 1.066 1.092\n\nMedian ( si/ki | si/ki > 1) 1.124 1.059 1.03 1.031 1.055\n\nki ≥ 2 412 432 465 467 411\n\nSPi = 1 157 (38.1%) 15 (3.5%) 21 (4.5%) 16 (3.4%) 17 (4.1%)\n\nk\nout\ni\n\n= 1 118 (28.6%) 21 (4.9%) 2 (0.4%) 2 (0.4%) 9 (2.2%)\n\nsi ≥ 2 412 432 465 467 411\n\nWSPi = 1 157 (38.1%) 15 (3.5%) 21 (4.5%) 16 (3.4%) 17 (4.1%)\n\ns\nout\ni\n\n= 1 118 (28.6%) 14 (3.2%) 2 (0.4%) 2 (0.4%) 9 (2.2%)\n\nki ≥ 2 412 432 465 467 411\n\nCi = 0 118 (28.6%) 152 (35.2%) 108 (23.2%) 154 (33.0%) 128 (31.1%)\n\nMean ( Ci | Ci > 0) 8.554× 10\n−3\n\n8.348× 10\n−3 9.500× 10\n\n−4\n2.231× 10\n\n−3\n3.810× 10\n\n−3\n\nMedian ( Ci | Ci > 0) 2.411× 10\n−3\n\n2.039× 10\n−3 5.288× 10\n\n−4\n6.494× 10\n\n−4\n1.337× 10\n\n−3\n\nTri ≥ 2 262 241 317 251 244\n\nmi = 0 17 (6.5%) 27 (11.2%) 54 (17.0%) 44 (17.5%) 32 (13.1%)\n\nmi = 1 12 (4.6%) 9 (3.7%) 4 (1.3%) 6 (2.4%) 11 (4.5%)\n\nMean ( mi | mi > 0) 8.554× 10\n−3\n\n8.348× 10\n−3 9.500× 10\n\n−4\n2.231× 10\n\n−3\n3.810× 10\n\n−3\n\nMedian ( mi | mi > 0) 2.411× 10\n−3\n\n2.039× 10\n−3 5.288× 10\n\n−4\n6.494× 10\n\n−4\n1.337× 10\n\n−3\n\nFFi + CYi ≥ 1 294 280 357 313 283\n\nCYPi = 0 234 (79.6%) 188 (67.1%) 222 (62.2%) 227 (72.5%) 202 (71.4%)\n\nMean ( CYPi | CYPi > 0) 1.987× 10\n−2\n\n7.367× 10\n−2\n\n6.739× 10\n−2\n\n8.551× 10\n−2\n\n5.544× 10\n−2\n\nMedian ( CYPi | CYPi > 0) 1.521× 10\n−2\n\n4.481× 10\n−2\n\n3.396× 10\n−2\n\n3.822× 10\n−2\n\n3.618× 10\n−2\n\n\n\nPage 10 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\nindicating that a relatively large fraction of normal users is almost exclusive buyer or \nseller. Note that, by definition, the sell probability is at least 1/(k ini + kouti ) because our \nsamples are sellers. Therefore, a peak around the sell probability of zero implies that \nthe users probably have no or few sell transactions apart from the one sell transaction \nbased on which the users have been sampled as seller. In contrast, the distribution \nfor any fraudulent type is relatively flat. Figure  5b shows the relationships between \nthe unweighted sell probability and the degree. On the dashed line in Fig. 5b, the sell \nprobability is equal to 1/(k ini + kouti ) , indicating that the node has kouti = 1 , which is \nthe smallest possible out-degree. The users on this line were buyers in all but one \n\na b\n\nFig. 4 Survival probability of the average number of transactions per neighbor. a si/ki for all nodes. b si/ki for \nthe nodes with si/ki > 1\n\na b\n\nc d\n\nFig. 5 Sell probability for each user type. a Distribution of the unweighted sell probability. b Relationship \nbetween the degree and the unweighted sell probability. c Distribution of the weighted sell probability. d \nRelationship between the node strength and the weighted sell probability. The dashed lines in b, d indicate \n1/(k in\n\ni\n+ k\n\nout\ni\n\n) and 1/(sin\ni\n+ s\n\nout\ni\n\n) , respectively\n\n\n\nPage 11 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\ntransaction. Figure 5b indicates that a majority of such users are normal as opposed \nto fraudulent users, which is quantitatively confirmed in Table 1. We also found that \nmost of the normal users were either on the horizontal line with the sell probability \nof one (38.1% of the normal users with ki ≥ 2 ; see Table 1 for the corresponding frac-\ntions of normal users with ki = 1 ) or on the dashed line (28.6%). This is not the case \nfor any type of fraudulent user (Table 1).\n\nThe distribution of the weighted sell probability for the different user types and the \nrelationships between the weighted sell probability and the node strength are shown \nin Fig.  5c, d, respectively. The results are similar to the case of the unweighted sell \nprobability in two aspects. First, the normal users and the fraudulent users form dis-\ntinct frequency distributions (Fig. 5c). Second, most of the normal users are either on \nthe horizontal line with the weighted sell probability of one or on the dashed line with \nthe smallest possible weighted sell probability, i.e., 1/si (Fig. 5d, Table 1).\n\nThe survival probability of the local clustering coefficient is shown in Fig.  6a. It \nshould be noted that, in this analysis, we confined ourselves to the users with ki ≥ 2 \nbecause Ci is undefined when ki = 1 . We found that the number of users with Ci = 0 is \nnot considerably different between the normal and fraudulent users (also see Table 1). \nFigure  6b shows the survival probability of Ci conditioned on Ci > 0 . The normal \nusers tend to have a larger value of Ci than fraudulent users, whereas this tendency is \nnot strong (Table 1).\n\nThe survival probability of the triangle congregation is shown in Fig. 7a. Contrary to \nour hypothesis, there is no clear difference between the distribution of the normal and \nfraudulent users. The triangle congregation tends to be large when the node strength \nis small (Fig. 7b) and the local clustering coefficient is large (Fig. 7d). It depends little \non the weighted sell probability (Fig. 7c). However, we did not find clear differences in \nthe triangle congregation between the normal and fraudulent users (also see Table 1).\n\nThe survival probability of the cycle probability is shown in Fig. 8a. A large fraction \nof any type of users has CYPi = 0 (Table 1). When the users with CYPi = 0 are dis-\ncarded, the normal users tend to have a smaller value of CYPi than any type of fraudu-\nlent users (Fig. 8b, Table 1).\n\na b\n\nFig. 6 Local clustering coefficient for each user type. a Survival probability. b Survival probability conditioned \non Ci > 0\n\n\n\nPage 12 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\nClassification of users\n\nBased on the eight indices whose descriptive statistics were analyzed in the previ-\nous section, we defined 12 features and fed them to the random forest classifier. The \naim of the classifier is to distinguish between normal and fraudulent users. The first \nfeature is binary and whether the degree ki = 1 or ki ≥ 2 . The second feature is also \nbinary and whether the node strength si = 1 or si ≥ 2 . The third feature is si/ki , which \nis a real number greater than or equal to 1. The fourth feature is binary and whether the \nunweighted sell probability SPi = 1 or SPi < 1 . The fifth feature is binary and whether \n\na b\n\nc d\n\nFig. 7 Triangle congregation for each user type. a Survival probability. b Relationship between the triangle \ncongregation, mi , and the node strength. c Relationship between mi and the weighted sell probability. d \nRelationship between mi and the local clustering coefficient\n\na b\n\nFig. 8 Cycle probability for each user type. a Survival probability. b Survival probability conditioned on \nCYPi > 0\n\n\n\nPage 13 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\nSPi = 1/(k ini + kouti ) or SPi > 1/(k ini + kouti ) , i.e., whether kouti = 1 or kouti > 1 . The sixth \nfeature is SPi , which ranges between 0 and 1. The seventh feature is binary and whether \nthe weighted sell probability WSPi = 1 or WSPi < 1 . The eighth feature is binary and \nwhether WS", "text": [ "C a O d OF b 04 O 0+ 0 04 OF 04 04 Q OS OF O OF OF O 04 O 04 0 04 04 04 -O O FO", "Published online: 16 November 2020" ], "layoutText": [ "{\"language\":\"en\",\"text\":\"C a O d OF b 04 O 0+ 0 04 OF 04 04 Q OS OF O OF OF O 04 O 04 0 04 04 04 -O O FO\",\"lines\":[{\"boundingBox\":[{\"x\":42,\"y\":934},{\"x\":70,\"y\":933},{\"x\":69,\"y\":962},{\"x\":42,\"y\":963}],\"text\":\"C\"},{\"boundingBox\":[{\"x\":38,\"y\":25},{\"x\":69,\"y\":25},{\"x\":69,\"y\":56},{\"x\":38,\"y\":56}],\"text\":\"a\"},{\"boundingBox\":[{\"x\":359,\"y\":1162},{\"x\":391,\"y\":1156},{\"x\":395,\"y\":1186},{\"x\":363,\"y\":1191}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":750,\"y\":931},{\"x\":779,\"y\":927},{\"x\":780,\"y\":961},{\"x\":751,\"y\":964}],\"text\":\"d\"},{\"boundingBox\":[{\"x\":781,\"y\":1390},{\"x\":874,\"y\":1354},{\"x\":882,\"y\":1379},{\"x\":789,\"y\":1417}],\"text\":\"OF\"},{\"boundingBox\":[{\"x\":748,\"y\":19},{\"x\":782,\"y\":19},{\"x\":781,\"y\":55},{\"x\":748,\"y\":54}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":858,\"y\":1538},{\"x\":890,\"y\":1513},{\"x\":906,\"y\":1532},{\"x\":874,\"y\":1557}],\"text\":\"04\"},{\"boundingBox\":[{\"x\":842,\"y\":1153},{\"x\":877,\"y\":1159},{\"x\":874,\"y\":1179},{\"x\":839,\"y\":1173}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":903,\"y\":1582},{\"x\":978,\"y\":1448},{\"x\":1002,\"y\":1461},{\"x\":926,\"y\":1595}],\"text\":\"0+ 0 04\"},{\"boundingBox\":[{\"x\":995,\"y\":1541},{\"x\":1007,\"y\":1497},{\"x\":1034,\"y\":1504},{\"x\":1022,\"y\":1548}],\"text\":\"OF\"},{\"boundingBox\":[{\"x\":909,\"y\":539},{\"x\":947,\"y\":516},{\"x\":958,\"y\":533},{\"x\":920,\"y\":556}],\"text\":\"04\"},{\"boundingBox\":[{\"x\":914,\"y\":430},{\"x\":960,\"y\":419},{\"x\":966,\"y\":443},{\"x\":919,\"y\":453}],\"text\":\"04\"},{\"boundingBox\":[{\"x\":1001,\"y\":1139},{\"x\":1040,\"y\":1152},{\"x\":1033,\"y\":1177},{\"x\":994,\"y\":1164}],\"text\":\"Q\"},{\"boundingBox\":[{\"x\":997,\"y\":612},{\"x\":1037,\"y\":514},{\"x\":1060,\"y\":522},{\"x\":1019,\"y\":617}],\"text\":\"OS OF\"},{\"boundingBox\":[{\"x\":1141,\"y\":1639},{\"x\":1138,\"y\":1604},{\"x\":1160,\"y\":1604},{\"x\":1162,\"y\":1639}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":1063,\"y\":667},{\"x\":1062,\"y\":609},{\"x\":1092,\"y\":607},{\"x\":1090,\"y\":667}],\"text\":\"OF\"},{\"boundingBox\":[{\"x\":1056,\"y\":621},{\"x\":1059,\"y\":573},{\"x\":1083,\"y\":574},{\"x\":1081,\"y\":623}],\"text\":\"OF\"},{\"boundingBox\":[{\"x\":1157,\"y\":1584},{\"x\":1153,\"y\":1544},{\"x\":1179,\"y\":1542},{\"x\":1182,\"y\":1582}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":1199,\"y\":1625},{\"x\":1191,\"y\":1585},{\"x\":1210,\"y\":1582},{\"x\":1218,\"y\":1622}],\"text\":\"04\"},{\"boundingBox\":[{\"x\":1213,\"y\":1566},{\"x\":1203,\"y\":1534},{\"x\":1220,\"y\":1531},{\"x\":1230,\"y\":1563}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":1167,\"y\":640},{\"x\":1154,\"y\":547},{\"x\":1179,\"y\":544},{\"x\":1193,\"y\":636}],\"text\":\"04 0\"},{\"boundingBox\":[{\"x\":1219,\"y\":627},{\"x\":1169,\"y\":486},{\"x\":1194,\"y\":479},{\"x\":1244,\"y\":622}],\"text\":\"04 04 04\"},{\"boundingBox\":[{\"x\":1271,\"y\":1173},{\"x\":1293,\"y\":1145},{\"x\":1310,\"y\":1157},{\"x\":1288,\"y\":1185}],\"text\":\"-O\"},{\"boundingBox\":[{\"x\":1268,\"y\":593},{\"x\":1253,\"y\":563},{\"x\":1271,\"y\":557},{\"x\":1286,\"y\":588}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":1292,\"y\":380},{\"x\":1321,\"y\":386},{\"x\":1319,\"y\":400},{\"x\":1290,\"y\":394}],\"text\":\"FO\"}],\"words\":[{\"boundingBox\":[{\"x\":42,\"y\":934},{\"x\":67,\"y\":933},{\"x\":68,\"y\":961},{\"x\":42,\"y\":962}],\"text\":\"C\"},{\"boundingBox\":[{\"x\":38,\"y\":25},{\"x\":69,\"y\":25},{\"x\":69,\"y\":56},{\"x\":38,\"y\":56}],\"text\":\"a\"},{\"boundingBox\":[{\"x\":362,\"y\":1161},{\"x\":389,\"y\":1156},{\"x\":394,\"y\":1185},{\"x\":367,\"y\":1190}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":750,\"y\":930},{\"x\":778,\"y\":927},{\"x\":781,\"y\":960},{\"x\":751,\"y\":964}],\"text\":\"d\"},{\"boundingBox\":[{\"x\":790,\"y\":1387},{\"x\":830,\"y\":1371},{\"x\":837,\"y\":1399},{\"x\":797,\"y\":1414}],\"text\":\"OF\"},{\"boundingBox\":[{\"x\":748,\"y\":19},{\"x\":782,\"y\":19},{\"x\":781,\"y\":55},{\"x\":748,\"y\":54}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":864,\"y\":1532},{\"x\":890,\"y\":1513},{\"x\":905,\"y\":1532},{\"x\":879,\"y\":1552}],\"text\":\"04\"},{\"boundingBox\":[{\"x\":842,\"y\":1153},{\"x\":859,\"y\":1156},{\"x\":856,\"y\":1175},{\"x\":839,\"y\":1173}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":907,\"y\":1577},{\"x\":927,\"y\":1539},{\"x\":952,\"y\":1552},{\"x\":931,\"y\":1589}],\"text\":\"0+\"},{\"boundingBox\":[{\"x\":933,\"y\":1527},{\"x\":946,\"y\":1504},{\"x\":972,\"y\":1516},{\"x\":959,\"y\":1539}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":961,\"y\":1478},{\"x\":979,\"y\":1449},{\"x\":1002,\"y\":1461},{\"x\":986,\"y\":1490}],\"text\":\"04\"},{\"boundingBox\":[{\"x\":997,\"y\":1533},{\"x\":1006,\"y\":1498},{\"x\":1033,\"y\":1506},{\"x\":1024,\"y\":1540}],\"text\":\"OF\"},{\"boundingBox\":[{\"x\":914,\"y\":535},{\"x\":946,\"y\":516},{\"x\":957,\"y\":533},{\"x\":925,\"y\":553}],\"text\":\"04\"},{\"boundingBox\":[{\"x\":921,\"y\":428},{\"x\":960,\"y\":419},{\"x\":966,\"y\":442},{\"x\":927,\"y\":451}],\"text\":\"04\"},{\"boundingBox\":[{\"x\":1004,\"y\":1139},{\"x\":1028,\"y\":1147},{\"x\":1020,\"y\":1172},{\"x\":996,\"y\":1164}],\"text\":\"Q\"},{\"boundingBox\":[{\"x\":1002,\"y\":601},{\"x\":1017,\"y\":561},{\"x\":1041,\"y\":566},{\"x\":1027,\"y\":605}],\"text\":\"OS\"},{\"boundingBox\":[{\"x\":1021,\"y\":552},{\"x\":1037,\"y\":514},{\"x\":1060,\"y\":521},{\"x\":1045,\"y\":558}],\"text\":\"OF\"},{\"boundingBox\":[{\"x\":1140,\"y\":1639},{\"x\":1138,\"y\":1617},{\"x\":1160,\"y\":1616},{\"x\":1162,\"y\":1638}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":1062,\"y\":661},{\"x\":1062,\"y\":613},{\"x\":1092,\"y\":613},{\"x\":1092,\"y\":661}],\"text\":\"OF\"},{\"boundingBox\":[{\"x\":1056,\"y\":607},{\"x\":1058,\"y\":574},{\"x\":1083,\"y\":575},{\"x\":1081,\"y\":608}],\"text\":\"OF\"},{\"boundingBox\":[{\"x\":1155,\"y\":1579},{\"x\":1153,\"y\":1553},{\"x\":1179,\"y\":1551},{\"x\":1181,\"y\":1577}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":1199,\"y\":1625},{\"x\":1191,\"y\":1586},{\"x\":1210,\"y\":1583},{\"x\":1217,\"y\":1621}],\"text\":\"04\"},{\"boundingBox\":[{\"x\":1211,\"y\":1564},{\"x\":1206,\"y\":1547},{\"x\":1223,\"y\":1541},{\"x\":1229,\"y\":1559}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":1166,\"y\":630},{\"x\":1160,\"y\":589},{\"x\":1186,\"y\":585},{\"x\":1193,\"y\":626}],\"text\":\"04\"},{\"boundingBox\":[{\"x\":1157,\"y\":565},{\"x\":1155,\"y\":547},{\"x\":1179,\"y\":545},{\"x\":1182,\"y\":561}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":1215,\"y\":615},{\"x\":1205,\"y\":576},{\"x\":1229,\"y\":571},{\"x\":1240,\"y\":612}],\"text\":\"04\"},{\"boundingBox\":[{\"x\":1202,\"y\":567},{\"x\":1188,\"y\":527},{\"x\":1210,\"y\":522},{\"x\":1226,\"y\":563}],\"text\":\"04\"},{\"boundingBox\":[{\"x\":1185,\"y\":519},{\"x\":1170,\"y\":486},{\"x\":1190,\"y\":481},{\"x\":1206,\"y\":514}],\"text\":\"04\"},{\"boundingBox\":[{\"x\":1271,\"y\":1173},{\"x\":1292,\"y\":1146},{\"x\":1308,\"y\":1158},{\"x\":1287,\"y\":1186}],\"text\":\"-O\"},{\"boundingBox\":[{\"x\":1267,\"y\":593},{\"x\":1258,\"y\":576},{\"x\":1275,\"y\":567},{\"x\":1284,\"y\":585}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":1292,\"y\":380},{\"x\":1319,\"y\":385},{\"x\":1317,\"y\":399},{\"x\":1290,\"y\":394}],\"text\":\"FO\"}]}", "{\"language\":\"en\",\"text\":\"Published online: 16 November 2020\",\"lines\":[{\"boundingBox\":[{\"x\":0,\"y\":16},{\"x\":1060,\"y\":17},{\"x\":1060,\"y\":69},{\"x\":0,\"y\":69}],\"text\":\"Published online: 16 November 2020\"}],\"words\":[{\"boundingBox\":[{\"x\":1,\"y\":17},{\"x\":283,\"y\":17},{\"x\":282,\"y\":70},{\"x\":0,\"y\":70}],\"text\":\"Published\"},{\"boundingBox\":[{\"x\":293,\"y\":17},{\"x\":503,\"y\":17},{\"x\":503,\"y\":70},{\"x\":293,\"y\":70}],\"text\":\"online:\"},{\"boundingBox\":[{\"x\":513,\"y\":17},{\"x\":589,\"y\":17},{\"x\":589,\"y\":70},{\"x\":513,\"y\":70}],\"text\":\"16\"},{\"boundingBox\":[{\"x\":599,\"y\":17},{\"x\":906,\"y\":17},{\"x\":905,\"y\":70},{\"x\":599,\"y\":70}],\"text\":\"November\"},{\"boundingBox\":[{\"x\":916,\"y\":17},{\"x\":1061,\"y\":18},{\"x\":1060,\"y\":70},{\"x\":916,\"y\":70}],\"text\":\"2020\"}]}" ] }, { "@search.score": 2.0799417, "content": "\nPrivacy preservation techniques in big \ndata analytics: a survey\nP. Ram Mohan Rao1,4*, S. Murali Krishna2 and A. P. Siva Kumar3\n\nIntroduction\nThere is an exponential growth in volume and variety of data as due to diverse applica-\ntions of computers in all domain areas. The growth has been achieved due to afford-\nable availability of computer technology, storage, and network connectivity. The large \nscale data, which also include person specific private and sensitive data like gender, zip \ncode, disease, caste, shopping cart, religion etc. is being stored in public domain. The \ndata holder can release this data to a third party data analyst to gain deeper insights and \nidentify hidden patterns which are useful in making important decisions that may help \nin improving businesses, provide value added services to customers [1], prediction, fore-\ncasting and recommendation [2]. One of the prominent applications of data analytics is \nrecommendation systems which is widely used by ecommerce sites like Amazon, Flip \nkart for suggesting products to customers based on their buying habits. Face book does \nsuggest friends, places to visit and even movie recommendation based on our interest. \nHowever releasing user activity data may lead inference attacks like identifying gender \nbased on user activity [3]. We have studied a number of privacy preserving techniques \nwhich are being employed to protect against privacy threats. Each of these techniques \nhas their own merits and demerits. This paper explores the merits and demerits of each \n\nAbstract \n\nIncredible amounts of data is being generated by various organizations like hospitals, \nbanks, e-commerce, retail and supply chain, etc. by virtue of digital technology. Not \nonly humans but machines also contribute to data in the form of closed circuit televi-\nsion streaming, web site logs, etc. Tons of data is generated every minute by social \nmedia and smart phones. The voluminous data generated from the various sources \ncan be processed and analyzed to support decision making. However data analytics \nis prone to privacy violations. One of the applications of data analytics is recommen-\ndation systems which is widely used by ecommerce sites like Amazon, Flip kart for \nsuggesting products to customers based on their buying habits leading to inference \nattacks. Although data analytics is useful in decision making, it will lead to serious \nprivacy concerns. Hence privacy preserving data analytics became very important. This \npaper examines various privacy threats, privacy preservation techniques and models \nwith their limitations, also proposes a data lake based modernistic privacy preservation \ntechnique to handle privacy preservation in unstructured data.\n\nKeywords: Data, Data analytics, Privacy threats, Privacy preservation\n\nOpen Access\n\n© The Author(s) 2018. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nSURVEY PAPER\n\nRam Mohan Rao et al. J Big Data (2018) 5:33 \nhttps://doi.org/10.1186/s40537-018-0141-8\n\n*Correspondence: \nrammohan04@gmail.com \n1 Department of Computer \nScience and Engineering, \nMLR Institute of Technology, \nHyderabad, India\nFull list of author information \nis available at the end of the \narticle\n\n\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-018-0141-8&domain=pdf\n\n\nPage 2 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\nof these techniques and also describes the research challenges in the area of privacy \npreservation. Always there exists a trade off between data utility and privacy. This paper \nalso proposes a data lake based modernistic privacy preservation technique to handle \nprivacy preservation in unstructured data with maximum data utility.\n\nPrivacy threats in data analytics\nPrivacy is the ability of an individual to determine what data can be shared, and employ \naccess control. If the data is in public domain then it is a threat to individual privacy \nas the data is held by data holder. Data holder can be social networking application, \nwebsites, mobile apps, ecommerce site, banks, hospitals etc. It is the responsibility of \nthe data holder to ensure privacy of the users data. Apart from the data held in public \ndomain, knowing or unknowingly users themself contribute to data leakage. For exam-\nple most of the mobile apps, seek access to our contacts, files, camera etc. and without \nreading the privacy statement we agree for all terms and conditions, there by contribut-\ning to data leakage.\n\nHence there is a need to educate the smart phone users regarding privacy and privacy \nthreats. Some of the key privacy threats include (1) Surveillance; (2) Disclosure; (3) Dis-\ncrimination; (4) Personal embracement and abuse.\n\nSurveillance\n\nMany organizations including retail, e-commerce, etc. study their customers buying \nhabits and try to come up with various offers and value added services [4]. Based on the \nopinion data and sentiment analysis, social media sites does provide recommendations \nof the new friends, places to visit, people to follow etc. This is possible only when they \ncontinuously monitor their customer’s transactions. This is a serious privacy threat as no \nindividual accepts surveillance.\n\nDisclosure\n\nConsider a hospital holding patient’s data which include (Zip, gender, age, disease) [5–7]. \nThe data holder has released data to a third party for analysis by anonymizing sensitive \nperson specific data so that the person cannot be identified. The third party data analyst \ncan map this information with the freely available external data sources like census data \nand can identify person suffering with some disorder. This is how private information of \na person can be disclosed which is considered to be a serious privacy breach.\n\nDiscrimination\n\nDiscrimination is the bias or inequality which can happen when some private informa-\ntion of a person is disclosed. For instance, statistical analysis of electoral results proved \nthat people of one community were completely against the party, which formed the gov-\nernment. Now the government can neglect that community or can have bias over them.\n\nPersonal embracement and abuse\n\nWhenever some private information of a person is disclosed, it can even lead to per-\nsonal embracement or abuse. For example, a person was privately undergoing medica-\ntion for some specific problem and was buying some medicines on a regular basis from a \n\n\n\nPage 3 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\nmedical shop. As part of their regular business model, the medical shop may send some \nreminder and offers related to these medicines over phone. If any family member has \nnoticed this, it will lead to personal embracement and even abuse [8].\n\nData analytics activity will affect data Privacy. Many countries are enforcing Privacy \npreservation laws. Lack of awareness is also a major reason for privacy attacks. For \nexample many smart phones users are not aware of the information that is stolen from \ntheir phones by many apps. Previous research shows only 17% of smart phone users are \naware of privacy threats [9].\n\nPrivacy preservation methods\nMany Privacy preserving techniques were developed, but most of them are based on \nanonymization of data. The list of privacy preservation techniques is given below.\n\n • K anonymity\n • L diversity\n • T closeness\n • Randomization\n • Data distribution\n • Cryptographic techniques\n • Multidimensional Sensitivity Based Anonymization (MDSBA).\n\nK anonymity [10]\n\nAnonymization is the process of modifying data before it is given for data analytics [11], \nso that de identification is not possible and will lead to K indistinguishable records if \nan attempt is made to de identify by mapping the anonymized data with external data \nsources. K anonymity is prone to two attacks namely homogeneity attack and back \nground knowledge attack. Some of the algorithms applied include, Incognito [12], Mon-\ndrian [13] to ensure Anonymization. K anonymity is applied on the patient data shown \nin Table 1. The table shows data before anonymization.\n\nK anonymity algorithm is applied with k value as 3 to ensure 3 indistinguishable \nrecords when an attempt is made to identify a particular person’s data. K anonymity is \napplied on the two attributes viz. Zip and age shown in Table 1. The result of applying \nanonymization on Zip and age attributes is shown in Table 2.\n\nTable 1 Patient data, before anonymization\n\nSno Zip Age Disease\n\n1 57677 29 Cardiac problem\n\n2 57602 22 Cardiac problem\n\n3 57678 27 Cardiac problem\n\n4 57905 43 Skin allergy\n\n5 57909 52 Cardiac problem\n\n6 57906 47 Cancer\n\n7 57605 30 Cardiac problem\n\n8 57673 36 Cancer\n\n9 57607 32 Cancer\n\n\n\nPage 4 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\nThe above technique has used generalization [14] to achieve Anonymization. Suppose \nif we know that John is 27 year old and lives in 57677 zip codes then we can conclude \nJohn to have Cardiac problem even after anonymization as shown in Table  2. This is \ncalled Homogeneity attack. For example if John is 36 year old and it is known that John \ndoes not have cancer, then definitely John must have Cardiac problem. This is called as \nbackground knowledge attack. Achieving K anonymity [15, 16] can be done either by \nusing generalization or suppression. K anonymity can optimized if the minimal gener-\nalization can be done without huge data loss [17]. Identity disclosure is the major pri-\nvacy threat which cannot be guaranteed by K anonymity [18]. Personalized privacy is the \nmost important aspect of individual privacy [19].\n\nL diversity\n\nTo address homogeneity attack, another technique called L diversity has been proposed. \nAs per L diversity there must be L well represented values for the sensitive attribute (dis-\nease) in each equivalence class.\n\nImplementing L diversity is not possible every time because of the variety of data. L \ndiversity is also prone to skewness attack. When overall distribution of data is skewed \ninto few equivalence classes attribute disclosure cannot be ensured. For example if the \nentire records are distributed into only three equivalence classes then semantic close-\nness of these values may lead to attribute disclosure. Also L diversity may lead to simi-\nlarity attack. From Table 3 it can be noticed that if we know that John is 27 year old and \nlives in 57677 zip, then definitely John is under low income group because salaries of all \n\nTable 2 After applying anonymization on Zip and age\n\nSno Zip Age Disease\n\n1 576** 2* Cardiac problem\n\n2 576** 2* Cardiac problem\n\n3 576** 2* Cardiac problem\n\n4 5790* > 40 Skin allergy\n\n5 5790* > 40 Cardiac problem\n\n6 5790* > 40 Cancer\n\n7 576** 3* Cardiac problem\n\n8 576** 3* Cancer\n\n9 576** 3* Cancer\n\nTable 3 L diversity privacy preservation technique\n\nSno Zip Age Salary Disease\n\n1 576** 2* 5k Cardiac problem\n\n2 576** 2* 6k Cardiac problem\n\n3 576** 2* 7k Cardiac problem\n\n4 5790* > 40 20k Skin allergy\n\n5 5790* > 40 22k Cardiac problem\n\n6 5790* > 40 24k Cancer\n\n\n\nPage 5 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\nthree persons in 576** zip is low compare to others in the table. This is called as similar-\nity attack.\n\nT closeness\n\nAnother improvement to L diversity is T closeness measure where an equivalence class \nis considered to have ‘T closeness’ if the distance between the distributions of sensi-\ntive attribute in the class is no more than a threshold and all equivalence classes have T \ncloseness [20]. T closeness can be calculated on every attribute with respect to sensitive \nattribute.\n\nFrom Table 4 it can be observed that if we know John is 27 year old, still it will be dif-\nficult to estimate whether John has Cardiac problem or not and he is under low income \ngroup or not. T closeness may ensure attribute disclosure but implementing T closeness \nmay not give proper distribution of data every time.\n\nRandomization technique\n\nRandomization is the process of adding noise to the data which is generally done by \nprobability distribution [21]. Randomization is applied in surveys, sentiment analy-\nsis etc. Randomization does not need knowledge of other records in the data. It can be \napplied during data collection and pre processing time. There is no anonymization over-\nhead in randomization. However, applying randomization on large datasets is not possi-\nble because of time complexity and data utility which has been proved in our experiment \ndescribed below.\n\nWe have loaded 10k records from an employee database into Hadoop Distributed File \nSystem and processed them by executing a Map Reduce Job. We have experimented to \nclassify the employees based on their salary and age groups. In order apply randomiza-\ntion we added noise in the form of 5k records which are randomly added to make a data-\nbase of 15k records and following observations were made after running Map Reduce \njob.\n\n • More number of Mappers and Reducers were used as data volume increased.\n • Results before and after randomization were significantly different.\n • Some of the records which are outliers remain unaffected with randomization and \n\nare vulnerable to adversary attack.\n • Privacy preservation at the cost of data utility is not appreciated and hence randomi-\n\nzation may not be suitable for privacy preservation especially attribute disclosure.\n\nTable 4 T closeness privacy preservation technique\n\nSno Zip Age Salary Disease\n\n1 576** 2* 5k Cardiac problem\n\n2 576** 2* 16k Cancer\n\n3 576** 2* 9k Skin allergy\n\n4 5790* > 40 20k Skin allergy\n\n5 5790* > 40 42k Cardiac problem\n\n6 5790* > 40 8k Flu\n\n\n\nPage 6 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\nData distribution technique\n\nIn this technique, the data is distributed across many sites. Distribution of the data can \nbe done in two ways:\n\ni. Horizontal distribution of data\nii. Vertical distribution of data\n\nHorizontal distribution When data is distributed across many sites with same attrib-\nutes then the distribution is said to be horizontal distribution which is described in \nFig. 1.\n\nHorizontal distribution of data can be applied only when some aggregate functions or \noperations are to be applied on the data without actually sharing the data. For example, \nif a retail store wants to analyse their sales across various branches, they may employ \nsome analytics which does computations on aggregate data. However, as part of data \nanalysis the data holder may need to share the data with third party analyst which may \nlead to privacy breach. Classification and Clustering algorithms can be applied on dis-\ntributed data but it does not ensure privacy. If the data is distributed across different \nsites which belong to different organizations, then results of aggregate functions may \nhelp one party in detecting the data held with other parties. In such situations we expect \nall participating sites to be honest with each other [21].\n\nVertical distribution of data When Person specific information is distributed across \ndifferent sites under custodian of different organizations, then the distribution is called \nvertical distribution as shown in Fig. 2. For example, in crime investigations, the police \nofficials would like to know details of a particular criminal which include health, profes-\nsion, financial, personal etc. All this information may not be available at one site. Such a \ndistribution is called vertical distribution where each site holds few set of attributes of a \nperson. When some analytics has to be done data has to be pooled in from all these sites \nand there is a vulnerability of privacy breach.\n\nIn order to perform data analytics on vertically distributed data, where the attributes \nare distributed across different sites under custodian of different parties, it is highly \n\nFig. 1 Distribution of sales data across different sites\n\n\n\n\n\nPage 7 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\ndifficult to ensure privacy if the datasets are shared. For example, as part of a police \ninvestigation, the investigating officer wants to access some information about the \naccused from his employer, health department, bank to gain more insights about the \ncharacter of the person. In this process some of the personal and sensitive information \nof the accused may be disclosed to investigating officer leading to personal embarrass-\nment or abuse. Anonymization cannot be applied when entire records are not needed \nfor analytics. Distribution of data will not ensure privacy preservation but it closely \noverlaps with cryptographic techniques.\n\nCryptographic techniques\n\nThe data holder may encrypt the data before releasing the same for analytics. But \nencrypting large scale data using conventional encryption techniques is highly difficult \nand must be applied only during data collection time. Differential privacy techniques \nhave already been applied where some aggregate computations on the data are done \nwithout actually sharing the inputs. For example, if x and y are two data items then a \nfunction F(x, y) will be computed to gain some aggregate information from both x and \ny without actually sharing x and y. This can be applied on when x and y are held with \ndifferent parties as in the case of vertical distribution. However, if the data is at single \nlocation under the custodian of a single organization, then differential privacy can-\nnot be employed. Another similar technique called secure multiparty computation has \nbeen used but proved to be inadequate in privacy preservation. Data utility will be less \nif encryption is applied during data analytics. Thus encryption is not only difficult to \nimplement but it reduces the data utility [22].\n\nMultidimensional Sensitivity Based Anonymization (MDSBA)\n\nBottom up Generalization [23] and Top down Generalization [24] are the conventional \nmethods of Anonymization which were applied on well represented structured data \nrecords. However, applying the same on large scale data sets is very difficult leading to \n\nFig. 2 Vertical distribution of person specific data\n\n\n\n\n\nPage 8 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\nissues of scalability and information loss. Multidimensional Sensitivity Based Anonymi-\nzation is a improved version of Anonymization proved to be more effective than conven-\ntional Anonymization techniques.\n\nMultidimensional Sensitivity Based Anonymization is an improved Anonymization \n[25] technique such that it can be applied on large data sets with reduced loss of informa-\ntion and predefined quasi identifiers. As part of this technique Apache MAP REDUCE \n[26] framework has been used to handle large data sets. In conventional Hadoop Distrib-\nuted Files System, the data will be divided into blocks of either 64 MB or 128 MB each \nand distributed across different nodes without considering the data inside the blocks. \nAs part of Multidimensional Sensitivity Based Anonymization [27] technique the data is \nsplit into different bags based on the probability distribution of the quasi identifiers by \nmaking use of filters in Apache Pig scripting language.\n\nMultidimensional Sensitivity Based Anonymization makes use of bottom up generali-\nzation but on a set of attributes with certain class values where class represents a sensi-\ntive attributes. Data distribution was made effectively when compared to conventional \nmethod of blocks. Data Anonymization was done using four quasi identifiers using \nApache Pig.\n\nSince the data is vertically partitioned into different groups, it can protect from back-\nground knowledge attack if the bag contains only few attributes. This method also \nmakes it difficult to map the data with external sources to disclose any person specific \ninformation.\n\nIn this method, the implementation was done using Apache Pig. Apache Pig is a script-\ning language, hence development effort is less. However, code efficiency of Apache Pig is \nrelatively less when compared to Map Reduce job because ultimately every Apache Pig \nscript has to be converted into a Map Reduce job. Multidimensional Sensitivity Based \nAnonymization [28] is more appropriate for large scale data but only when the data is at \nrest. Multidimensional Sensitivity Based Anonymization cannot be applied for stream-\ning data.\n\nAnalysis\nVarious privacy preservation techniques have been studied with respect to features \nincluding, type of data, data utility, attribute preservation and complexity. The compari-\nson of various privacy preservation techniques is shown in Table 5.\n\nTable 5 Comparison of privacy preservation techniques\n\nFeatures Privacy preservation techniques\n\nAnonymization \ntechniques\n\nCryptographic \ntechniques\n\nData \ndistribution\n\nRandomization MDSBA\n\nSuitability for unstructured data No No No No Yes\n\nAttribute preservation No No No Yes Yes\n\nDamage to data utility No No Yes No Yes\n\nVery complex to apply No Yes Yes Yes Yes\n\nAccuracy of results of data \nanalytics\n\nNo Yes No No No\n\n\n\nPage 9 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\nResults and discussions\nAs part of systematic literature review, it has been observed that all existing mecha-\nnisms of privacy preservation are with respect to structured data. More than 80% of data \nbeing generated today is unstructured [29]. As such, there is a need to address following \nchallenges.\n\ni. Develop concrete solution to protect privacy in both structured and unstructured \ndata.\n\nii. Scalable and robust techniques to be developed to handle large scale heterogeneous \ndata sets.\n\niii. Data should be allowed to stay in its native form without need for transformation \nand data analytics can be carried out while ensuring privacy preservation.\n\niv. New techniques apart from Anonymization must be developed to ensure protection \nagainst key privacy threats which include identity disclosure, discrimination, surveil-\nlance etc.\n\nv. Maximizing data utility while ensuring data privacy.\n\nConclusion\nNo concrete solution for unstructured data has been developed yet. Conventional \ndata mining algorithms can be applied for classification and clustering problems but \ncannot be used in privacy preservation especially when dealing with person specific \ninformation. Machine learning and soft computing techniques can be used to develop \nnew and more appropriate solution to privacy problems which include identity dis-\nclosure that can lead to personal embarrassment and abuse.\n\nThere is a strong need for law enforcement by governments of all countries to \nensure individual privacy. European Union [30] is making an attempt to enforce pri-\nvacy preservation law. Apart from technological solutions, there is a strong need to \ncreate awareness among the people regarding privacy hazards to safeguard them-\nselves form privacy breaches. One of the serious privacy threats is smart phone. Lot \nof personal information in the form of contacts, messages, chats and files are being \naccessed by many apps running in our smart phone without our knowledge. Most \nof the time people do not even read the privacy statement before installing any app. \nHence there is a strong need to educate people on the various vulnerabilities which \ncan contribute to leakage of private information.\n\nWe propose a novel privacy preservation model based on Data Lake concept to \nhold variety of data from diverse sources. Data lake is a repository to hold data from \ndiverse sources in their raw format [31, 32]. Data ingestion from variety of sources can \nbe done using Apache Flume and an intelligent algorithm based on machine learning \ncan be applied to identify sensitive attributes dynamically [33, 34]. The algorithm will \nbe trained with existing data sets with known sensitive attributes and rigorous train-\ning of the model will help in predicting the sensitive attributes in a given data set [35]. \nAccuracy of the model can be improved by adding more layers of training leading \nto deep learning techniques [36]. Advanced computing techniques like Apache Spark \ncan be used in implementing privacy preserving algorithms which is a distributed \n\n\n\nPage 10 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\nmassive parallel computing with in memory processing to ensure very fast processing \n[37]. The proposed model is shown in Fig. 3.\n\nData analytics is done on the data collected from various sources. If an ecommerce \nsite would like to perform data analytics, they need transactional data, website logs and \ncustomers opinion through social media pages. A Data lake is used to collect data from \ndifferent sources. Apache Flume is used to ingest data from social media sites, website \nlogs into Hadoop Distributed File System(HDFS). Using SQOOP relational data can be \nloaded into HDFS.\n\nIn Data lake the data can remain in its native form which is either structured or \nunstructured. When data has to be processed, it can be transformed into HIVE tables. A \nHadoop map reduce job using machine learning can be executed on the data to classify \nthe sensitive attributes [38]. The data can be vertically distributed to separate the sensi-\ntive attributes from rest of the data and apply tokenization to map the vertically distrib-\nuted data. The data without any sensitive attributes can be published for data analytics.\n\nAbbreviations\nCCTV: closed circuit television; MDSBA: Multidimensional Sensitivity Based Anonymization.\n\nAuthors’ contributions\nPRMR: as part of Ph.D. work I have done my literature survey and submitted my work in the form of a paper. SMK: \nsupported me in compiling the paper. APSK: suggested necessary amendments and helped in revising the paper. All \nauthors read and approved the final manuscript.\n\nAuthor details\n1 Department of Computer Science and Engineering, MLR Institute of Technology, Hyderabad, India. 2 Department \nof Computer Science and Engineering, Sri Venkateswara College of Engineering, Tirupati, Andhra Pradesh, India. \n3 Department of Computer Science and Engineering, JNTU Anantapur, Anantapuramu, Andhra Pradesh, India. 4 JNTU \nAnantapur, Anantapur, Andhra Pradesh, India. \n\nAcknowledgements\nI would like to thank my guides, for supporting my work and for suggesting necessary corrections.\n\nData Lake\n\nSqoop to load data from RDBMS\n\nApache \nFlume \nto load \nsocial \nmedia \ndata\n\nLoad data from\ndifferent sources\nand varie�es into\nHive Table for\nprocessing\n\nHadoop\nMap\nReduce\nJob to\nclassify\nsensi�ve\ndata\n\nNovel Privacy \nPreserva�on \nalgorithm \nbased on \nver�cal \ndistribu�on and \ntokeniza�on\n\nFig. 3 A Novel privacy preservation model based on vertical distribution and tokenization\n\n\n\nPage 11 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\nCompeting interests\nThe authors declare that they have no competing interests.\n\nAvailability of data and materials\nIf any one is interested in our work, we are ready to provide more details of the map reduce job which we have \nexecuted and the data processing techniques applied. However the data is used in our work, is freely available in many \nrepositories.\n\nFunding\nNo Funding.\n\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nReceived: 21 March 2018 Accepted: 4 September 2018\n\nReferences\n 1. Ducange Pietro, Pecori Riccardo, Mezzina Paolo. A glimpse on big data analytics in the framework of marketing \n\nstrategies. Soft Comput. 2018;22(1):325–42.\n 2. Chauhan Arun, Kummamuru Krishna, Toshniwal Durga. Prediction of places of visit using tweets. Knowl Inf Syst. \n\n2017;50(1):145–66.\n 3. Yang D, Bingqing Q, Cudre-Mauroux P. Privacy-preserving social media data publishing for personalized ranking-\n\nbased recommendation. IEEE Trans Knowl Data Eng. 2018. ISSN (Print):1041-4347, ISSN (Electronic):1558-2191.\n 4. Liu Y et al. A practical privacy-preserving data aggregation (3PDA) scheme for smart grid. IEEE Trans Ind Inf. 2018.\n 5. Duncan GT et al. Disclosure limitation methods and information loss for tabular data. In: Confidentiality, disclosure \n\nand data access: theory and practical applications for statistical agencies. 2001. p. 135–166.\n 6. Duncan GT, Diane L. Disclosure-limited data dissemination. J Am Stat Assoc. 1986;81(393):10–8.\n 7. Lambert Diane. Measures of disclosure risk and harm. J Off Stat. 1993;9(2):313.\n 8. Spiller K, et al. Data privacy: users’ thoughts on quantified self personal data. Self-Tracking. Cham: Palgrave Macmil-\n\nlan; 2018. p. 111–24.\n 9. Hettig M, Kiss E, Kassel J-F, Weber S, Harbach M. Visualizing risk by example: demonstrating threats arising from \n\nandroid apps. In: Smith M, editor. Symposium on usable privacy and security (SOUPS), Newcastle, UK, July 24–26, \n2013.\n\n 10. Bayardo RJ, Agrawal A. Data privacy through optimal k-anonymization. In: Proceedings 21st international confer-\nence on data engineering, 2005 (ICDE 2005). Piscataway: IEEE; 2005.\n\n 11. Iyengar S. Transforming data to satisfy privacy constraints. In: Proceedings of the eighth ACM SIGKDD international \nconference on knowledge discovery and data mining. New York: ACM; 2002.\n\n 12. LeFevre K, DeWitt DJ, Ramakrishnan R. Incognito: efficient full-domain k-anonymity. In: Proceedings of the 2005 \nACM SIGMOD international conference on management of data. New York: ACM; 2005.\n\n 13. LeFevre K, DeWitt DJ, Ramakrishnan R. Mondrian multidimensional k-anonymity. In: Proceedings of the 22nd inter-\nnational conference (ICDE’06) on data engineering, 2006. New York: ACM; 2006.\n\n 14. Samarati, Pierangela, and Latanya Sweeney. In: Protecting privacy when disclosing information: k-anonymity and its \nenforcement through generalization and suppression. Technical report, SRI International, 1998.\n\n 15. Sweeney Latanya. Achieving k-anonymity privacy protection using generalization and suppression. In J Uncertain \nFuzziness Knowl Based Syst. 2002;10(05):571–88.\n\n 16. Sweeney Latanya. k-Anonymity: a model for protecting privacy. Int J Uncertain, Fuzziness Knowl Based Syst. \n2002;10(05):557–70.\n\n 17. Williams R. On the complexity of optimal k-anonymity. In: Proc. 23rd ACM SIGMOD-SIGACT-SIGART symp. principles \nof database systems (PODS). New York: ACM; 2004.\n\n 18. Machanavajjhala A et al. L-diversity: privacy beyond k-anonymity. In: Proceedings of the 22nd international confer-\nence on data engineering (ICDE’06), 2006. Piscataway: IEEE; 2006.\n\n 19. Xiao X, Yufei T. Personalized privacy preservation. In: Proceedings of the 2006 ACM SIGMOD international confer-\nence on Management of data. New York: ACM; 2006.\n\n 20. Rubner Y, Tomasi T, Guibas LJ. The earth mover’s distance as a metric for image retrieval. Int J Comput Vision. \n2000;40(2):99–121.\n\n 21. Aggarwal CC, Philip SY. A general survey of privacy-preserving data mining models and algorithms. Privacy-preserv-\ning data mining. Springer: US; 2008. p. 11–52.\n\n 22. Jiang R, Lu R, Choo KK. Achieving high performance and privacy-preserving query over encrypted multidimensional \nbig metering data. Future Gen Comput Syst. 2018;78:392–401.\n\n 23. Wang K, Yu PS, Chakraborty S. Bottom-up generalization: A data mining solution to privacy protection. In: Fourth \nIEEE international conference on data mining, 2004 (ICDM’04). Piscataway: IEEE; 2004.\n\n 24. Fung BCM, Wang K, Yu PS. Top-down specialization for information and privacy preservation. In: Proceedings 21st \ninternational conference on data engineering, 2005 (ICDE 2005). Piscataway: IEEE; 2005.\n\n 25. Zhang X et al. A MapReduce based approach of scalable multidimensional anonymization for big data privacy \npreservation on cloud. In: Third international conference on cloud and green computing (CGC), 2013. Piscataway: \nIEEE; 2013.\n\n\n\n\n\nPage 12 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\n 26. Zhang X, et al. A scalable two-phase top-down specialization approach for data anonymization using mapreduce \non cloud. IEEE Trans Parallel Distrib Syst. 2014;25(2):363–73.\n\n 27. Al-Zobbi M, Shahrestani S, Ruan C. Improving MapReduce privacy by implementing multi-dimensional sensitivity-\nbased anonymization. J Big Data. 2017;4(1):45.\n\n 28. Al-Zobbi M, Shahrestani S, Ruan C. Implementing a framework for big data anonymity and analytics access control. \nIn: Trustcom/BigDataSE/ICESS, 2017 IEEE. Piscataway: IEEE; 2017.\n\n 29. Schneider C. IBM Blogs; 2016. https ://www.ibm.com/blogs /watso n/2016/05/bigge st-data-chall enges -might \n-not-even-know/.\n\n 30. TCS. Emphasizing the need for government regulations on data privacy; 2016. https ://www.tcs.com/conte nt/dam/\ntcs/pdf/techn ologi es/Cyber -Secur ity/Abstr act/Stren gthen ing-Priva cy-Pro", "metadata_storage_path": "aHR0cHM6Ly9zdG9yYWdlYWNjb3VudGxpZ2lyay5ibG9iLmNvcmUud2luZG93cy5uZXQvcGFwZXIvczQwNTM3LTAxOC0wMTQxLTgucGRm0", "metadata_author": "P. Ram Mohan Rao ", "metadata_title": "Privacy preservation techniques in big data analytics: a survey", "keyphrases": [ "Creative Commons Attribution 4.0 International License", "A. P. Siva Kumar3", "P. Ram Mohan Rao", "third party data analyst", "modernistic privacy preservation technique", "Privacy preservation Open Access", "Creative Commons license", "Ram Mohan Rao", "S. Murali Krishna2", "diverse applica- tions", "person specific private", "value added services", "web site logs", "creat iveco mmons", "original author(s", "J Big Data", "Privacy preservation techniques", "privacy preserving techniques", "The data holder", "various privacy threats", "user activity data", "big data analytics", "The Author", "privacy violations", "privacy concerns", "various organizations", "various sources", "author information", "domain areas", "able availability", "network connectivity", "scale data", "sensitive data", "zip code", "shopping cart", "public domain", "deeper insights", "hidden patterns", "important decisions", "ecommerce sites", "buying habits", "Face book", "inference attacks", "Incredible amounts", "supply chain", "sion streaming", "social media", "smart phones", "voluminous data", "decision making", "dation systems", "data lake", "unstructured data", "unrestricted use", "appropriate credit", "MLR Institute", "Full list", "research challenges", "data utility", "movie recommendation", "digital technology", "exponential growth", "prominent applications", "doi.org", "computer technology", "Flip kart", "SURVEY PAPER", "Introduction", "volume", "variety", "computers", "storage", "gender", "disease", "caste", "religion", "businesses", "customers", "prediction", "casting", "Amazon", "products", "friends", "places", "interest", "number", "merits", "Abstract", "hospitals", "banks", "retail", "virtue", "humans", "machines", "Tons", "serious", "models", "limitations", "Keywords", "article", "terms", "distribution", "reproduction", "medium", "link", "changes", "Correspondence", "rammohan04", "1 Department", "Science", "Engineering", "Hyderabad", "India", "creativecommons", "licenses", "crossmark", "crossref", "dialog", "Page", "trade", "Multidimensional Sensitivity Based Anonymization", "Many Privacy preserving techniques", "many smart phones users", "social networking application", "social media sites", "hospital holding patient", "12Ram Mohan Rao", "regular business model", "serious privacy breach", "Privacy preservation laws", "Privacy preservation methods", "privacy preservation techniques", "smart phone users", "maximum data utility", "external data sources", "private informa- tion", "key privacy threats", "Data analytics activity", "serious privacy threat", "person specific data", "Cryptographic techniques", "Many organizations", "Many countries", "many apps", "users data", "medica- tion", "specific problem", "regular basis", "privacy statement", "data Privacy", "privacy attacks", "data holder", "mobile apps", "ecommerce site", "data leakage", "Personal embracement", "opinion data", "new friends", "census data", "electoral results", "medical shop", "family member", "major reason", "Previous research", "K anonymity", "L diversity", "T closeness", "Data distribution", "private information", "individual privacy", "sentiment analysis", "statistical analysis", "access control", "various offers", "one community", "ability", "websites", "responsibility", "contacts", "files", "camera", "conditions", "need", "Surveillance", "Disclosure", "crimination", "abuse", "habits", "recommendations", "people", "transactions", "Zip", "sensitive", "disorder", "bias", "inequality", "instance", "government", "example", "medicines", "reminder", "Lack", "awareness", "list", "Randomization", "MDSBA", "Sno Zip Age Salary Disease", "Table 3 L diversity privacy preservation technique", "Sno Zip Age Disease", "equivalence classes attribute disclosure", "minimal gener- alization", "low income group", "sensi- tive attribute", "ground knowledge attack", "three equivalence classes", "huge data loss", "40 20k Skin allergy", "T closeness measure", "2* 6k Cardiac problem", "7k Cardiac problem", "22k Cardiac problem", "K anonymity algorithm", "K indistinguishable records", "Personalized privacy", "age attributes", "sensitive attribute", "40 Skin allergy", "Identity disclosure", "three persons", "40 Cardiac problem", "k value", "57677 zip codes", "larity attack", "de identification", "two attacks", "particular person", "two attributes", "vacy threat", "important aspect", "overall distribution", "entire records", "close- ness", "data analytics", "anonymized data", "patient data", "homogeneity attack", "40 24k Cancer", "576** zip", "3 indistinguishable", "Table 1", "40 Cancer", "Anonymization", "process", "attempt", "algorithms", "Incognito", "drian", "result", "generalization", "John", "Achieving", "suppression", "values", "salaries", "others", "improvement", "distance", "distributions", "threshold", "Table 4 T closeness privacy preservation technique", "Hadoop Distributed File System", "2* 9k Skin allergy", "same attrib- utes", "pre processing time", "third party analyst", "40 42k Cardiac problem", "Map Reduce Job", "Person specific information", "Data distribution technique", "age groups", "privacy breach", "time complexity", "one party", "Randomization technique", "low income", "large datasets", "employee database", "data- base", "More number", "adversary attack", "40 8k Flu", "many sites", "two ways", "aggregate functions", "retail store", "various branches", "Clustering algorithms", "different sites", "different organizations", "other parties", "participating sites", "crime investigations", "police officials", "particular criminal", "other records", "10k records", "5k records", "proper distribution", "probability distribution", "Horizontal distribution", "Vertical distribution", "attribute disclosure", "one site", "data collection", "data volume", "aggregate data", "respect", "ficult", "noise", "surveys", "sis", "knowledge", "anonymization", "head", "experiment", "employees", "order", "observations", "Mappers", "Reducers", "Results", "outliers", "cost", "Fig.", "operations", "sales", "analytics", "computations", "Classification", "situations", "custodian", "details", "sion", "attributes", "10K 10K to 20 sales K sales records records", "20K to 30K sales records", "social different network organizations accounts", "person specific data Health Department Data", "Apache Pig scripting language", "large scale data sets", "Apache MAP REDUCE", "large data sets", "data collection time", "two data items", "tional Anonymization techniques", "sales data", "data records", "Differential privacy techniques", "conventional encryption techniques", "Personal Bank custodian", "cryptographic techniques", "conventional methods", "different parties", "different nodes", "different bags", "Data utility", "police investigation", "embarrass- ment", "privacy preservation", "aggregate computations", "single location", "single organization", "reduced loss", "informa- tion", "quasi identifiers", "Files System", "sensitive information", "aggregate information", "information loss", "vertical distribution", "investigating officer", "similar technique", "Employer site", "vulnerability", "datasets", "insights", "character", "overlaps", "inputs", "function", "case", "Generalization", "email", "issues", "scalability", "version", "predefined", "blocks", "64 MB", "128 MB", "filters", "bottom", "Cryptographic techniques Data distribution Randomization", "Various privacy preservation techniques", "soft computing techniques", "four quasi identifiers", "script- ing language", "systematic literature review", "large scale heterogeneous", "serious privacy threats", "vacy preservation law", "large scale data", "stream- ing data", "data mining algorithms", "Apache Pig script", "Map Reduce job", "various vulnerabilities", "Anonymization techniques", "robust techniques", "attribute preservation", "New techniques", "law enforcement", "privacy problems", "privacy hazards", "privacy breaches", "data privacy", "different groups", "external sources", "specific information", "development effort", "code efficiency", "concrete solution", "identity disclosure", "clustering problems", "Machine learning", "appropriate solution", "personal embarrassment", "European Union", "technological solutions", "smart phone", "personal information", "Data Anonymization", "Big Data", "data sets", "strong need", "class values", "native form", "time people", "conventional", "method", "bag", "implementation", "rest", "Analysis", "features", "type", "complexity", "compari", "Table", "Suitability", "Damage", "Accuracy", "results", "discussions", "part", "nisms", "More", "challenges", "Scalable", "transformation", "protection", "discrimination", "lance", "Conclusion", "classification", "governments", "countries", "selves", "Lot", "messages", "chats", "novel privacy preservation model", "privacy preserving algorithms", "rigorous train- ing", "Advanced computing techniques", "massive parallel computing", "social media pages", "closed circuit television", "Sri Venkateswara College", "deep learning techniques", "existing data sets", "SQOOP relational data", "social media data", "Ph.D. work", "Data Lake concept", "A Data lake", "Data Lake Sqoop", "data processing techniques", "proposed model", "machine learning", "Hadoop map", "raw format", "sensitive attributes", "memory processing", "fast processing", "website logs", "customers opinion", "HIVE tables", "Abbreviations CCTV", "literature survey", "necessary amendments", "final manuscript", "Computer Science", "Andhra Pradesh", "necessary corrections", "Competing interests", "Springer Nature", "jurisdictional claims", "institutional affiliations", "diverse sources", "different sources", "Data ingestion", "Data analytics", "transactional data", "Load data", "Apache Spark", "Author details", "Apache Flume", "intelligent algorithm", "Authors’ contributions", "JNTU Anantapur", "4 JNTU", "leakage", "repository", "layers", "training", "HDFS", "job", "tokenization", "PRMR", "paper", "SMK", "APSK", "Technology", "Tirupati", "Anantapuramu", "Acknowledgements", "guides", "RDBMS", "�cal", "Availability", "materials", "one", "many", "repositories", "Funding", "Publisher", "Note", "regard", "maps", "23rd ACM SIGMOD-SIGACT-SIGART symp. principles", "Privacy-preserving social media data publishing", "Diane L. Disclosure-limited data dissemination", "eighth ACM SIGKDD international conference", "2006 ACM SIGMOD international confer- ence", "IEEE Trans Knowl Data Eng", "J Am Stat Assoc", "quantified self personal data", "IEEE Trans Ind Inf.", "ACM SIGMOD international conference", "Int J Comput Vision", "Future Gen Comput Syst", "practical privacy-preserving data aggregation", "privacy-preserving data mining models", "IEEE international conference", "J Off Stat", "Int J Uncertain", "Knowl Inf Syst.", "big metering data", "Ramakrishnan R. Incognito", "Ramakrishnan R. Mondrian", "data mining solution", "Disclosure limitation methods", "efficient full-domain k-anonymity", "Personalized privacy preservation", "k-anonymity privacy protection", "SRI International", "privacy-preserving query", "Lambert Diane", "Soft Comput", "practical applications", "Fuzziness Knowl", "tabular data", "data access", "data engineering", "Data privacy", "Williams R.", "Jiang R", "Lu R", "Ducange Pietro", "Pecori Riccardo", "Mezzina Paolo", "marketing strategies", "Chauhan Arun", "Kummamuru Krishna", "Toshniwal Durga", "Yang D", "Bingqing Q", "Cudre-Mauroux P", "Liu Y", "smart grid", "Duncan GT", "statistical agencies", "Spiller K", "Hettig M", "Kiss E", "Kassel J-F", "Weber S", "Harbach M", "android apps", "Smith M", "Bayardo RJ", "Agrawal A.", "optimal k-anonymization", "Iyengar S.", "knowledge discovery", "New York", "LeFevre K", "DeWitt DJ", "Latanya Sweeney", "Technical report", "Sweeney Latanya", "optimal k-anonymity", "database systems", "Machanavajjhala A", "Xiao X", "Yufei T.", "Rubner Y", "Tomasi T", "Guibas LJ.", "earth mover", "image retrieval", "Aggarwal CC", "Philip SY", "general survey", "Choo KK", "high performance", "Wang K", "Yu PS", "Chakraborty S", "usable privacy", "privacy constraints", "disclosure risk", "multidimensional k-anonymity", "Bottom-up generalization", "References", "glimpse", "framework", "Prediction", "visit", "tweets", "recommendation", "ISSN", "Print", "Confidentiality", "theory", "Measures", "harm", "users", "thoughts", "Self-Tracking", "Cham", "lan", "threats", "editor", "Symposium", "security", "SOUPS", "Newcastle", "UK", "July", "Proceedings", "ICDE", "Piscataway", "management", "Samarati", "Pierangela", "enforcement", "Proc.", "PODS", "L-diversity", "metric", "Springer", "Fourth", "ICDM", "4", "IEEE Trans Parallel Distrib Syst", "scalable two-phase top-down specialization approach", "big data privacy preservation", "scalable multidimensional anonymization", "analytics access control", "Secur ity/Abstr act", "big data anonymity", "Third international conference", "data anonymization", "MapReduce privacy", "Fung BCM", "Zhang X", "green computing", "Al-Zobbi M", "Shahrestani S", "Ruan C", "29. Schneider C.", "government regulations", "techn ologi", "Stren gthen", "Priva cy-Pro", "A MapReduce", "IBM Blogs", "information", "cloud", "CGC", "Trustcom/BigDataSE/ICESS", "bigge", "data-chall", "TCS", "conte", "Cyber" ], "merged_content": "\nPrivacy preservation techniques in big \ndata analytics: a survey\nP. Ram Mohan Rao1,4*, S. Murali Krishna2 and A. P. Siva Kumar3\n\nIntroduction\nThere is an exponential growth in volume and variety of data as due to diverse applica-\ntions of computers in all domain areas. The growth has been achieved due to afford-\nable availability of computer technology, storage, and network connectivity. The large \nscale data, which also include person specific private and sensitive data like gender, zip \ncode, disease, caste, shopping cart, religion etc. is being stored in public domain. The \ndata holder can release this data to a third party data analyst to gain deeper insights and \nidentify hidden patterns which are useful in making important decisions that may help \nin improving businesses, provide value added services to customers [1], prediction, fore-\ncasting and recommendation [2]. One of the prominent applications of data analytics is \nrecommendation systems which is widely used by ecommerce sites like Amazon, Flip \nkart for suggesting products to customers based on their buying habits. Face book does \nsuggest friends, places to visit and even movie recommendation based on our interest. \nHowever releasing user activity data may lead inference attacks like identifying gender \nbased on user activity [3]. We have studied a number of privacy preserving techniques \nwhich are being employed to protect against privacy threats. Each of these techniques \nhas their own merits and demerits. This paper explores the merits and demerits of each \n\nAbstract \n\nIncredible amounts of data is being generated by various organizations like hospitals, \nbanks, e-commerce, retail and supply chain, etc. by virtue of digital technology. Not \nonly humans but machines also contribute to data in the form of closed circuit televi-\nsion streaming, web site logs, etc. Tons of data is generated every minute by social \nmedia and smart phones. The voluminous data generated from the various sources \ncan be processed and analyzed to support decision making. However data analytics \nis prone to privacy violations. One of the applications of data analytics is recommen-\ndation systems which is widely used by ecommerce sites like Amazon, Flip kart for \nsuggesting products to customers based on their buying habits leading to inference \nattacks. Although data analytics is useful in decision making, it will lead to serious \nprivacy concerns. Hence privacy preserving data analytics became very important. This \npaper examines various privacy threats, privacy preservation techniques and models \nwith their limitations, also proposes a data lake based modernistic privacy preservation \ntechnique to handle privacy preservation in unstructured data.\n\nKeywords: Data, Data analytics, Privacy threats, Privacy preservation\n\nOpen Access\n\n© The Author(s) 2018. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nSURVEY PAPER\n\nRam Mohan Rao et al. J Big Data (2018) 5:33 \nhttps://doi.org/10.1186/s40537-018-0141-8\n\n*Correspondence: \nrammohan04@gmail.com \n1 Department of Computer \nScience and Engineering, \nMLR Institute of Technology, \nHyderabad, India\nFull list of author information \nis available at the end of the \narticle\n\n \n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-018-0141-8&domain=pdf\n\n\nPage 2 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\nof these techniques and also describes the research challenges in the area of privacy \npreservation. Always there exists a trade off between data utility and privacy. This paper \nalso proposes a data lake based modernistic privacy preservation technique to handle \nprivacy preservation in unstructured data with maximum data utility.\n\nPrivacy threats in data analytics\nPrivacy is the ability of an individual to determine what data can be shared, and employ \naccess control. If the data is in public domain then it is a threat to individual privacy \nas the data is held by data holder. Data holder can be social networking application, \nwebsites, mobile apps, ecommerce site, banks, hospitals etc. It is the responsibility of \nthe data holder to ensure privacy of the users data. Apart from the data held in public \ndomain, knowing or unknowingly users themself contribute to data leakage. For exam-\nple most of the mobile apps, seek access to our contacts, files, camera etc. and without \nreading the privacy statement we agree for all terms and conditions, there by contribut-\ning to data leakage.\n\nHence there is a need to educate the smart phone users regarding privacy and privacy \nthreats. Some of the key privacy threats include (1) Surveillance; (2) Disclosure; (3) Dis-\ncrimination; (4) Personal embracement and abuse.\n\nSurveillance\n\nMany organizations including retail, e-commerce, etc. study their customers buying \nhabits and try to come up with various offers and value added services [4]. Based on the \nopinion data and sentiment analysis, social media sites does provide recommendations \nof the new friends, places to visit, people to follow etc. This is possible only when they \ncontinuously monitor their customer’s transactions. This is a serious privacy threat as no \nindividual accepts surveillance.\n\nDisclosure\n\nConsider a hospital holding patient’s data which include (Zip, gender, age, disease) [5–7]. \nThe data holder has released data to a third party for analysis by anonymizing sensitive \nperson specific data so that the person cannot be identified. The third party data analyst \ncan map this information with the freely available external data sources like census data \nand can identify person suffering with some disorder. This is how private information of \na person can be disclosed which is considered to be a serious privacy breach.\n\nDiscrimination\n\nDiscrimination is the bias or inequality which can happen when some private informa-\ntion of a person is disclosed. For instance, statistical analysis of electoral results proved \nthat people of one community were completely against the party, which formed the gov-\nernment. Now the government can neglect that community or can have bias over them.\n\nPersonal embracement and abuse\n\nWhenever some private information of a person is disclosed, it can even lead to per-\nsonal embracement or abuse. For example, a person was privately undergoing medica-\ntion for some specific problem and was buying some medicines on a regular basis from a \n\n\n\nPage 3 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\nmedical shop. As part of their regular business model, the medical shop may send some \nreminder and offers related to these medicines over phone. If any family member has \nnoticed this, it will lead to personal embracement and even abuse [8].\n\nData analytics activity will affect data Privacy. Many countries are enforcing Privacy \npreservation laws. Lack of awareness is also a major reason for privacy attacks. For \nexample many smart phones users are not aware of the information that is stolen from \ntheir phones by many apps. Previous research shows only 17% of smart phone users are \naware of privacy threats [9].\n\nPrivacy preservation methods\nMany Privacy preserving techniques were developed, but most of them are based on \nanonymization of data. The list of privacy preservation techniques is given below.\n\n • K anonymity\n • L diversity\n • T closeness\n • Randomization\n • Data distribution\n • Cryptographic techniques\n • Multidimensional Sensitivity Based Anonymization (MDSBA).\n\nK anonymity [10]\n\nAnonymization is the process of modifying data before it is given for data analytics [11], \nso that de identification is not possible and will lead to K indistinguishable records if \nan attempt is made to de identify by mapping the anonymized data with external data \nsources. K anonymity is prone to two attacks namely homogeneity attack and back \nground knowledge attack. Some of the algorithms applied include, Incognito [12], Mon-\ndrian [13] to ensure Anonymization. K anonymity is applied on the patient data shown \nin Table 1. The table shows data before anonymization.\n\nK anonymity algorithm is applied with k value as 3 to ensure 3 indistinguishable \nrecords when an attempt is made to identify a particular person’s data. K anonymity is \napplied on the two attributes viz. Zip and age shown in Table 1. The result of applying \nanonymization on Zip and age attributes is shown in Table 2.\n\nTable 1 Patient data, before anonymization\n\nSno Zip Age Disease\n\n1 57677 29 Cardiac problem\n\n2 57602 22 Cardiac problem\n\n3 57678 27 Cardiac problem\n\n4 57905 43 Skin allergy\n\n5 57909 52 Cardiac problem\n\n6 57906 47 Cancer\n\n7 57605 30 Cardiac problem\n\n8 57673 36 Cancer\n\n9 57607 32 Cancer\n\n\n\nPage 4 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\nThe above technique has used generalization [14] to achieve Anonymization. Suppose \nif we know that John is 27 year old and lives in 57677 zip codes then we can conclude \nJohn to have Cardiac problem even after anonymization as shown in Table  2. This is \ncalled Homogeneity attack. For example if John is 36 year old and it is known that John \ndoes not have cancer, then definitely John must have Cardiac problem. This is called as \nbackground knowledge attack. Achieving K anonymity [15, 16] can be done either by \nusing generalization or suppression. K anonymity can optimized if the minimal gener-\nalization can be done without huge data loss [17]. Identity disclosure is the major pri-\nvacy threat which cannot be guaranteed by K anonymity [18]. Personalized privacy is the \nmost important aspect of individual privacy [19].\n\nL diversity\n\nTo address homogeneity attack, another technique called L diversity has been proposed. \nAs per L diversity there must be L well represented values for the sensitive attribute (dis-\nease) in each equivalence class.\n\nImplementing L diversity is not possible every time because of the variety of data. L \ndiversity is also prone to skewness attack. When overall distribution of data is skewed \ninto few equivalence classes attribute disclosure cannot be ensured. For example if the \nentire records are distributed into only three equivalence classes then semantic close-\nness of these values may lead to attribute disclosure. Also L diversity may lead to simi-\nlarity attack. From Table 3 it can be noticed that if we know that John is 27 year old and \nlives in 57677 zip, then definitely John is under low income group because salaries of all \n\nTable 2 After applying anonymization on Zip and age\n\nSno Zip Age Disease\n\n1 576** 2* Cardiac problem\n\n2 576** 2* Cardiac problem\n\n3 576** 2* Cardiac problem\n\n4 5790* > 40 Skin allergy\n\n5 5790* > 40 Cardiac problem\n\n6 5790* > 40 Cancer\n\n7 576** 3* Cardiac problem\n\n8 576** 3* Cancer\n\n9 576** 3* Cancer\n\nTable 3 L diversity privacy preservation technique\n\nSno Zip Age Salary Disease\n\n1 576** 2* 5k Cardiac problem\n\n2 576** 2* 6k Cardiac problem\n\n3 576** 2* 7k Cardiac problem\n\n4 5790* > 40 20k Skin allergy\n\n5 5790* > 40 22k Cardiac problem\n\n6 5790* > 40 24k Cancer\n\n\n\nPage 5 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\nthree persons in 576** zip is low compare to others in the table. This is called as similar-\nity attack.\n\nT closeness\n\nAnother improvement to L diversity is T closeness measure where an equivalence class \nis considered to have ‘T closeness’ if the distance between the distributions of sensi-\ntive attribute in the class is no more than a threshold and all equivalence classes have T \ncloseness [20]. T closeness can be calculated on every attribute with respect to sensitive \nattribute.\n\nFrom Table 4 it can be observed that if we know John is 27 year old, still it will be dif-\nficult to estimate whether John has Cardiac problem or not and he is under low income \ngroup or not. T closeness may ensure attribute disclosure but implementing T closeness \nmay not give proper distribution of data every time.\n\nRandomization technique\n\nRandomization is the process of adding noise to the data which is generally done by \nprobability distribution [21]. Randomization is applied in surveys, sentiment analy-\nsis etc. Randomization does not need knowledge of other records in the data. It can be \napplied during data collection and pre processing time. There is no anonymization over-\nhead in randomization. However, applying randomization on large datasets is not possi-\nble because of time complexity and data utility which has been proved in our experiment \ndescribed below.\n\nWe have loaded 10k records from an employee database into Hadoop Distributed File \nSystem and processed them by executing a Map Reduce Job. We have experimented to \nclassify the employees based on their salary and age groups. In order apply randomiza-\ntion we added noise in the form of 5k records which are randomly added to make a data-\nbase of 15k records and following observations were made after running Map Reduce \njob.\n\n • More number of Mappers and Reducers were used as data volume increased.\n • Results before and after randomization were significantly different.\n • Some of the records which are outliers remain unaffected with randomization and \n\nare vulnerable to adversary attack.\n • Privacy preservation at the cost of data utility is not appreciated and hence randomi-\n\nzation may not be suitable for privacy preservation especially attribute disclosure.\n\nTable 4 T closeness privacy preservation technique\n\nSno Zip Age Salary Disease\n\n1 576** 2* 5k Cardiac problem\n\n2 576** 2* 16k Cancer\n\n3 576** 2* 9k Skin allergy\n\n4 5790* > 40 20k Skin allergy\n\n5 5790* > 40 42k Cardiac problem\n\n6 5790* > 40 8k Flu\n\n\n\nPage 6 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\nData distribution technique\n\nIn this technique, the data is distributed across many sites. Distribution of the data can \nbe done in two ways:\n\ni. Horizontal distribution of data\nii. Vertical distribution of data\n\nHorizontal distribution When data is distributed across many sites with same attrib-\nutes then the distribution is said to be horizontal distribution which is described in \nFig. 1.\n\nHorizontal distribution of data can be applied only when some aggregate functions or \noperations are to be applied on the data without actually sharing the data. For example, \nif a retail store wants to analyse their sales across various branches, they may employ \nsome analytics which does computations on aggregate data. However, as part of data \nanalysis the data holder may need to share the data with third party analyst which may \nlead to privacy breach. Classification and Clustering algorithms can be applied on dis-\ntributed data but it does not ensure privacy. If the data is distributed across different \nsites which belong to different organizations, then results of aggregate functions may \nhelp one party in detecting the data held with other parties. In such situations we expect \nall participating sites to be honest with each other [21].\n\nVertical distribution of data When Person specific information is distributed across \ndifferent sites under custodian of different organizations, then the distribution is called \nvertical distribution as shown in Fig. 2. For example, in crime investigations, the police \nofficials would like to know details of a particular criminal which include health, profes-\nsion, financial, personal etc. All this information may not be available at one site. Such a \ndistribution is called vertical distribution where each site holds few set of attributes of a \nperson. When some analytics has to be done data has to be pooled in from all these sites \nand there is a vulnerability of privacy breach.\n\nIn order to perform data analytics on vertically distributed data, where the attributes \nare distributed across different sites under custodian of different parties, it is highly \n\nFig. 1 Distribution of sales data across different sites\n\n Site 1 Site 2 1 to 10K 10K to 20 sales K sales records records Site 3 20K to 30K sales records \n\n\n\nPage 7 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\ndifficult to ensure privacy if the datasets are shared. For example, as part of a police \ninvestigation, the investigating officer wants to access some information about the \naccused from his employer, health department, bank to gain more insights about the \ncharacter of the person. In this process some of the personal and sensitive information \nof the accused may be disclosed to investigating officer leading to personal embarrass-\nment or abuse. Anonymization cannot be applied when entire records are not needed \nfor analytics. Distribution of data will not ensure privacy preservation but it closely \noverlaps with cryptographic techniques.\n\nCryptographic techniques\n\nThe data holder may encrypt the data before releasing the same for analytics. But \nencrypting large scale data using conventional encryption techniques is highly difficult \nand must be applied only during data collection time. Differential privacy techniques \nhave already been applied where some aggregate computations on the data are done \nwithout actually sharing the inputs. For example, if x and y are two data items then a \nfunction F(x, y) will be computed to gain some aggregate information from both x and \ny without actually sharing x and y. This can be applied on when x and y are held with \ndifferent parties as in the case of vertical distribution. However, if the data is at single \nlocation under the custodian of a single organization, then differential privacy can-\nnot be employed. Another similar technique called secure multiparty computation has \nbeen used but proved to be inadequate in privacy preservation. Data utility will be less \nif encryption is applied during data analytics. Thus encryption is not only difficult to \nimplement but it reduces the data utility [22].\n\nMultidimensional Sensitivity Based Anonymization (MDSBA)\n\nBottom up Generalization [23] and Top down Generalization [24] are the conventional \nmethods of Anonymization which were applied on well represented structured data \nrecords. However, applying the same on large scale data sets is very difficult leading to \n\nFig. 2 Vertical distribution of person specific data\n\n Health Department Data under Personal Bank custodian of (email, social different network organizations accounts etc) Employer site \n\n\n\nPage 8 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\nissues of scalability and information loss. Multidimensional Sensitivity Based Anonymi-\nzation is a improved version of Anonymization proved to be more effective than conven-\ntional Anonymization techniques.\n\nMultidimensional Sensitivity Based Anonymization is an improved Anonymization \n[25] technique such that it can be applied on large data sets with reduced loss of informa-\ntion and predefined quasi identifiers. As part of this technique Apache MAP REDUCE \n[26] framework has been used to handle large data sets. In conventional Hadoop Distrib-\nuted Files System, the data will be divided into blocks of either 64 MB or 128 MB each \nand distributed across different nodes without considering the data inside the blocks. \nAs part of Multidimensional Sensitivity Based Anonymization [27] technique the data is \nsplit into different bags based on the probability distribution of the quasi identifiers by \nmaking use of filters in Apache Pig scripting language.\n\nMultidimensional Sensitivity Based Anonymization makes use of bottom up generali-\nzation but on a set of attributes with certain class values where class represents a sensi-\ntive attributes. Data distribution was made effectively when compared to conventional \nmethod of blocks. Data Anonymization was done using four quasi identifiers using \nApache Pig.\n\nSince the data is vertically partitioned into different groups, it can protect from back-\nground knowledge attack if the bag contains only few attributes. This method also \nmakes it difficult to map the data with external sources to disclose any person specific \ninformation.\n\nIn this method, the implementation was done using Apache Pig. Apache Pig is a script-\ning language, hence development effort is less. However, code efficiency of Apache Pig is \nrelatively less when compared to Map Reduce job because ultimately every Apache Pig \nscript has to be converted into a Map Reduce job. Multidimensional Sensitivity Based \nAnonymization [28] is more appropriate for large scale data but only when the data is at \nrest. Multidimensional Sensitivity Based Anonymization cannot be applied for stream-\ning data.\n\nAnalysis\nVarious privacy preservation techniques have been studied with respect to features \nincluding, type of data, data utility, attribute preservation and complexity. The compari-\nson of various privacy preservation techniques is shown in Table 5.\n\nTable 5 Comparison of privacy preservation techniques\n\nFeatures Privacy preservation techniques\n\nAnonymization \ntechniques\n\nCryptographic \ntechniques\n\nData \ndistribution\n\nRandomization MDSBA\n\nSuitability for unstructured data No No No No Yes\n\nAttribute preservation No No No Yes Yes\n\nDamage to data utility No No Yes No Yes\n\nVery complex to apply No Yes Yes Yes Yes\n\nAccuracy of results of data \nanalytics\n\nNo Yes No No No\n\n\n\nPage 9 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\nResults and discussions\nAs part of systematic literature review, it has been observed that all existing mecha-\nnisms of privacy preservation are with respect to structured data. More than 80% of data \nbeing generated today is unstructured [29]. As such, there is a need to address following \nchallenges.\n\ni. Develop concrete solution to protect privacy in both structured and unstructured \ndata.\n\nii. Scalable and robust techniques to be developed to handle large scale heterogeneous \ndata sets.\n\niii. Data should be allowed to stay in its native form without need for transformation \nand data analytics can be carried out while ensuring privacy preservation.\n\niv. New techniques apart from Anonymization must be developed to ensure protection \nagainst key privacy threats which include identity disclosure, discrimination, surveil-\nlance etc.\n\nv. Maximizing data utility while ensuring data privacy.\n\nConclusion\nNo concrete solution for unstructured data has been developed yet. Conventional \ndata mining algorithms can be applied for classification and clustering problems but \ncannot be used in privacy preservation especially when dealing with person specific \ninformation. Machine learning and soft computing techniques can be used to develop \nnew and more appropriate solution to privacy problems which include identity dis-\nclosure that can lead to personal embarrassment and abuse.\n\nThere is a strong need for law enforcement by governments of all countries to \nensure individual privacy. European Union [30] is making an attempt to enforce pri-\nvacy preservation law. Apart from technological solutions, there is a strong need to \ncreate awareness among the people regarding privacy hazards to safeguard them-\nselves form privacy breaches. One of the serious privacy threats is smart phone. Lot \nof personal information in the form of contacts, messages, chats and files are being \naccessed by many apps running in our smart phone without our knowledge. Most \nof the time people do not even read the privacy statement before installing any app. \nHence there is a strong need to educate people on the various vulnerabilities which \ncan contribute to leakage of private information.\n\nWe propose a novel privacy preservation model based on Data Lake concept to \nhold variety of data from diverse sources. Data lake is a repository to hold data from \ndiverse sources in their raw format [31, 32]. Data ingestion from variety of sources can \nbe done using Apache Flume and an intelligent algorithm based on machine learning \ncan be applied to identify sensitive attributes dynamically [33, 34]. The algorithm will \nbe trained with existing data sets with known sensitive attributes and rigorous train-\ning of the model will help in predicting the sensitive attributes in a given data set [35]. \nAccuracy of the model can be improved by adding more layers of training leading \nto deep learning techniques [36]. Advanced computing techniques like Apache Spark \ncan be used in implementing privacy preserving algorithms which is a distributed \n\n\n\nPage 10 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\nmassive parallel computing with in memory processing to ensure very fast processing \n[37]. The proposed model is shown in Fig. 3.\n\nData analytics is done on the data collected from various sources. If an ecommerce \nsite would like to perform data analytics, they need transactional data, website logs and \ncustomers opinion through social media pages. A Data lake is used to collect data from \ndifferent sources. Apache Flume is used to ingest data from social media sites, website \nlogs into Hadoop Distributed File System(HDFS). Using SQOOP relational data can be \nloaded into HDFS.\n\nIn Data lake the data can remain in its native form which is either structured or \nunstructured. When data has to be processed, it can be transformed into HIVE tables. A \nHadoop map reduce job using machine learning can be executed on the data to classify \nthe sensitive attributes [38]. The data can be vertically distributed to separate the sensi-\ntive attributes from rest of the data and apply tokenization to map the vertically distrib-\nuted data. The data without any sensitive attributes can be published for data analytics.\n\nAbbreviations\nCCTV: closed circuit television; MDSBA: Multidimensional Sensitivity Based Anonymization.\n\nAuthors’ contributions\nPRMR: as part of Ph.D. work I have done my literature survey and submitted my work in the form of a paper. SMK: \nsupported me in compiling the paper. APSK: suggested necessary amendments and helped in revising the paper. All \nauthors read and approved the final manuscript.\n\nAuthor details\n1 Department of Computer Science and Engineering, MLR Institute of Technology, Hyderabad, India. 2 Department \nof Computer Science and Engineering, Sri Venkateswara College of Engineering, Tirupati, Andhra Pradesh, India. \n3 Department of Computer Science and Engineering, JNTU Anantapur, Anantapuramu, Andhra Pradesh, India. 4 JNTU \nAnantapur, Anantapur, Andhra Pradesh, India. \n\nAcknowledgements\nI would like to thank my guides, for supporting my work and for suggesting necessary corrections.\n\nData Lake\n\nSqoop to load data from RDBMS\n\nApache \nFlume \nto load \nsocial \nmedia \ndata\n\nLoad data from\ndifferent sources\nand varie�es into\nHive Table for\nprocessing\n\nHadoop\nMap\nReduce\nJob to\nclassify\nsensi�ve\ndata\n\nNovel Privacy \nPreserva�on \nalgorithm \nbased on \nver�cal \ndistribu�on and \ntokeniza�on\n\nFig. 3 A Novel privacy preservation model based on vertical distribution and tokenization\n\n\n\nPage 11 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\nCompeting interests\nThe authors declare that they have no competing interests.\n\nAvailability of data and materials\nIf any one is interested in our work, we are ready to provide more details of the map reduce job which we have \nexecuted and the data processing techniques applied. However the data is used in our work, is freely available in many \nrepositories.\n\nFunding\nNo Funding.\n\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nReceived: 21 March 2018 Accepted: 4 September 2018\n\nReferences\n 1. Ducange Pietro, Pecori Riccardo, Mezzina Paolo. A glimpse on big data analytics in the framework of marketing \n\nstrategies. Soft Comput. 2018;22(1):325–42.\n 2. Chauhan Arun, Kummamuru Krishna, Toshniwal Durga. Prediction of places of visit using tweets. Knowl Inf Syst. \n\n2017;50(1):145–66.\n 3. Yang D, Bingqing Q, Cudre-Mauroux P. Privacy-preserving social media data publishing for personalized ranking-\n\nbased recommendation. IEEE Trans Knowl Data Eng. 2018. ISSN (Print):1041-4347, ISSN (Electronic):1558-2191.\n 4. Liu Y et al. A practical privacy-preserving data aggregation (3PDA) scheme for smart grid. IEEE Trans Ind Inf. 2018.\n 5. Duncan GT et al. Disclosure limitation methods and information loss for tabular data. In: Confidentiality, disclosure \n\nand data access: theory and practical applications for statistical agencies. 2001. p. 135–166.\n 6. Duncan GT, Diane L. Disclosure-limited data dissemination. J Am Stat Assoc. 1986;81(393):10–8.\n 7. Lambert Diane. Measures of disclosure risk and harm. J Off Stat. 1993;9(2):313.\n 8. Spiller K, et al. Data privacy: users’ thoughts on quantified self personal data. Self-Tracking. Cham: Palgrave Macmil-\n\nlan; 2018. p. 111–24.\n 9. Hettig M, Kiss E, Kassel J-F, Weber S, Harbach M. Visualizing risk by example: demonstrating threats arising from \n\nandroid apps. In: Smith M, editor. Symposium on usable privacy and security (SOUPS), Newcastle, UK, July 24–26, \n2013.\n\n 10. Bayardo RJ, Agrawal A. Data privacy through optimal k-anonymization. In: Proceedings 21st international confer-\nence on data engineering, 2005 (ICDE 2005). Piscataway: IEEE; 2005.\n\n 11. Iyengar S. Transforming data to satisfy privacy constraints. In: Proceedings of the eighth ACM SIGKDD international \nconference on knowledge discovery and data mining. New York: ACM; 2002.\n\n 12. LeFevre K, DeWitt DJ, Ramakrishnan R. Incognito: efficient full-domain k-anonymity. In: Proceedings of the 2005 \nACM SIGMOD international conference on management of data. New York: ACM; 2005.\n\n 13. LeFevre K, DeWitt DJ, Ramakrishnan R. Mondrian multidimensional k-anonymity. In: Proceedings of the 22nd inter-\nnational conference (ICDE’06) on data engineering, 2006. New York: ACM; 2006.\n\n 14. Samarati, Pierangela, and Latanya Sweeney. In: Protecting privacy when disclosing information: k-anonymity and its \nenforcement through generalization and suppression. Technical report, SRI International, 1998.\n\n 15. Sweeney Latanya. Achieving k-anonymity privacy protection using generalization and suppression. In J Uncertain \nFuzziness Knowl Based Syst. 2002;10(05):571–88.\n\n 16. Sweeney Latanya. k-Anonymity: a model for protecting privacy. Int J Uncertain, Fuzziness Knowl Based Syst. \n2002;10(05):557–70.\n\n 17. Williams R. On the complexity of optimal k-anonymity. In: Proc. 23rd ACM SIGMOD-SIGACT-SIGART symp. principles \nof database systems (PODS). New York: ACM; 2004.\n\n 18. Machanavajjhala A et al. L-diversity: privacy beyond k-anonymity. In: Proceedings of the 22nd international confer-\nence on data engineering (ICDE’06), 2006. Piscataway: IEEE; 2006.\n\n 19. Xiao X, Yufei T. Personalized privacy preservation. In: Proceedings of the 2006 ACM SIGMOD international confer-\nence on Management of data. New York: ACM; 2006.\n\n 20. Rubner Y, Tomasi T, Guibas LJ. The earth mover’s distance as a metric for image retrieval. Int J Comput Vision. \n2000;40(2):99–121.\n\n 21. Aggarwal CC, Philip SY. A general survey of privacy-preserving data mining models and algorithms. Privacy-preserv-\ning data mining. Springer: US; 2008. p. 11–52.\n\n 22. Jiang R, Lu R, Choo KK. Achieving high performance and privacy-preserving query over encrypted multidimensional \nbig metering data. Future Gen Comput Syst. 2018;78:392–401.\n\n 23. Wang K, Yu PS, Chakraborty S. Bottom-up generalization: A data mining solution to privacy protection. In: Fourth \nIEEE international conference on data mining, 2004 (ICDM’04). Piscataway: IEEE; 2004.\n\n 24. Fung BCM, Wang K, Yu PS. Top-down specialization for information and privacy preservation. In: Proceedings 21st \ninternational conference on data engineering, 2005 (ICDE 2005). Piscataway: IEEE; 2005.\n\n 25. Zhang X et al. A MapReduce based approach of scalable multidimensional anonymization for big data privacy \npreservation on cloud. In: Third international conference on cloud and green computing (CGC), 2013. Piscataway: \nIEEE; 2013.\n\n Published online: 22 September 2018 \n\n\n\nPage 12 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\n 26. Zhang X, et al. A scalable two-phase top-down specialization approach for data anonymization using mapreduce \non cloud. IEEE Trans Parallel Distrib Syst. 2014;25(2):363–73.\n\n 27. Al-Zobbi M, Shahrestani S, Ruan C. Improving MapReduce privacy by implementing multi-dimensional sensitivity-\nbased anonymization. J Big Data. 2017;4(1):45.\n\n 28. Al-Zobbi M, Shahrestani S, Ruan C. Implementing a framework for big data anonymity and analytics access control. \nIn: Trustcom/BigDataSE/ICESS, 2017 IEEE. Piscataway: IEEE; 2017.\n\n 29. Schneider C. IBM Blogs; 2016. https ://www.ibm.com/blogs /watso n/2016/05/bigge st-data-chall enges -might \n-not-even-know/.\n\n 30. TCS. Emphasizing the need for government regulations on data privacy; 2016. https ://www.tcs.com/conte nt/dam/\ntcs/pdf/techn ologi es/Cyber -Secur ity/Abstr act/Stren gthen ing-Priva cy-Pro", "text": [ "", "Site 1 Site 2 1 to 10K 10K to 20 sales K sales records records Site 3 20K to 30K sales records", "Health Department Data under Personal Bank custodian of (email, social different network organizations accounts etc) Employer site", "Published online: 22 September 2018" ], "layoutText": [ "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}", "{\"language\":\"en\",\"text\":\"Site 1 Site 2 1 to 10K 10K to 20 sales K sales records records Site 3 20K to 30K sales records\",\"lines\":[{\"boundingBox\":[{\"x\":133,\"y\":145},{\"x\":246,\"y\":144},{\"x\":246,\"y\":185},{\"x\":133,\"y\":185}],\"text\":\"Site 1\"},{\"boundingBox\":[{\"x\":436,\"y\":132},{\"x\":552,\"y\":132},{\"x\":551,\"y\":172},{\"x\":435,\"y\":172}],\"text\":\"Site 2\"},{\"boundingBox\":[{\"x\":105,\"y\":218},{\"x\":283,\"y\":216},{\"x\":283,\"y\":258},{\"x\":106,\"y\":260}],\"text\":\"1 to 10K\"},{\"boundingBox\":[{\"x\":394,\"y\":205},{\"x\":597,\"y\":205},{\"x\":597,\"y\":248},{\"x\":394,\"y\":248}],\"text\":\"10K to 20\"},{\"boundingBox\":[{\"x\":141,\"y\":274},{\"x\":244,\"y\":271},{\"x\":244,\"y\":310},{\"x\":141,\"y\":312}],\"text\":\"sales\"},{\"boundingBox\":[{\"x\":415,\"y\":258},{\"x\":569,\"y\":259},{\"x\":569,\"y\":300},{\"x\":415,\"y\":300}],\"text\":\"K sales\"},{\"boundingBox\":[{\"x\":113,\"y\":327},{\"x\":268,\"y\":325},{\"x\":269,\"y\":364},{\"x\":113,\"y\":366}],\"text\":\"records\"},{\"boundingBox\":[{\"x\":414,\"y\":313},{\"x\":572,\"y\":311},{\"x\":572,\"y\":352},{\"x\":414,\"y\":353}],\"text\":\"records\"},{\"boundingBox\":[{\"x\":290,\"y\":463},{\"x\":404,\"y\":462},{\"x\":405,\"y\":503},{\"x\":290,\"y\":504}],\"text\":\"Site 3\"},{\"boundingBox\":[{\"x\":224,\"y\":536},{\"x\":467,\"y\":535},{\"x\":468,\"y\":578},{\"x\":225,\"y\":578}],\"text\":\"20K to 30K\"},{\"boundingBox\":[{\"x\":215,\"y\":591},{\"x\":480,\"y\":589},{\"x\":481,\"y\":631},{\"x\":215,\"y\":632}],\"text\":\"sales records\"}],\"words\":[{\"boundingBox\":[{\"x\":133,\"y\":144},{\"x\":218,\"y\":144},{\"x\":218,\"y\":185},{\"x\":133,\"y\":185}],\"text\":\"Site\"},{\"boundingBox\":[{\"x\":226,\"y\":144},{\"x\":245,\"y\":144},{\"x\":245,\"y\":185},{\"x\":226,\"y\":185}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":435,\"y\":132},{\"x\":515,\"y\":132},{\"x\":515,\"y\":172},{\"x\":435,\"y\":172}],\"text\":\"Site\"},{\"boundingBox\":[{\"x\":523,\"y\":132},{\"x\":550,\"y\":132},{\"x\":550,\"y\":172},{\"x\":523,\"y\":172}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":106,\"y\":219},{\"x\":127,\"y\":219},{\"x\":128,\"y\":260},{\"x\":107,\"y\":260}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":135,\"y\":219},{\"x\":186,\"y\":219},{\"x\":187,\"y\":260},{\"x\":136,\"y\":260}],\"text\":\"to\"},{\"boundingBox\":[{\"x\":194,\"y\":219},{\"x\":283,\"y\":217},{\"x\":283,\"y\":259},{\"x\":195,\"y\":260}],\"text\":\"10K\"},{\"boundingBox\":[{\"x\":395,\"y\":206},{\"x\":479,\"y\":206},{\"x\":479,\"y\":249},{\"x\":394,\"y\":249}],\"text\":\"10K\"},{\"boundingBox\":[{\"x\":487,\"y\":206},{\"x\":535,\"y\":206},{\"x\":535,\"y\":249},{\"x\":487,\"y\":249}],\"text\":\"to\"},{\"boundingBox\":[{\"x\":543,\"y\":206},{\"x\":596,\"y\":205},{\"x\":597,\"y\":249},{\"x\":543,\"y\":249}],\"text\":\"20\"},{\"boundingBox\":[{\"x\":141,\"y\":273},{\"x\":244,\"y\":271},{\"x\":245,\"y\":309},{\"x\":141,\"y\":312}],\"text\":\"sales\"},{\"boundingBox\":[{\"x\":416,\"y\":259},{\"x\":455,\"y\":259},{\"x\":454,\"y\":301},{\"x\":415,\"y\":301}],\"text\":\"K\"},{\"boundingBox\":[{\"x\":463,\"y\":259},{\"x\":568,\"y\":260},{\"x\":568,\"y\":301},{\"x\":463,\"y\":301}],\"text\":\"sales\"},{\"boundingBox\":[{\"x\":114,\"y\":329},{\"x\":267,\"y\":325},{\"x\":267,\"y\":365},{\"x\":113,\"y\":366}],\"text\":\"records\"},{\"boundingBox\":[{\"x\":416,\"y\":315},{\"x\":571,\"y\":312},{\"x\":571,\"y\":353},{\"x\":414,\"y\":353}],\"text\":\"records\"},{\"boundingBox\":[{\"x\":290,\"y\":462},{\"x\":368,\"y\":462},{\"x\":368,\"y\":503},{\"x\":290,\"y\":504}],\"text\":\"Site\"},{\"boundingBox\":[{\"x\":376,\"y\":462},{\"x\":404,\"y\":462},{\"x\":405,\"y\":503},{\"x\":377,\"y\":503}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":225,\"y\":536},{\"x\":312,\"y\":536},{\"x\":312,\"y\":579},{\"x\":225,\"y\":579}],\"text\":\"20K\"},{\"boundingBox\":[{\"x\":321,\"y\":536},{\"x\":368,\"y\":536},{\"x\":368,\"y\":579},{\"x\":320,\"y\":579}],\"text\":\"to\"},{\"boundingBox\":[{\"x\":377,\"y\":536},{\"x\":466,\"y\":536},{\"x\":466,\"y\":578},{\"x\":376,\"y\":579}],\"text\":\"30K\"},{\"boundingBox\":[{\"x\":215,\"y\":593},{\"x\":314,\"y\":592},{\"x\":314,\"y\":632},{\"x\":216,\"y\":632}],\"text\":\"sales\"},{\"boundingBox\":[{\"x\":322,\"y\":592},{\"x\":480,\"y\":589},{\"x\":478,\"y\":632},{\"x\":322,\"y\":632}],\"text\":\"records\"}]}", "{\"language\":\"en\",\"text\":\"Health Department Data under Personal Bank custodian of (email, social different network organizations accounts etc) Employer site\",\"lines\":[{\"boundingBox\":[{\"x\":454,\"y\":75},{\"x\":575,\"y\":75},{\"x\":575,\"y\":111},{\"x\":454,\"y\":112}],\"text\":\"Health\"},{\"boundingBox\":[{\"x\":410,\"y\":122},{\"x\":618,\"y\":123},{\"x\":617,\"y\":162},{\"x\":410,\"y\":161}],\"text\":\"Department\"},{\"boundingBox\":[{\"x\":422,\"y\":331},{\"x\":614,\"y\":330},{\"x\":614,\"y\":368},{\"x\":422,\"y\":369}],\"text\":\"Data under\"},{\"boundingBox\":[{\"x\":827,\"y\":329},{\"x\":964,\"y\":328},{\"x\":965,\"y\":364},{\"x\":827,\"y\":366}],\"text\":\"Personal\"},{\"boundingBox\":[{\"x\":98,\"y\":397},{\"x\":205,\"y\":397},{\"x\":206,\"y\":436},{\"x\":98,\"y\":437}],\"text\":\"Bank\"},{\"boundingBox\":[{\"x\":408,\"y\":376},{\"x\":625,\"y\":375},{\"x\":626,\"y\":412},{\"x\":408,\"y\":414}],\"text\":\"custodian of\"},{\"boundingBox\":[{\"x\":790,\"y\":387},{\"x\":1005,\"y\":386},{\"x\":1005,\"y\":421},{\"x\":790,\"y\":423}],\"text\":\"(email, social\"},{\"boundingBox\":[{\"x\":441,\"y\":418},{\"x\":590,\"y\":418},{\"x\":591,\"y\":455},{\"x\":441,\"y\":455}],\"text\":\"different\"},{\"boundingBox\":[{\"x\":831,\"y\":431},{\"x\":963,\"y\":430},{\"x\":964,\"y\":460},{\"x\":831,\"y\":461}],\"text\":\"network\"},{\"boundingBox\":[{\"x\":399,\"y\":467},{\"x\":630,\"y\":464},{\"x\":630,\"y\":503},{\"x\":400,\"y\":507}],\"text\":\"organizations\"},{\"boundingBox\":[{\"x\":792,\"y\":472},{\"x\":1000,\"y\":470},{\"x\":1000,\"y\":501},{\"x\":793,\"y\":502}],\"text\":\"accounts etc)\"},{\"boundingBox\":[{\"x\":415,\"y\":669},{\"x\":613,\"y\":671},{\"x\":613,\"y\":717},{\"x\":414,\"y\":715}],\"text\":\"Employer\"},{\"boundingBox\":[{\"x\":480,\"y\":727},{\"x\":550,\"y\":726},{\"x\":549,\"y\":757},{\"x\":480,\"y\":759}],\"text\":\"site\"}],\"words\":[{\"boundingBox\":[{\"x\":455,\"y\":76},{\"x\":574,\"y\":76},{\"x\":574,\"y\":112},{\"x\":454,\"y\":113}],\"text\":\"Health\"},{\"boundingBox\":[{\"x\":411,\"y\":122},{\"x\":618,\"y\":123},{\"x\":618,\"y\":162},{\"x\":410,\"y\":161}],\"text\":\"Department\"},{\"boundingBox\":[{\"x\":423,\"y\":332},{\"x\":504,\"y\":333},{\"x\":504,\"y\":369},{\"x\":423,\"y\":369}],\"text\":\"Data\"},{\"boundingBox\":[{\"x\":511,\"y\":333},{\"x\":613,\"y\":331},{\"x\":614,\"y\":369},{\"x\":511,\"y\":369}],\"text\":\"under\"},{\"boundingBox\":[{\"x\":829,\"y\":329},{\"x\":964,\"y\":328},{\"x\":964,\"y\":365},{\"x\":828,\"y\":366}],\"text\":\"Personal\"},{\"boundingBox\":[{\"x\":98,\"y\":397},{\"x\":204,\"y\":397},{\"x\":205,\"y\":437},{\"x\":98,\"y\":437}],\"text\":\"Bank\"},{\"boundingBox\":[{\"x\":408,\"y\":377},{\"x\":573,\"y\":376},{\"x\":573,\"y\":414},{\"x\":408,\"y\":415}],\"text\":\"custodian\"},{\"boundingBox\":[{\"x\":580,\"y\":376},{\"x\":626,\"y\":375},{\"x\":627,\"y\":413},{\"x\":581,\"y\":414}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":790,\"y\":387},{\"x\":899,\"y\":387},{\"x\":900,\"y\":422},{\"x\":792,\"y\":424}],\"text\":\"(email,\"},{\"boundingBox\":[{\"x\":906,\"y\":387},{\"x\":1005,\"y\":386},{\"x\":1005,\"y\":421},{\"x\":907,\"y\":422}],\"text\":\"social\"},{\"boundingBox\":[{\"x\":442,\"y\":418},{\"x\":589,\"y\":419},{\"x\":589,\"y\":456},{\"x\":441,\"y\":456}],\"text\":\"different\"},{\"boundingBox\":[{\"x\":831,\"y\":432},{\"x\":963,\"y\":430},{\"x\":963,\"y\":461},{\"x\":832,\"y\":461}],\"text\":\"network\"},{\"boundingBox\":[{\"x\":400,\"y\":467},{\"x\":631,\"y\":464},{\"x\":631,\"y\":503},{\"x\":400,\"y\":508}],\"text\":\"organizations\"},{\"boundingBox\":[{\"x\":793,\"y\":474},{\"x\":933,\"y\":472},{\"x\":934,\"y\":502},{\"x\":793,\"y\":502}],\"text\":\"accounts\"},{\"boundingBox\":[{\"x\":939,\"y\":472},{\"x\":1000,\"y\":471},{\"x\":1000,\"y\":502},{\"x\":939,\"y\":502}],\"text\":\"etc)\"},{\"boundingBox\":[{\"x\":415,\"y\":669},{\"x\":614,\"y\":672},{\"x\":613,\"y\":716},{\"x\":415,\"y\":716}],\"text\":\"Employer\"},{\"boundingBox\":[{\"x\":480,\"y\":727},{\"x\":548,\"y\":726},{\"x\":548,\"y\":758},{\"x\":480,\"y\":759}],\"text\":\"site\"}]}", "{\"language\":\"en\",\"text\":\"Published online: 22 September 2018\",\"lines\":[{\"boundingBox\":[{\"x\":0,\"y\":15},{\"x\":1069,\"y\":16},{\"x\":1069,\"y\":74},{\"x\":0,\"y\":72}],\"text\":\"Published online: 22 September 2018\"}],\"words\":[{\"boundingBox\":[{\"x\":1,\"y\":18},{\"x\":285,\"y\":16},{\"x\":285,\"y\":72},{\"x\":0,\"y\":66}],\"text\":\"Published\"},{\"boundingBox\":[{\"x\":295,\"y\":16},{\"x\":500,\"y\":16},{\"x\":500,\"y\":74},{\"x\":294,\"y\":72}],\"text\":\"online:\"},{\"boundingBox\":[{\"x\":509,\"y\":16},{\"x\":592,\"y\":16},{\"x\":592,\"y\":74},{\"x\":510,\"y\":74}],\"text\":\"22\"},{\"boundingBox\":[{\"x\":601,\"y\":16},{\"x\":911,\"y\":17},{\"x\":912,\"y\":73},{\"x\":602,\"y\":74}],\"text\":\"September\"},{\"boundingBox\":[{\"x\":920,\"y\":17},{\"x\":1069,\"y\":18},{\"x\":1070,\"y\":70},{\"x\":922,\"y\":72}],\"text\":\"2018\"}]}" ] }, { "@search.score": 2.0799417, "content": "\nBig data stream analysis: a systematic \nliterature review\nTaiwo Kolajo1,2* , Olawande Daramola3 and Ayodele Adebiyi1,4 \n\nIntroduction\nAdvances in information technology have facilitated large volume, high-velocity of data, \nand the ability to store data continuously leading to several computational challenges. \nDue to the nature of big data in terms of volume, velocity, variety, variability, veracity, \nvolatility, and value [1] that are being generated recently, big data computing is a new \ntrend for future computing.\n\nBig data computing can be generally categorized into two types based on the process-\ning requirements, which are big data batch computing and big data stream computing \n\nAbstract \n\nRecently, big data streams have become ubiquitous due to the fact that a number of \napplications generate a huge amount of data at a great velocity. This made it difficult \nfor existing data mining tools, technologies, methods, and techniques to be applied \ndirectly on big data streams due to the inherent dynamic characteristics of big data. In \nthis paper, a systematic review of big data streams analysis which employed a rigorous \nand methodical approach to look at the trends of big data stream tools and technolo-\ngies as well as methods and techniques employed in analysing big data streams. It \nprovides a global view of big data stream tools and technologies and its comparisons. \nThree major databases, Scopus, ScienceDirect and EBSCO, which indexes journals and \nconferences that are promoted by entities such as IEEE, ACM, SpringerLink, and Elsevier \nwere explored as data sources. Out of the initial 2295 papers that resulted from the \nfirst search string, 47 papers were found to be relevant to our research questions after \nimplementing the inclusion and exclusion criteria. The study found that scalability, \nprivacy and load balancing issues as well as empirical analysis of big data streams and \ntechnologies are still open for further research efforts. We also found that although, sig-\nnificant research efforts have been directed to real-time analysis of big data stream not \nmuch attention has been given to the preprocessing stage of big data streams. Only a \nfew big data streaming tools and technologies can do all of the batch, streaming, and \niterative jobs; there seems to be no big data tool and technology that offers all the key \nfeatures required for now and standard benchmark dataset for big data streaming ana-\nlytics has not been widely adopted. In conclusion, it was recommended that research \nefforts should be geared towards developing scalable frameworks and algorithms that \nwill accommodate data stream computing mode, effective resource allocation strategy \nand parallelization issues to cope with the ever-growing size and complexity of data.\n\nKeywords: Big data stream analysis, Stream computing, Big data streaming tools and \ntechnologies\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nSURVEY PAPER\n\nKolajo et al. J Big Data (2019) 6:47 \nhttps://doi.org/10.1186/s40537-019-0210-7\n\n*Correspondence: \ntaiwo.kolajo@stu.cu.edu.ng; \ntaiwo.kolajo@fulokoja.edu.ng \n1 Department of Computer \nand Information Sciences, \nCovenant University, Ota, \nNigeria\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0001-6780-2495\nhttp://orcid.org/0000-0001-6340-078X\nhttp://orcid.org/0000-0002-3114-6315\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-019-0210-7&domain=pdf\n\n\nPage 2 of 30Kolajo et al. J Big Data (2019) 6:47 \n\n[2]. Big data batch processing is not sufficient when it comes to analysing real-time \napplication scenarios. Most of the data generated in a real-time data stream need real-\ntime data analysis. In addition, the output must be generated with low-latency and any \nincoming data must be reflected in the newly generated output within seconds. This \nnecessitates big data stream analysis [3].\n\nThe demand for stream processing is increasing. The reason being not only that huge \nvolume of data need to be processed but that data must be speedily processed so that \norganisations or businesses can react to changing conditions in real-time.\n\nThis paper presents a systematic review of big data stream analysis. The purpose is to \npresent an overview of research works, findings, as well as implications for research and \npractice. This is necessary to (1) provide an update about the state of research, (2) iden-\ntify areas that are well researched, (3) showcase areas that are lacking and need further \nresearch, and (4) build a common understanding of the challenges that exist for the ben-\nefit of the scientific community.\n\nThe rest of the paper is organized as follows: “Background and related work” section \nprovides information on stream computing and big data stream analysis and the key \nissues involved in it and presents a review on big data streaming analytics. In “Research \nmethod” section, the adopted research methodology is discussed, while “Result” section \npresents the findings of the study. “Discussion” section presents a detailed evaluation \nperformed on big data stream analysis, “Limitation of the review” section highlights the \nlimitations of the study, while “Conclusion and further work” concludes the paper.\n\nBackground and related work\nStream computing\n\nStream computing refers to the processing of massive amount of data generated at high-\nvelocity from multiple sources with low latency in real-time. It is a new paradigm neces-\nsitated because of new sources of data generating scenarios which include ubiquity of \nlocation services, mobile devices, and sensor pervasiveness [4]. It can be applied to the \nhigh-velocity flow of data from real-time sources such as the Internet of Things, Sensors, \nmarket data, mobile, and clickstream.\n\nThe fundamental assumption of this paradigm is that the potential value of data lies in \nits freshness. As a result, data are analysed as soon as they arrive in a stream to produce \nresult as opposed to what obtains in batch computing where data are first stored before \nthey are analysed. There is a crucial need for parallel architectures and scalable com-\nputing platforms [5]. With stream computing, organisations can analyse and respond in \nreal-time to rapidly changing data. Streaming processing frameworks include Storm, S4, \nKafka, and Spark [6–8]. The real contrasts between the batch processing and the stream \nprocessing paradigms are outlined in Table 1.\n\nIncorporating streaming data into decision-making process necessitates a program-\nming paradigm called stream computing. With stream computing, fairly static questions \ncan be evaluated on data in motion (i.e. real-time data) continuously [9].\n\nBig data stream analysis\n\nThe essence of big data streaming analytics is the need to analyse and respond to real-\ntime streaming data using continuous queries so that it is possible to continuously \n\n\n\nPage 3 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nperform analysis on the fly within the stream. Stream processing solutions must be \nable to handle a real-time, high volume of data from diverse sources putting into con-\nsideration availability, scalability and fault tolerance. Big data stream analysis involves \nassimilation of data as an infinite tuple, analysis and production of actionable results \nusually in a form of stream [10].\n\nIn a stream processor, applications are represented as data flow graph made up of \noperations and interconnected streams as depicted in Fig. 1. In a streaming analytics \nsystem, application comes in a form of continuous queries, data are ingested continu-\nously, analysed and correlated, and stream of results are generated. Streaming analytic \napplications is usually a set of operators connected by streams. Streaming analytics \nsystems must be able to identify new information, incrementally build models and \naccess whether the new incoming data deviate from model predictions [9].\n\nThe idea of streaming analytics is that each of the received data tuples is processed \nin the data processing node. Such processing includes removing duplicates, filling \nmissing data, data normalization, parsing, feature extraction, which are typically done \nin a single pass due to the high data rates of external feeds. When a new tuple arrives, \nthis node is triggered, and it expels tuples older than the time specified in the sliding \nwindow (sliding window is a typical example of windows used in stream computing \nwhich keeps only the latest tuples up to the time specified in the windows). A window \n\nTable 1 Comparison between batch processing and streaming processing [82]\n\nDimension Batch processing Streaming processing\n\nInput Data chunks Stream of new data or updates\n\nData size Known and finite Infinite or unknown in advance\n\nHardware Multiple CPUs Typical single limited amount of memory\n\nStorage Store Not store or store non-trivial portion in memory\n\nProcessing Processed in multiple rounds A single or few passes over data\n\nTime Much longer A few seconds or even milliseconds\n\nApplications Widely adopted in almost every domain Web mining, traffic monitoring, sensor networks\n\nFig. 1 Data flow graph of a stream processor. The figure shows how applications (made up of operations and \ninterconnected streams) are represented as data flow graph in a stream processor [10]\n\n\n\n\n\nPage 4 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nis referred to as a logical container for data tuples received. It defines how frequently \ndata is refreshed in the container as well as when data processing is triggered [4].\n\nKey issues in big data stream analysis\n\nBig data stream analysis is relevant when there is a need to obtain useful knowledge \nfrom current happenings in an efficient and speedy manner in order to enable organisa-\ntions to quickly react to problems, or detect new trends which can help improve their \nperformance. However, there are some challenges such as scalability, integration, fault-\ntolerance, timeliness, consistency, heterogeneity and incompleteness, load balancing, \nprivacy issues, and accuracy [3, 11–18] which arises from the nature of big data streams \nthat must be dealt with.\n\nScalability\n\nOne of the main challenges in big data streaming analysis is the issue of scalability. The \nbig data stream is experiencing exponential growth in a way much faster than computer \nresources. The processors follow Moore’s law, but the size of data is exploding. There-\nfore, research efforts should be geared towards developing scalable frameworks and \nalgorithms that will accommodate data stream computing mode, effective resource allo-\ncation strategy and parallelization issues to cope with the ever-growing size and com-\nplexity of data.\n\nIntegration\n\nBuilding a distributed system where each node has a view of the data flow, that is, every \nnode performing analysis with a small number of sources, then aggregating these views \nto build a global view is non-trivial. An integration technique should be designed to ena-\nble efficient operations across different datasets.\n\nFault‑tolerance\n\nHigh fault-tolerance is required in life-critical systems. As data is real-time and infinite \nin big data stream computing environments, a good scalable high fault-tolerance strat-\negy is required that allows an application to continue working despite component failure \nwithout interruption.\n\nTimeliness\n\nTime is of the essence for time-sensitive processes such as mitigating security threats, \nthwarting fraud, or responding to a natural disaster. There is a need for scalable architec-\ntures or platforms that will enable continuous processing of data streams which can be \nused to maximize the timeliness of data. The main challenge is implementing a distrib-\nuted architecture that will aggregate local views of data into global view with minimal \nlatency between communicating nodes.\n\nConsistency\n\nAchieving high consistency (i.e. stability) in big data stream computing environments is \nnon-trivial as it is difficult to determine which data are needed and which nodes should \nbe consistent. Hence a good system structure is required.\n\n\n\nPage 5 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nHeterogeneity and incompleteness\n\nBig data streams are heterogeneous in structure, organisations, semantics, accessi-\nbility and granularity. The challenge here is how to handle an always ever-increas-\ning data, extract meaningful content out of it, aggregate and correlate streaming \ndata from multiple sources in real-time. A competent data presentation should be \ndesigned to reflect the structure, diversity and hierarchy of the streaming data.\n\nLoad balancing\n\nA big data stream computing system is expected to be self-adaptive to data streams \nchanges and avoid load shedding. This is challenging as dedicating resources to cover \npeak loads 24/7 is impossible and load shedding is not feasible when the variance \nbetween the average load and the peak load is high. As a result, a distributing envi-\nronment that automatically streams partial data streams to a global centre when local \nresources become insufficient is required.\n\nHigh throughput\n\nDecision with respect to identifying the sub-graph that needs replication, how many \nreplicas are needed and the portion of the data stream to assign to each replica is an \nissue in big data stream computing environment. There is a need for good multiple \ninstances replication if high throughput is to be achieved.\n\nPrivacy\n\nBig data stream analytics created opportunities for analyzing a huge amount of data \nin real-time but also created a big threat to individual privacy. According to the Inter-\nnational Data Cooperation (IDC), not more than half of the entire information that \nneeds protection is effectively protected. The main challenge is proposing techniques \nfor protecting a big data stream dataset before its analysis.\n\nAccuracy\n\nOne of the main objectives of big data stream analysis is to develop effective tech-\nniques that can accurately predict future observations. However, as a result of inher-\nent characteristics of big data such as volume, velocity, variety, variability, veracity, \nvolatility, and value, big data analysis strongly constrain processing algorithms spatio-\ntemporally and hence stream-specific requirements must be taken into consideration \nto ensure high accuracy.\n\nRelated work\n\nThis section discusses some of the previous research efforts that relate to big data \nstreaming analytics.\n\nThe work of [13] presented a review of various tools, technologies and methods \nfor big data analytics by categorizing big data analytics literature according to their \nresearch focus. This paper is different in that it presents a systematic literature review \nthat focused on big data “streaming” analytics.\n\n\n\nPage 6 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nAuthors in [19] presented a systematic review of big data analytics in e-commerce. The \nstudy explored characteristics, definitions, business values, types and challenges of big \ndata analytics in the e-commerce landscape. Likewise, [20] conducted a study that is cen-\ntred on big data analytics in technology and organisational resource management specifi-\ncally focusing on reviews that present big data challenges and big data analytics methods. \nAlthough they are systematic reviews, the focus is not, particularly on big data streaming.\n\nAuthors in [21] presented the status of empirical research and application areas in big \ndata by employing a systematic mapping method. In the same vein, authors in [22] also \nconducted a survey on big data technologies and machine learning algorithms with a \nparticular focus on anomaly detection. A systematic review of literature which aims to \ndetermine the scope, application, and challenges of big data analytics in healthcare was \npresented by [23]. The work of [2] presented a review of four big data streaming tools \nand technologies. While the study conducted in this paper provided a comprehensive \nreview of not only big data streaming tools and technologies but also methods and tech-\nniques employed in analyzing big data streams. In addition, authors [2] did not provide a \nclear explanation of the methodical approach for selecting the reviewed papers.\n\nResearch method\nThe study was grounded in a systematic literature review of tools and technologies \nwith methods and techniques used in analysing big data streams by adopting [24, 25] as \nmodels.\n\nResearch question\n\nThe study tries to answer the following research questions:\n\nResearch Question 1: What are the tools and technologies employed for big data \nstream analysis?\nResearch Question 2: What methods and techniques are used in analysing big data \nstreams?\nResearch Question 3: What do these tools and technologies have in common and \ntheir differences in terms of concept, purpose and capabilities?\nResearch Question 4: What are the limitations and strengths of these tools and tech-\nnologies?\nResearch Question 5: What are the evaluation techniques or benchmarks used for \nevaluating big data streaming tools and technology?\n\nSearch string\n\nCreating a good search string requires structuring in terms of population, compari-\nson, intervention and outcome [24]. Relevant publications were identified by forming \na search string that combined keywords driven by the research questions earlier stated. \nThe searches were conducted by employing three standard database indexes, which are \nScopus, Science Direct and EBSCOhost. The search string is “big data stream analysis” \nOR “big data stream technologies” OR “big data stream framework” OR “big data stream \nalgorithms” OR “big data stream analysis tools” OR “big data stream processing” OR “big \n\n\n\nPage 7 of 30Kolajo et al. J Big Data (2019) 6:47 \n\ndata stream analysis reviews” OR “big data stream literature review” OR “big data stream \nanalytics”.\n\nData sources\n\nAs research becomes increasingly interdisciplinary, global and collaborative, it is expedi-\nent to select from rich and standard databases. The databases consulted are as follows:\n\n i. Scopus1: Scopus is a bibliographic database containing abstracts and citations for \nacademic journal articles launched in 2004. It covers nearly 36,377 titles from over \n11,678 publishers of which 34,346 are peer-reviewed journals, delivering a compre-\nhensive overview of the world’s research output in the scientific, technical, medi-\ncal, and social sciences (including arts and humanities). It is the largest abstract \nand citation database of peer-reviewed literature.\n\n ii. ScienceDirect2: ScienceDirect is Elsevier’s leading information solution for \nresearchers, students, teachers, information professionals and healthcare profes-\nsionals. It provides both subscription-based and open access-based to a large data-\nbase combining authoritative, full-text scientific, technical and health publications \nwith smart intuitive functionality. It covers over 14 million publications from over \n3800 journals and more than 35,000 books. The journals are grouped into four \ncategories: Life Sciences, Physical Sciences and Engineering, Health Sciences, and \nSocial Sciences and Humanities.\n\n iii. EBSCOhost3: EBSCOhost covers a wide range of bibliographic and full-text data-\nbases for researchers, providing electronic journal service available to both cor-\nporate and academic researchers. It has a total of 16,711 journals and magazine \nindexed and abstracted of which 14,914 are peer-reviewed; more than 900,000 \nhigh-quality e-books and titles and over 60,000 audiobooks from more than 1500 \nmajor academic publishers.\n\n iv. ResearchGate4: A free online professional network for scientists and researchers to \nask and answer questions, share papers and find collaborators. It covers over 100 \nmillion publications from over 11 million researchers. ResearchGate was used as \na secondary source where the authors could not access some papers due to lack of \nsubscription.\n\nData retrieval\n\nThe search was conducted in Scopus, ScienceDirect and EBSCOhost since most of \nthe high impact journals and conferences are indexed in these set of rich databases. \nBoolean ‘OR’ was used in combining the nine (9) search strings. A total of 2295 arti-\ncles from the three databases were retrieved as shown in Table 2.\n\n1 http://www.scopu s.com.\n2 http://www.scien cedir ect.com.\n3 https ://www.ebsco host.com.\n4 https ://www.resea archg ate.net.\n\nhttp://www.scopus.com\nhttp://www.sciencedirect.com\nhttps://www.ebscohost.com\nhttps://www.reseaarchgate.net\n\n\nPage 8 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nFurther refinement was performed by (i) limiting the search to journals and confer-\nence papers; (ii) selecting computer science and IT related as the subject domain; (iii) \nselecting ACM, IEEE, SpringerLink, Elsevier as sources; and year of publication to \nbetween 2004 and 2018. The year range was selected due to the fact that interest in \nbig data stream analysis actually started in 2004. At this stage, a total of 1989 papers \nwere excluded leaving a total of 315 papers (see Table  3). The result of the search \nstring was exported to PDF.\n\nBy going through the title of the papers, 111 seemingly relevant papers were extracted \nexcluding a total number of 213 that were not relevant at this stage (see Table 4).\n\nThe abstracts of 111 papers and introduction (for papers that the abstracts were not \nclear enough) were then read to have a quick overview of the paper and to ascertain \nwhether they are suitable or at variance with the research questions. The citations of \nthe papers were exported to Microsoft Excel for easy analysis. The papers were grouped \ninto three categories; “relevant”, “may be relevant” and “irrelevant”. The “relevant” papers \nwere marked with black colour, “may be relevant” and “irrelevant” with green and red \ncolours respectively. At the end of this stage, 45 papers were classified as “relevant”, 9 \npapers as “may be relevant” and 11 as “irrelevant”. Looking critically at the abstract again, \n18 papers were excluded by using the exclusion criteria leaving a total of 47 papers (see \nTable 5) which were manually reviewed in line with the research questions.\n\nInclusion criteria\n\nPapers published in journals, peer-reviewed conferences, workshops, technical and \nsymposium from 2004 and 2018 were included. In addition, the most recent papers \nwere selected in case of papers with similar investigations and results.\n\nTable 2 First search string result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 2097 65 133 2295\n\nTable 3 Second search string result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 196 27 92 315\n\nTable 4 Third Search string refinement result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 64 23 24 111\n\nTable 5 Final Selection\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 25 10 12 47\n\n\n\nPage 9 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nExclusion criteria\n\nPapers that belong to the following categories were excluded from selection as part of \nthe primary study: (i) papers written in source language other than English; (ii) papers \nwith an abstract and or introduction that does not clearly define the contributions of the \nwork; (iii) papers whose abstract do not relate to big data stream analysis.\n\nResult\nThe findings of the study are now presented with respect to the research questions that \nguided the execution of the systematic literature review.\n\nResearch Question 1: What are the tools and technologies employed for big data stream \n\nanalysis?\n\nBig data stream platforms provide functionalities and features that enable big data \nstream applications to develop, operate, deploy, and manage big data streams. Such \nplatforms must be able to pull in streams of data, process the data and stream it back \nas a single flow. Several tools and technologies have been employed to analyse big data \nstreams. In response to the growing demand for big data streaming analytics, a large \nnumber of alternative big data streaming solutions have been developed both by the \nopen source community and enterprise technology vendors. According to [26], there are \nsome factors to consider when selecting big data streaming tools and technologies in \norder to make effective data management decisions. These are briefly described below.\n\nShape of the data\n\nStreaming data sources require serialization technologies for capturing, storing and rep-\nresenting such high-velocity data. For instance, some tools and technologies allow pro-\njection of different structures across data stores, giving room for flexibility for storage \nand access of data in different ways. However, the performance of such platforms may \nnot be suitable for high-velocity data.\n\nData access\n\nThere is a need to put into consideration how the data will be accessed by users and \napplications. For instance, many NoSQL databases require specific application interfaces \nfor data access. Hence there is a need to consider the integration of some other neces-\nsary tools for data access.\n\nAvailability and consistency requirement\n\nIf a distributed system is needed, then CAP theorem states that consistency and avail-\nability cannot be both guaranteed in the presence of network partition (i.e. when there is \na break in the network). In such a scenario, consistency is often traded off for availability \nto ensure that requests can always be processed.\n\nWorkload profile required\n\nPlatform as a service deployment may be appropriate for a spike load profile platform. \nIf platform distribution can be deployed on Infrastructure as a service cloud, then this \noption may be preferred as users will need to pay only when processing. On-premise \n\n\n\nPage 10 of 30Kolajo et al. J Big Data (2019) 6:47 \n\ndeployment may be considered for predictable or consistent loads. But if workloads are \nmixed (i.e. consistent flows or spikes), a combination of cloud and on-premise approach \nmay be considered so as to give room for easy integration of web-based services or soft-\nware and access to critical functions on the go.\n\nLatency requirement\n\nIf a minimal delay or low latency is required, key-value stores may be considered or bet-\nter still, an in-memory solution which allows the process of large datasets in real-time is \nrequired in order to optimize the data loading procedure.\n\nThe tools and technologies for big data stream analysis can be broadly categorized into \ntwo, which are open source and proprietary solutions. These are listed in Tables 6 and 7.\n\nThe selection of big data streaming tools and technologies should be based on the impor-\ntance of each factor earlier mentioned in this section. Proprietary solutions may not be eas-\nily available because of pricing and licensing issues. While open source supports innovation \nand development at a large scale, careful selection must be made especially when choosing \na recent technology still in production due to limited maturity and lack of support from \nacademic researchers or developer communities. In addition, open source solutions may \nlead to outdating and modification challenges [27]. Moreover, the selection of whether pro-\nprietary or open source or combination of both should depend on the problem to address, \nthe understanding of the true costs, and benefits of both open and proprietary solutions.\n\nTable 6 Open source tools and technologies for big data stream analysis\n\nTools and technology Article\n\nBlockMon [83]\n\nNoSQL [4, 84–86]\n\nSpark streaming [67, 87–91]\n\nApache storm [68, 85, 86, 92–97]\n\nKafka [85, 91, 95, 96, 98]\n\nYahoo! S4 [6, 45, 87, 99]\n\nApache Samza [46, 67, 100]\n\nPhoton [67, 101]\n\nApache Aurora [67, 102]\n\nMavEStream [103]\n\nEsperTech [104, 105]\n\nRedis [106]\n\nC-SPARQL [107, 108]\n\nSAMOA [56, 78, 109]\n\nCQELS [108, 110, 111]\n\nETALIS [112]\n\nXSEQ [73]\n\nApache Kylin [113]\n\nSplunk stream [114]\n\n\n\nPage 11 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nResearch Question 2: What methods and techniques are used in analysing big data \n\nstreams?\n\nGiven the real-time nature, velocity and volume of social media streams, the clus-\ntering algorithms that are applied on streaming data must be highly scalable and \nefficient. Also, the dynamic nature of data makes it difficult to know the required or \ndesirable number of clusters in advance. This renders partitioning clustering tech-\nniques (such as k-median, k-means and k-medoid) or expectation-maximization \n(EM) algorithms-based approaches unsuitable for analysing real-time social media \ndata because they require prior knowledge of clusters in advance. In addition, due \nto concept drift inherent in social media streams, scalable graph partitioning algo-\nrithms are not also suitable because of their tendency towards balanced partitioning. \nSocial media streams must be analysed dynamically in order to provide decisions at \nany given time within a limited space and time window [28–30].\n\nDensity-based clustering algorithm (such as DenStream, OpticStream, Flock-\nStream, Exclusive and Complete Clustering) unlike partitioning algorithms does not \nrequire apriori number of clusters in advance and can detect outliers [31]. However, \nthe issue with density-based clustering algorithms is that most of them except for few \nlike HDDStream, PreDeCon-Stream and PKS-Stream (which are memory intensive) \nperform less efficiently in the face of high dimensional data and as a result are not \nsuitable for analyzing social media streams [32].\n\nThreshold-based techniques, hierarchical clustering, and incremental clustering \nor online clustering are more relevant to social media analysis. Several online thresh-\nold-based stream clustering approaches or incremental clustering approaches such as \nMarkov Random Field [33, 34], Online Spherical K-means [35], and Condensed Clusters \n[36] have been adopted. Incremental approaches are suitable for continuously generated \ndata grouping by setting a maximum similarity threshold between the incoming stream \n\nTable 7 Proprietary tools and technologies for big data stream analysis\n\nTools and technology Article\n\nCodeBlue [115]\n\nAnodot [116]\n\nCloudet [117]\n\nSentiment brand monitoring [118]\n\nNumenta [119]\n\nElastic streaming processing engine [120]\n\nMicrosoft azure stream analytics [121]\n\nIBM InfoSphere streams [8, 122]\n\nGoogle MillWheel [123]\n\nArtemis [124]\n\nWSO2 analytics [125]\n\nMicrosoft StreamInsight [126]\n\nTIBCO StreamBase [127]\n\nStriim [128]\n\nKyvos insights [129]\n\nAtScale [130, 131]\n\nLambda architecture [57]\n\n\n\nPage 12 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nand the existing clusters. Much work has been done in improving the efficiency of online \nclustering algorithms, however, little research efforts have been directed to threshold \nand fragmentation issues. Incremental algorithm threshold setting should employ adap-\ntive approach instead of relying on static values [37, 38]. Some of the methods and tech-\nniques that have been employed in analysing big data streams are outlined in Table 8.\n\nTable 8 Methods and techniques for big data stream analysis\n\nMethods and techniques Article\n\nSPADE [132]\n\nLocally supervised metric learning (LSML) [133]\n\nKTS [106]\n\nMultinomial latent dirichlet allocation [106]\n\nVoltage clustering algorithm [106]\n\nLocality sensitive hashing (LSH) [134]\n\nUser profile vector update algorithm [134]\n\nTag assignment stream clustering (TASC) [134]\n\nStreamMap [117]\n\nDensity cognition [117]\n\nQRS detection algorithm [87]\n\nForward chaining rule [110]\n\nStream [135]\n\nCluStream [136, 137]\n\nHPClustering [138]\n\nDenStream [139]\n\nD-Stream [140]\n\nACluStream [141]\n\nDCStream [142]\n\nP-Stream [143]\n\nADStream [144]\n\nContinuous query processing (CQR) [145]\n\nFPSPAN-growth [146]\n\nOutlier method for cloud computing algorithm (OMCA) [147]\n\nMulti-query optimization strategy (MQOS) [148]\n\nParallel K-means clustering [72]\n\nVisibly push down automata (VPA) [73]\n\nIncremental MI outlier detection algorithm (Inc I-MLOF) [149]\n\nAdaptive windowing based online ensemble (AWOE) [74]\n\nDynamic prime-number based security verification [84]\n\nK-anonymity, I-diversity, t-closeness [90]\n\nSingular spectrum matrix completion (SS-MC) [76]\n\nTemporal fuzzy concept analysis [96]\n\nECM-sketch [77]\n\nNearest neighbour [91]\n\nMarkov chains [91]\n\nBlock-QuickSort-AdjacentJobMatch [86]\n\nBlock-QuickSort-OverlapReplicat", "metadata_storage_path": "aHR0cHM6Ly9zdG9yYWdlYWNjb3VudGxpZ2lyay5ibG9iLmNvcmUud2luZG93cy5uZXQvcGFwZXIvczQwNTM3LTAxOS0wMjEwLTcucGRm0", "metadata_author": "Taiwo Kolajo ", "metadata_title": "Big data stream analysis: a systematic literature review", "keyphrases": [ "Creative Commons Attribution 4.0 International License", "effective resource allocation strategy", "existing data mining tools", "big data stream tools", "data stream computing mode", "big data streaming tools", "big data streaming ana", "Big data stream analysis", "big data stream computing", "big data streams analysis", "big data batch computing", "Creative Commons license", "big data computing", "big data tool", "J Big Data", "several computational challenges", "process- ing requirements", "inherent dynamic characteristics", "Three major databases", "first search string", "standard benchmark dataset", "creat iveco mmons", "load balancing issues", "nificant research efforts", "technologies Open Access", "SURVEY PAPER Kolajo", "empirical analysis", "real-time analysis", "future computing", "data sources", "parallelization issues", "research questions", "literature review", "Olawande Daramola3", "Ayodele Adebiyi", "two types", "huge amount", "methodical approach", "global view", "exclusion criteria", "preprocessing stage", "iterative jobs", "scalable frameworks", "growing size", "unrestricted use", "appropriate credit", "original author", "Information Sciences", "Covenant University", "Full list", "author information", "doi.org", "orcid.org", "information technology", "large volume", "great velocity", "systematic review", "initial 2295 papers", "Taiwo Kolajo", "47 papers", "Introduction", "Advances", "high-velocity", "ability", "nature", "terms", "variety", "veracity", "volatility", "value", "new", "trend", "Abstract", "fact", "number", "applications", "methods", "techniques", "rigorous", "comparisons", "Scopus", "ScienceDirect", "EBSCO", "journals", "conferences", "entities", "IEEE", "ACM", "SpringerLink", "Elsevier", "inclusion", "study", "privacy", "attention", "key", "features", "lytics", "conclusion", "algorithms", "complexity", "article", "distribution", "reproduction", "medium", "changes", "Correspondence", "fulokoja", "1 Department", "Computer", "Ota", "Nigeria", "creativecommons", "licenses", "crossmark", "crossref", "Page", "30Kolajo", "scalable com- puting platforms", "big data streaming analytics", "big data stream analysis", "Big data batch processing", "Streaming processing frameworks", "stream processing paradigms", "Stream processing solutions", "data flow graph", "real-time application scenarios", "real-time, high volume", "time data analysis", "real-time data stream", "batch computing", "stream computing", "stream processor", "high-velocity flow", "incoming data", "data generating", "market data", "real-time sources", "changing conditions", "common understanding", "ben- efit", "scientific community", "key issues", "detailed evaluation", "massive amount", "high- velocity", "multiple sources", "low latency", "new sources", "location services", "mobile devices", "sensor pervasiveness", "fundamental assumption", "potential value", "parallel architectures", "decision-making process", "static questions", "continuous queries", "diverse sources", "sideration availability", "fault tolerance", "infinite tuple", "actionable results", "interconnected streams", "related work", "research works", "Research method", "crucial need", "real contrasts", "ming paradigm", "Discussion” section", "Result” section", "addition", "output", "low-latency", "seconds", "demand", "reason", "huge", "organisations", "businesses", "paper", "purpose", "overview", "findings", "implications", "practice", "update", "state", "areas", "challenges", "rest", "Background", "information", "Limitation", "Conclusion", "ubiquity", "Internet", "Things", "Sensors", "freshness", "Storm", "Kafka", "Spark", "Table", "motion", "essence", "fly", "scalability", "assimilation", "production", "operations", "Fig.", "Dimension Batch processing Streaming processing Input Data chunks", "Hardware Multiple CPUs Typical single limited amount", "big data stream computing environments", "big data streaming analysis", "domain Web mining", "streaming analytics system", "big data streams", "Data flow graph", "high data rates", "data processing node", "memory Storage Store", "new incoming data", "ble efficient operations", "typical example", "multiple rounds", "Such processing", "single pass", "new data", "operator stream", "missing data", "data normalization", "distributed system", "High fault-tolerance", "life-critical systems", "data tuples", "Data size", "new information", "new tuple", "new trends", "model predictions", "feature extraction", "external feeds", "non-trivial portion", "traffic monitoring", "sensor networks", "Key issues", "useful knowledge", "current happenings", "speedy manner", "organisa- tions", "load balancing", "privacy issues", "exponential growth", "computer resources", "research efforts", "effective resource", "cation strategy", "small number", "different datasets", "component failure", "time-sensitive processes", "security threats", "data Time", "latest tuples", "analytic applications", "sliding window", "milliseconds Applications", "logical container", "main challenges", "Fault‑tolerance", "integration technique", "results", "operators", "models", "access", "idea", "duplicates", "parsing", "windows", "Table 1", "Comparison", "updates", "advance", "passes", "figure", "NYMEX", "need", "order", "problems", "performance", "timeliness", "consistency", "heterogeneity", "incompleteness", "accuracy", "The", "way", "processors", "Moore", "law", "fore", "plexity", "views", "interruption", "big data stream computing system", "organisational resource management specifi", "big data stream dataset", "Big data stream analytics", "good multiple instances replication", "big data analytics literature", "big data analytics methods", "good system structure", "distributing envi- ronment", "effective tech- niques", "Big data streams", "competent data presentation", "national Data Cooperation", "big data analysis", "data streams changes", "partial data streams", "previous research efforts", "big data streaming", "big data challenges", "systematic literature review", "High throughput Decision", "big threat", "streaming analytics", "streaming” analytics", "streaming data", "natural disaster", "continuous processing", "local views", "accessi- bility", "meaningful content", "Load balancing", "load shedding", "peak loads", "average load", "global centre", "entire information", "needs protection", "main objectives", "future observations", "processing algorithms", "stream-specific requirements", "various tools", "research focus", "business values", "high consistency", "high accuracy", "main challenge", "communicating nodes", "individual privacy", "ent characteristics", "Related work", "The study", "reviews", "fraud", "tures", "platforms", "architecture", "minimal", "latency", "stability", "Heterogeneity", "semantics", "granularity", "correlate", "real-time", "diversity", "hierarchy", "resources", "variance", "result", "respect", "sub-graph", "replicas", "portion", "issue", "opportunities", "IDC", "half", "volume", "velocity", "variability", "consideration", "section", "technologies", "Authors", "definitions", "types", "technology", "cally", "four big data streaming tools", "big data stream analysis tools", "big data stream literature review", "authoritative, full-text scientific, technical", "data stream analysis reviews", "big data stream framework", "big data stream processing", "big data stream algorithms", "big data stream analytics", "three standard database indexes", "big data stream technologies", "big data analytics", "machine learning algorithms", "full-text data- bases", "large data- base", "smart intuitive functionality", "electronic journal service", "big data technologies", "academic journal articles", "leading information solution", "systematic mapping method", "following research questions", "good search string", "Data sources", "four categories", "peer-reviewed literature", "citation database", "standard databases", "information professionals", "bibliographic database", "empirical research", "same vein", "particular focus", "anomaly detection", "tech- niques", "clear explanation", "Relevant publications", "Science Direct", "hensive overview", "research output", "social sciences", "largest abstract", "open access", "health publications", "14 million publications", "Life Sciences", "Physical Sciences", "Health Sciences", "wide range", "academic researchers", "application areas", "healthcare profes", "evaluation techniques", "peer-reviewed journals", "3800 journals", "status", "survey", "scope", "comprehensive", "differences", "concept", "capabilities", "limitations", "strengths", "benchmarks", "population", "son", "intervention", "outcome", "keywords", "searches", "EBSCOhost", "rich", "abstracts", "citations", "36,377 titles", "11,678 publishers", "world", "arts", "humanities", "students", "teachers", "35,000 books", "Engineering", "porate", "Third Search string refinement result", "free online professional network", "First search string result", "Second search string result", "Scopus ScienceDirect EBSCOhost Total", "major academic publishers", "nine (9) search strings", "high impact journals", "111 seemingly relevant papers", "Inclusion criteria Papers", "Table 5 Final Selection", "Further refinement", "Data retrieval", "easy analysis", "high-quality e-books", "100 million publications", "secondary source", "rich databases", "Boolean ‘OR", "2295 arti- cles", "three databases", "computer science", "subject domain", "quick overview", "Microsoft Excel", "three categories", "black colour", "similar investigations", "following categories", "primary study", "source language", "relevant” papers", "11 million researchers", "year range", "total number", "peer-reviewed conferences", "recent papers", "16,711 journals", "Table 2", "Table 3", "Table 4", "1989 papers", "315 papers", "111 papers", "45 papers", "18 papers", "magazine", "titles", "60,000 audiobooks", "iv.", "ResearchGate4", "scientists", "collaborators", "authors", "subscription", "set", "reseaarchgate", "sources", "interest", "stage", "PDF", "introduction", "green", "red", "colours", "end", "workshops", "technical", "symposium", "case", "part", "English", "contributions", "900,000", "1500", "alternative big data streaming solutions", "effective data management decisions", "spike load profile platform", "Big data stream platforms", "Streaming data sources", "many NoSQL databases", "specific application interfaces", "data loading procedure", "enterprise technology vendors", "open source community", "open source solutions", "Workload profile", "proprietary solutions", "stream applications", "high-velocity data", "data stores", "recent technology", "Such platforms", "platform distribution", "Data access", "Several tools", "sary tools", "single flow", "growing demand", "pro- jection", "different structures", "different ways", "CAP theorem", "consistent loads", "consistent flows", "web-based services", "soft- ware", "critical functions", "Latency requirement", "minimal delay", "key-value stores", "memory solution", "licensing issues", "limited maturity", "developer communities", "modification challenges", "large datasets", "large scale", "network partition", "service deployment", "service cloud", "premise approach", "easy integration", "consistency requirement", "careful selection", "serialization technologies", "execution", "functionalities", "response", "factors", "Shape", "capturing", "storing", "rep", "instance", "room", "flexibility", "storage", "users", "Availability", "presence", "break", "scenario", "requests", "Infrastructure", "option", "processing", "predictable", "workloads", "spikes", "combination", "go", "Tables", "pricing", "innovation", "development", "lack", "support", "outdating", "problem", "big data stream analysis Tools", "Multinomial latent dirichlet allocation", "Elastic streaming processing engine", "real-time social media data", "Microsoft azure stream analytics", "old-based stream clustering approaches", "Incremental algorithm threshold setting", "Table 6 Open source tools", "social media analysis", "social media streams", "high dimensional data", "Markov Random Field", "Sentiment brand monitoring", "IBM InfoSphere streams", "Density-based clustering algorithm", "Voltage clustering algorithm", "maximum similarity threshold", "little research efforts", "incremental clustering approaches", "Online Spherical K-means", "Incremental approaches", "Splunk stream", "incoming stream", "data grouping", "Proprietary tools", "WSO2 analytics", "Microsoft StreamInsight", "clustering algorithms", "Spark streaming", "Complete Clustering", "hierarchical clustering", "online clustering", "Research Question", "true costs", "Apache storm", "Apache Samza", "Apache Aurora", "Apache Kylin", "dynamic nature", "desirable number", "prior knowledge", "scalable graph", "limited space", "apriori number", "Google MillWheel", "TIBCO StreamBase", "Kyvos insights", "Lambda architecture", "Much work", "fragmentation issues", "tive approach", "static values", "metric learning", "partitioning algorithms", "technology Article", "balanced partitioning", "time window", "Condensed Clusters", "existing clusters", "Threshold-based techniques", "Table 8 Methods", "Table 7", "understanding", "benefits", "BlockMon", "NoSQL", "Photon", "MavEStream", "EsperTech", "Redis", "C-SPARQL", "SAMOA", "CQELS", "ETALIS", "XSEQ", "k-median", "k-medoid", "expectation-maximization", "tendency", "decisions", "DenStream", "OpticStream", "Exclusive", "outliers", "HDDStream", "PreDeCon-Stream", "PKS-Stream", "memory", "face", "CodeBlue", "Anodot", "Cloudet", "Numenta", "Artemis", "Striim", "AtScale", "efficiency", "SPADE", "LSML", "KTS", "Dynamic prime-number based security verification", "User profile vector update algorithm", "Incremental MI outlier detection algorithm", "Singular spectrum matrix completion", "Temporal fuzzy concept analysis", "Tag assignment stream clustering", "QRS detection algorithm", "cloud computing algorithm", "Parallel K-means clustering", "Locality sensitive hashing", "Forward chaining rule", "Continuous query processing", "Multi-query optimization strategy", "Outlier method", "Density cognition", "Inc I-MLOF", "Adaptive windowing", "online ensemble", "Nearest neighbour", "Markov chains", "LSH", "TASC", "StreamMap", "CluStream", "HPClustering", "D-Stream", "DCStream", "P-Stream", "ADStream", "CQR", "FPSPAN-growth", "OMCA", "MQOS", "automata", "VPA", "AWOE", "K-anonymity", "closeness", "ECM-sketch", "Block-QuickSort-AdjacentJobMatch", "Block-QuickSort-OverlapReplicat" ], "merged_content": "\nBig data stream analysis: a systematic \nliterature review\nTaiwo Kolajo1,2* , Olawande Daramola3 and Ayodele Adebiyi1,4 \n\nIntroduction\nAdvances in information technology have facilitated large volume, high-velocity of data, \nand the ability to store data continuously leading to several computational challenges. \nDue to the nature of big data in terms of volume, velocity, variety, variability, veracity, \nvolatility, and value [1] that are being generated recently, big data computing is a new \ntrend for future computing.\n\nBig data computing can be generally categorized into two types based on the process-\ning requirements, which are big data batch computing and big data stream computing \n\nAbstract \n\nRecently, big data streams have become ubiquitous due to the fact that a number of \napplications generate a huge amount of data at a great velocity. This made it difficult \nfor existing data mining tools, technologies, methods, and techniques to be applied \ndirectly on big data streams due to the inherent dynamic characteristics of big data. In \nthis paper, a systematic review of big data streams analysis which employed a rigorous \nand methodical approach to look at the trends of big data stream tools and technolo-\ngies as well as methods and techniques employed in analysing big data streams. It \nprovides a global view of big data stream tools and technologies and its comparisons. \nThree major databases, Scopus, ScienceDirect and EBSCO, which indexes journals and \nconferences that are promoted by entities such as IEEE, ACM, SpringerLink, and Elsevier \nwere explored as data sources. Out of the initial 2295 papers that resulted from the \nfirst search string, 47 papers were found to be relevant to our research questions after \nimplementing the inclusion and exclusion criteria. The study found that scalability, \nprivacy and load balancing issues as well as empirical analysis of big data streams and \ntechnologies are still open for further research efforts. We also found that although, sig-\nnificant research efforts have been directed to real-time analysis of big data stream not \nmuch attention has been given to the preprocessing stage of big data streams. Only a \nfew big data streaming tools and technologies can do all of the batch, streaming, and \niterative jobs; there seems to be no big data tool and technology that offers all the key \nfeatures required for now and standard benchmark dataset for big data streaming ana-\nlytics has not been widely adopted. In conclusion, it was recommended that research \nefforts should be geared towards developing scalable frameworks and algorithms that \nwill accommodate data stream computing mode, effective resource allocation strategy \nand parallelization issues to cope with the ever-growing size and complexity of data.\n\nKeywords: Big data stream analysis, Stream computing, Big data streaming tools and \ntechnologies\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nSURVEY PAPER\n\nKolajo et al. J Big Data (2019) 6:47 \nhttps://doi.org/10.1186/s40537-019-0210-7\n\n*Correspondence: \ntaiwo.kolajo@stu.cu.edu.ng; \ntaiwo.kolajo@fulokoja.edu.ng \n1 Department of Computer \nand Information Sciences, \nCovenant University, Ota, \nNigeria\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0001-6780-2495\nhttp://orcid.org/0000-0001-6340-078X\nhttp://orcid.org/0000-0002-3114-6315\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-019-0210-7&domain=pdf\n\n\nPage 2 of 30Kolajo et al. J Big Data (2019) 6:47 \n\n[2]. Big data batch processing is not sufficient when it comes to analysing real-time \napplication scenarios. Most of the data generated in a real-time data stream need real-\ntime data analysis. In addition, the output must be generated with low-latency and any \nincoming data must be reflected in the newly generated output within seconds. This \nnecessitates big data stream analysis [3].\n\nThe demand for stream processing is increasing. The reason being not only that huge \nvolume of data need to be processed but that data must be speedily processed so that \norganisations or businesses can react to changing conditions in real-time.\n\nThis paper presents a systematic review of big data stream analysis. The purpose is to \npresent an overview of research works, findings, as well as implications for research and \npractice. This is necessary to (1) provide an update about the state of research, (2) iden-\ntify areas that are well researched, (3) showcase areas that are lacking and need further \nresearch, and (4) build a common understanding of the challenges that exist for the ben-\nefit of the scientific community.\n\nThe rest of the paper is organized as follows: “Background and related work” section \nprovides information on stream computing and big data stream analysis and the key \nissues involved in it and presents a review on big data streaming analytics. In “Research \nmethod” section, the adopted research methodology is discussed, while “Result” section \npresents the findings of the study. “Discussion” section presents a detailed evaluation \nperformed on big data stream analysis, “Limitation of the review” section highlights the \nlimitations of the study, while “Conclusion and further work” concludes the paper.\n\nBackground and related work\nStream computing\n\nStream computing refers to the processing of massive amount of data generated at high-\nvelocity from multiple sources with low latency in real-time. It is a new paradigm neces-\nsitated because of new sources of data generating scenarios which include ubiquity of \nlocation services, mobile devices, and sensor pervasiveness [4]. It can be applied to the \nhigh-velocity flow of data from real-time sources such as the Internet of Things, Sensors, \nmarket data, mobile, and clickstream.\n\nThe fundamental assumption of this paradigm is that the potential value of data lies in \nits freshness. As a result, data are analysed as soon as they arrive in a stream to produce \nresult as opposed to what obtains in batch computing where data are first stored before \nthey are analysed. There is a crucial need for parallel architectures and scalable com-\nputing platforms [5]. With stream computing, organisations can analyse and respond in \nreal-time to rapidly changing data. Streaming processing frameworks include Storm, S4, \nKafka, and Spark [6–8]. The real contrasts between the batch processing and the stream \nprocessing paradigms are outlined in Table 1.\n\nIncorporating streaming data into decision-making process necessitates a program-\nming paradigm called stream computing. With stream computing, fairly static questions \ncan be evaluated on data in motion (i.e. real-time data) continuously [9].\n\nBig data stream analysis\n\nThe essence of big data streaming analytics is the need to analyse and respond to real-\ntime streaming data using continuous queries so that it is possible to continuously \n\n\n\nPage 3 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nperform analysis on the fly within the stream. Stream processing solutions must be \nable to handle a real-time, high volume of data from diverse sources putting into con-\nsideration availability, scalability and fault tolerance. Big data stream analysis involves \nassimilation of data as an infinite tuple, analysis and production of actionable results \nusually in a form of stream [10].\n\nIn a stream processor, applications are represented as data flow graph made up of \noperations and interconnected streams as depicted in Fig. 1. In a streaming analytics \nsystem, application comes in a form of continuous queries, data are ingested continu-\nously, analysed and correlated, and stream of results are generated. Streaming analytic \napplications is usually a set of operators connected by streams. Streaming analytics \nsystems must be able to identify new information, incrementally build models and \naccess whether the new incoming data deviate from model predictions [9].\n\nThe idea of streaming analytics is that each of the received data tuples is processed \nin the data processing node. Such processing includes removing duplicates, filling \nmissing data, data normalization, parsing, feature extraction, which are typically done \nin a single pass due to the high data rates of external feeds. When a new tuple arrives, \nthis node is triggered, and it expels tuples older than the time specified in the sliding \nwindow (sliding window is a typical example of windows used in stream computing \nwhich keeps only the latest tuples up to the time specified in the windows). A window \n\nTable 1 Comparison between batch processing and streaming processing [82]\n\nDimension Batch processing Streaming processing\n\nInput Data chunks Stream of new data or updates\n\nData size Known and finite Infinite or unknown in advance\n\nHardware Multiple CPUs Typical single limited amount of memory\n\nStorage Store Not store or store non-trivial portion in memory\n\nProcessing Processed in multiple rounds A single or few passes over data\n\nTime Much longer A few seconds or even milliseconds\n\nApplications Widely adopted in almost every domain Web mining, traffic monitoring, sensor networks\n\nFig. 1 Data flow graph of a stream processor. The figure shows how applications (made up of operations and \ninterconnected streams) are represented as data flow graph in a stream processor [10]\n\n operator stream NYMEX \n\n\n\nPage 4 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nis referred to as a logical container for data tuples received. It defines how frequently \ndata is refreshed in the container as well as when data processing is triggered [4].\n\nKey issues in big data stream analysis\n\nBig data stream analysis is relevant when there is a need to obtain useful knowledge \nfrom current happenings in an efficient and speedy manner in order to enable organisa-\ntions to quickly react to problems, or detect new trends which can help improve their \nperformance. However, there are some challenges such as scalability, integration, fault-\ntolerance, timeliness, consistency, heterogeneity and incompleteness, load balancing, \nprivacy issues, and accuracy [3, 11–18] which arises from the nature of big data streams \nthat must be dealt with.\n\nScalability\n\nOne of the main challenges in big data streaming analysis is the issue of scalability. The \nbig data stream is experiencing exponential growth in a way much faster than computer \nresources. The processors follow Moore’s law, but the size of data is exploding. There-\nfore, research efforts should be geared towards developing scalable frameworks and \nalgorithms that will accommodate data stream computing mode, effective resource allo-\ncation strategy and parallelization issues to cope with the ever-growing size and com-\nplexity of data.\n\nIntegration\n\nBuilding a distributed system where each node has a view of the data flow, that is, every \nnode performing analysis with a small number of sources, then aggregating these views \nto build a global view is non-trivial. An integration technique should be designed to ena-\nble efficient operations across different datasets.\n\nFault‑tolerance\n\nHigh fault-tolerance is required in life-critical systems. As data is real-time and infinite \nin big data stream computing environments, a good scalable high fault-tolerance strat-\negy is required that allows an application to continue working despite component failure \nwithout interruption.\n\nTimeliness\n\nTime is of the essence for time-sensitive processes such as mitigating security threats, \nthwarting fraud, or responding to a natural disaster. There is a need for scalable architec-\ntures or platforms that will enable continuous processing of data streams which can be \nused to maximize the timeliness of data. The main challenge is implementing a distrib-\nuted architecture that will aggregate local views of data into global view with minimal \nlatency between communicating nodes.\n\nConsistency\n\nAchieving high consistency (i.e. stability) in big data stream computing environments is \nnon-trivial as it is difficult to determine which data are needed and which nodes should \nbe consistent. Hence a good system structure is required.\n\n\n\nPage 5 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nHeterogeneity and incompleteness\n\nBig data streams are heterogeneous in structure, organisations, semantics, accessi-\nbility and granularity. The challenge here is how to handle an always ever-increas-\ning data, extract meaningful content out of it, aggregate and correlate streaming \ndata from multiple sources in real-time. A competent data presentation should be \ndesigned to reflect the structure, diversity and hierarchy of the streaming data.\n\nLoad balancing\n\nA big data stream computing system is expected to be self-adaptive to data streams \nchanges and avoid load shedding. This is challenging as dedicating resources to cover \npeak loads 24/7 is impossible and load shedding is not feasible when the variance \nbetween the average load and the peak load is high. As a result, a distributing envi-\nronment that automatically streams partial data streams to a global centre when local \nresources become insufficient is required.\n\nHigh throughput\n\nDecision with respect to identifying the sub-graph that needs replication, how many \nreplicas are needed and the portion of the data stream to assign to each replica is an \nissue in big data stream computing environment. There is a need for good multiple \ninstances replication if high throughput is to be achieved.\n\nPrivacy\n\nBig data stream analytics created opportunities for analyzing a huge amount of data \nin real-time but also created a big threat to individual privacy. According to the Inter-\nnational Data Cooperation (IDC), not more than half of the entire information that \nneeds protection is effectively protected. The main challenge is proposing techniques \nfor protecting a big data stream dataset before its analysis.\n\nAccuracy\n\nOne of the main objectives of big data stream analysis is to develop effective tech-\nniques that can accurately predict future observations. However, as a result of inher-\nent characteristics of big data such as volume, velocity, variety, variability, veracity, \nvolatility, and value, big data analysis strongly constrain processing algorithms spatio-\ntemporally and hence stream-specific requirements must be taken into consideration \nto ensure high accuracy.\n\nRelated work\n\nThis section discusses some of the previous research efforts that relate to big data \nstreaming analytics.\n\nThe work of [13] presented a review of various tools, technologies and methods \nfor big data analytics by categorizing big data analytics literature according to their \nresearch focus. This paper is different in that it presents a systematic literature review \nthat focused on big data “streaming” analytics.\n\n\n\nPage 6 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nAuthors in [19] presented a systematic review of big data analytics in e-commerce. The \nstudy explored characteristics, definitions, business values, types and challenges of big \ndata analytics in the e-commerce landscape. Likewise, [20] conducted a study that is cen-\ntred on big data analytics in technology and organisational resource management specifi-\ncally focusing on reviews that present big data challenges and big data analytics methods. \nAlthough they are systematic reviews, the focus is not, particularly on big data streaming.\n\nAuthors in [21] presented the status of empirical research and application areas in big \ndata by employing a systematic mapping method. In the same vein, authors in [22] also \nconducted a survey on big data technologies and machine learning algorithms with a \nparticular focus on anomaly detection. A systematic review of literature which aims to \ndetermine the scope, application, and challenges of big data analytics in healthcare was \npresented by [23]. The work of [2] presented a review of four big data streaming tools \nand technologies. While the study conducted in this paper provided a comprehensive \nreview of not only big data streaming tools and technologies but also methods and tech-\nniques employed in analyzing big data streams. In addition, authors [2] did not provide a \nclear explanation of the methodical approach for selecting the reviewed papers.\n\nResearch method\nThe study was grounded in a systematic literature review of tools and technologies \nwith methods and techniques used in analysing big data streams by adopting [24, 25] as \nmodels.\n\nResearch question\n\nThe study tries to answer the following research questions:\n\nResearch Question 1: What are the tools and technologies employed for big data \nstream analysis?\nResearch Question 2: What methods and techniques are used in analysing big data \nstreams?\nResearch Question 3: What do these tools and technologies have in common and \ntheir differences in terms of concept, purpose and capabilities?\nResearch Question 4: What are the limitations and strengths of these tools and tech-\nnologies?\nResearch Question 5: What are the evaluation techniques or benchmarks used for \nevaluating big data streaming tools and technology?\n\nSearch string\n\nCreating a good search string requires structuring in terms of population, compari-\nson, intervention and outcome [24]. Relevant publications were identified by forming \na search string that combined keywords driven by the research questions earlier stated. \nThe searches were conducted by employing three standard database indexes, which are \nScopus, Science Direct and EBSCOhost. The search string is “big data stream analysis” \nOR “big data stream technologies” OR “big data stream framework” OR “big data stream \nalgorithms” OR “big data stream analysis tools” OR “big data stream processing” OR “big \n\n\n\nPage 7 of 30Kolajo et al. J Big Data (2019) 6:47 \n\ndata stream analysis reviews” OR “big data stream literature review” OR “big data stream \nanalytics”.\n\nData sources\n\nAs research becomes increasingly interdisciplinary, global and collaborative, it is expedi-\nent to select from rich and standard databases. The databases consulted are as follows:\n\n i. Scopus1: Scopus is a bibliographic database containing abstracts and citations for \nacademic journal articles launched in 2004. It covers nearly 36,377 titles from over \n11,678 publishers of which 34,346 are peer-reviewed journals, delivering a compre-\nhensive overview of the world’s research output in the scientific, technical, medi-\ncal, and social sciences (including arts and humanities). It is the largest abstract \nand citation database of peer-reviewed literature.\n\n ii. ScienceDirect2: ScienceDirect is Elsevier’s leading information solution for \nresearchers, students, teachers, information professionals and healthcare profes-\nsionals. It provides both subscription-based and open access-based to a large data-\nbase combining authoritative, full-text scientific, technical and health publications \nwith smart intuitive functionality. It covers over 14 million publications from over \n3800 journals and more than 35,000 books. The journals are grouped into four \ncategories: Life Sciences, Physical Sciences and Engineering, Health Sciences, and \nSocial Sciences and Humanities.\n\n iii. EBSCOhost3: EBSCOhost covers a wide range of bibliographic and full-text data-\nbases for researchers, providing electronic journal service available to both cor-\nporate and academic researchers. It has a total of 16,711 journals and magazine \nindexed and abstracted of which 14,914 are peer-reviewed; more than 900,000 \nhigh-quality e-books and titles and over 60,000 audiobooks from more than 1500 \nmajor academic publishers.\n\n iv. ResearchGate4: A free online professional network for scientists and researchers to \nask and answer questions, share papers and find collaborators. It covers over 100 \nmillion publications from over 11 million researchers. ResearchGate was used as \na secondary source where the authors could not access some papers due to lack of \nsubscription.\n\nData retrieval\n\nThe search was conducted in Scopus, ScienceDirect and EBSCOhost since most of \nthe high impact journals and conferences are indexed in these set of rich databases. \nBoolean ‘OR’ was used in combining the nine (9) search strings. A total of 2295 arti-\ncles from the three databases were retrieved as shown in Table 2.\n\n1 http://www.scopu s.com.\n2 http://www.scien cedir ect.com.\n3 https ://www.ebsco host.com.\n4 https ://www.resea archg ate.net.\n\nhttp://www.scopus.com\nhttp://www.sciencedirect.com\nhttps://www.ebscohost.com\nhttps://www.reseaarchgate.net\n\n\nPage 8 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nFurther refinement was performed by (i) limiting the search to journals and confer-\nence papers; (ii) selecting computer science and IT related as the subject domain; (iii) \nselecting ACM, IEEE, SpringerLink, Elsevier as sources; and year of publication to \nbetween 2004 and 2018. The year range was selected due to the fact that interest in \nbig data stream analysis actually started in 2004. At this stage, a total of 1989 papers \nwere excluded leaving a total of 315 papers (see Table  3). The result of the search \nstring was exported to PDF.\n\nBy going through the title of the papers, 111 seemingly relevant papers were extracted \nexcluding a total number of 213 that were not relevant at this stage (see Table 4).\n\nThe abstracts of 111 papers and introduction (for papers that the abstracts were not \nclear enough) were then read to have a quick overview of the paper and to ascertain \nwhether they are suitable or at variance with the research questions. The citations of \nthe papers were exported to Microsoft Excel for easy analysis. The papers were grouped \ninto three categories; “relevant”, “may be relevant” and “irrelevant”. The “relevant” papers \nwere marked with black colour, “may be relevant” and “irrelevant” with green and red \ncolours respectively. At the end of this stage, 45 papers were classified as “relevant”, 9 \npapers as “may be relevant” and 11 as “irrelevant”. Looking critically at the abstract again, \n18 papers were excluded by using the exclusion criteria leaving a total of 47 papers (see \nTable 5) which were manually reviewed in line with the research questions.\n\nInclusion criteria\n\nPapers published in journals, peer-reviewed conferences, workshops, technical and \nsymposium from 2004 and 2018 were included. In addition, the most recent papers \nwere selected in case of papers with similar investigations and results.\n\nTable 2 First search string result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 2097 65 133 2295\n\nTable 3 Second search string result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 196 27 92 315\n\nTable 4 Third Search string refinement result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 64 23 24 111\n\nTable 5 Final Selection\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 25 10 12 47\n\n\n\nPage 9 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nExclusion criteria\n\nPapers that belong to the following categories were excluded from selection as part of \nthe primary study: (i) papers written in source language other than English; (ii) papers \nwith an abstract and or introduction that does not clearly define the contributions of the \nwork; (iii) papers whose abstract do not relate to big data stream analysis.\n\nResult\nThe findings of the study are now presented with respect to the research questions that \nguided the execution of the systematic literature review.\n\nResearch Question 1: What are the tools and technologies employed for big data stream \n\nanalysis?\n\nBig data stream platforms provide functionalities and features that enable big data \nstream applications to develop, operate, deploy, and manage big data streams. Such \nplatforms must be able to pull in streams of data, process the data and stream it back \nas a single flow. Several tools and technologies have been employed to analyse big data \nstreams. In response to the growing demand for big data streaming analytics, a large \nnumber of alternative big data streaming solutions have been developed both by the \nopen source community and enterprise technology vendors. According to [26], there are \nsome factors to consider when selecting big data streaming tools and technologies in \norder to make effective data management decisions. These are briefly described below.\n\nShape of the data\n\nStreaming data sources require serialization technologies for capturing, storing and rep-\nresenting such high-velocity data. For instance, some tools and technologies allow pro-\njection of different structures across data stores, giving room for flexibility for storage \nand access of data in different ways. However, the performance of such platforms may \nnot be suitable for high-velocity data.\n\nData access\n\nThere is a need to put into consideration how the data will be accessed by users and \napplications. For instance, many NoSQL databases require specific application interfaces \nfor data access. Hence there is a need to consider the integration of some other neces-\nsary tools for data access.\n\nAvailability and consistency requirement\n\nIf a distributed system is needed, then CAP theorem states that consistency and avail-\nability cannot be both guaranteed in the presence of network partition (i.e. when there is \na break in the network). In such a scenario, consistency is often traded off for availability \nto ensure that requests can always be processed.\n\nWorkload profile required\n\nPlatform as a service deployment may be appropriate for a spike load profile platform. \nIf platform distribution can be deployed on Infrastructure as a service cloud, then this \noption may be preferred as users will need to pay only when processing. On-premise \n\n\n\nPage 10 of 30Kolajo et al. J Big Data (2019) 6:47 \n\ndeployment may be considered for predictable or consistent loads. But if workloads are \nmixed (i.e. consistent flows or spikes), a combination of cloud and on-premise approach \nmay be considered so as to give room for easy integration of web-based services or soft-\nware and access to critical functions on the go.\n\nLatency requirement\n\nIf a minimal delay or low latency is required, key-value stores may be considered or bet-\nter still, an in-memory solution which allows the process of large datasets in real-time is \nrequired in order to optimize the data loading procedure.\n\nThe tools and technologies for big data stream analysis can be broadly categorized into \ntwo, which are open source and proprietary solutions. These are listed in Tables 6 and 7.\n\nThe selection of big data streaming tools and technologies should be based on the impor-\ntance of each factor earlier mentioned in this section. Proprietary solutions may not be eas-\nily available because of pricing and licensing issues. While open source supports innovation \nand development at a large scale, careful selection must be made especially when choosing \na recent technology still in production due to limited maturity and lack of support from \nacademic researchers or developer communities. In addition, open source solutions may \nlead to outdating and modification challenges [27]. Moreover, the selection of whether pro-\nprietary or open source or combination of both should depend on the problem to address, \nthe understanding of the true costs, and benefits of both open and proprietary solutions.\n\nTable 6 Open source tools and technologies for big data stream analysis\n\nTools and technology Article\n\nBlockMon [83]\n\nNoSQL [4, 84–86]\n\nSpark streaming [67, 87–91]\n\nApache storm [68, 85, 86, 92–97]\n\nKafka [85, 91, 95, 96, 98]\n\nYahoo! S4 [6, 45, 87, 99]\n\nApache Samza [46, 67, 100]\n\nPhoton [67, 101]\n\nApache Aurora [67, 102]\n\nMavEStream [103]\n\nEsperTech [104, 105]\n\nRedis [106]\n\nC-SPARQL [107, 108]\n\nSAMOA [56, 78, 109]\n\nCQELS [108, 110, 111]\n\nETALIS [112]\n\nXSEQ [73]\n\nApache Kylin [113]\n\nSplunk stream [114]\n\n\n\nPage 11 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nResearch Question 2: What methods and techniques are used in analysing big data \n\nstreams?\n\nGiven the real-time nature, velocity and volume of social media streams, the clus-\ntering algorithms that are applied on streaming data must be highly scalable and \nefficient. Also, the dynamic nature of data makes it difficult to know the required or \ndesirable number of clusters in advance. This renders partitioning clustering tech-\nniques (such as k-median, k-means and k-medoid) or expectation-maximization \n(EM) algorithms-based approaches unsuitable for analysing real-time social media \ndata because they require prior knowledge of clusters in advance. In addition, due \nto concept drift inherent in social media streams, scalable graph partitioning algo-\nrithms are not also suitable because of their tendency towards balanced partitioning. \nSocial media streams must be analysed dynamically in order to provide decisions at \nany given time within a limited space and time window [28–30].\n\nDensity-based clustering algorithm (such as DenStream, OpticStream, Flock-\nStream, Exclusive and Complete Clustering) unlike partitioning algorithms does not \nrequire apriori number of clusters in advance and can detect outliers [31]. However, \nthe issue with density-based clustering algorithms is that most of them except for few \nlike HDDStream, PreDeCon-Stream and PKS-Stream (which are memory intensive) \nperform less efficiently in the face of high dimensional data and as a result are not \nsuitable for analyzing social media streams [32].\n\nThreshold-based techniques, hierarchical clustering, and incremental clustering \nor online clustering are more relevant to social media analysis. Several online thresh-\nold-based stream clustering approaches or incremental clustering approaches such as \nMarkov Random Field [33, 34], Online Spherical K-means [35], and Condensed Clusters \n[36] have been adopted. Incremental approaches are suitable for continuously generated \ndata grouping by setting a maximum similarity threshold between the incoming stream \n\nTable 7 Proprietary tools and technologies for big data stream analysis\n\nTools and technology Article\n\nCodeBlue [115]\n\nAnodot [116]\n\nCloudet [117]\n\nSentiment brand monitoring [118]\n\nNumenta [119]\n\nElastic streaming processing engine [120]\n\nMicrosoft azure stream analytics [121]\n\nIBM InfoSphere streams [8, 122]\n\nGoogle MillWheel [123]\n\nArtemis [124]\n\nWSO2 analytics [125]\n\nMicrosoft StreamInsight [126]\n\nTIBCO StreamBase [127]\n\nStriim [128]\n\nKyvos insights [129]\n\nAtScale [130, 131]\n\nLambda architecture [57]\n\n\n\nPage 12 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nand the existing clusters. Much work has been done in improving the efficiency of online \nclustering algorithms, however, little research efforts have been directed to threshold \nand fragmentation issues. Incremental algorithm threshold setting should employ adap-\ntive approach instead of relying on static values [37, 38]. Some of the methods and tech-\nniques that have been employed in analysing big data streams are outlined in Table 8.\n\nTable 8 Methods and techniques for big data stream analysis\n\nMethods and techniques Article\n\nSPADE [132]\n\nLocally supervised metric learning (LSML) [133]\n\nKTS [106]\n\nMultinomial latent dirichlet allocation [106]\n\nVoltage clustering algorithm [106]\n\nLocality sensitive hashing (LSH) [134]\n\nUser profile vector update algorithm [134]\n\nTag assignment stream clustering (TASC) [134]\n\nStreamMap [117]\n\nDensity cognition [117]\n\nQRS detection algorithm [87]\n\nForward chaining rule [110]\n\nStream [135]\n\nCluStream [136, 137]\n\nHPClustering [138]\n\nDenStream [139]\n\nD-Stream [140]\n\nACluStream [141]\n\nDCStream [142]\n\nP-Stream [143]\n\nADStream [144]\n\nContinuous query processing (CQR) [145]\n\nFPSPAN-growth [146]\n\nOutlier method for cloud computing algorithm (OMCA) [147]\n\nMulti-query optimization strategy (MQOS) [148]\n\nParallel K-means clustering [72]\n\nVisibly push down automata (VPA) [73]\n\nIncremental MI outlier detection algorithm (Inc I-MLOF) [149]\n\nAdaptive windowing based online ensemble (AWOE) [74]\n\nDynamic prime-number based security verification [84]\n\nK-anonymity, I-diversity, t-closeness [90]\n\nSingular spectrum matrix completion (SS-MC) [76]\n\nTemporal fuzzy concept analysis [96]\n\nECM-sketch [77]\n\nNearest neighbour [91]\n\nMarkov chains [91]\n\nBlock-QuickSort-AdjacentJobMatch [86]\n\nBlock-QuickSort-OverlapReplicat", "text": [ "operator stream NYMEX", "Magnitude of change in paper distribution over the studied years 180 160 140 120 100 No of Papers 80 156 60 98 40 20 22 28 38 0 2 10 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 Years", "Percentage of Publication Type 34, 9% -Journal .Conferences 192, 50% 155, 41% Workshop/Technical/Sym posium", "90 80 70 No of Researchers 60 50 40 30 20 10 O Italy Frequency of Researchers Canada China India USA Germany France Japan Turkey Republic of Korea Ireland Spain Countries Poland Los Angeles Switzerland Iran Greece Norway", "Published online: 06 June 2019" ], "layoutText": [ "{\"language\":\"en\",\"text\":\"operator stream NYMEX\",\"lines\":[{\"boundingBox\":[{\"x\":340,\"y\":5},{\"x\":519,\"y\":2},{\"x\":520,\"y\":41},{\"x\":341,\"y\":44}],\"text\":\"operator\"},{\"boundingBox\":[{\"x\":778,\"y\":107},{\"x\":921,\"y\":108},{\"x\":921,\"y\":143},{\"x\":778,\"y\":142}],\"text\":\"stream\"},{\"boundingBox\":[{\"x\":21,\"y\":615},{\"x\":124,\"y\":613},{\"x\":125,\"y\":634},{\"x\":21,\"y\":636}],\"text\":\"NYMEX\"}],\"words\":[{\"boundingBox\":[{\"x\":341,\"y\":6},{\"x\":520,\"y\":3},{\"x\":520,\"y\":43},{\"x\":341,\"y\":44}],\"text\":\"operator\"},{\"boundingBox\":[{\"x\":780,\"y\":108},{\"x\":913,\"y\":109},{\"x\":910,\"y\":144},{\"x\":779,\"y\":142}],\"text\":\"stream\"},{\"boundingBox\":[{\"x\":48,\"y\":615},{\"x\":124,\"y\":613},{\"x\":125,\"y\":635},{\"x\":49,\"y\":636}],\"text\":\"NYMEX\"}]}", "{\"language\":\"en\",\"text\":\"Magnitude of change in paper distribution over the studied years 180 160 140 120 100 No of Papers 80 156 60 98 40 20 22 28 38 0 2 10 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 Years\",\"lines\":[{\"boundingBox\":[{\"x\":203,\"y\":0},{\"x\":1034,\"y\":0},{\"x\":1034,\"y\":31},{\"x\":203,\"y\":34}],\"text\":\"Magnitude of change in paper distribution over the\"},{\"boundingBox\":[{\"x\":511,\"y\":43},{\"x\":727,\"y\":44},{\"x\":727,\"y\":77},{\"x\":511,\"y\":74}],\"text\":\"studied years\"},{\"boundingBox\":[{\"x\":52,\"y\":79},{\"x\":96,\"y\":79},{\"x\":96,\"y\":100},{\"x\":53,\"y\":100}],\"text\":\"180\"},{\"boundingBox\":[{\"x\":51,\"y\":123},{\"x\":96,\"y\":122},{\"x\":97,\"y\":144},{\"x\":52,\"y\":144}],\"text\":\"160\"},{\"boundingBox\":[{\"x\":52,\"y\":167},{\"x\":96,\"y\":166},{\"x\":95,\"y\":187},{\"x\":53,\"y\":187}],\"text\":\"140\"},{\"boundingBox\":[{\"x\":52,\"y\":212},{\"x\":96,\"y\":211},{\"x\":96,\"y\":232},{\"x\":53,\"y\":232}],\"text\":\"120\"},{\"boundingBox\":[{\"x\":52,\"y\":256},{\"x\":97,\"y\":255},{\"x\":97,\"y\":276},{\"x\":53,\"y\":277}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":1,\"y\":393},{\"x\":1,\"y\":186},{\"x\":32,\"y\":186},{\"x\":31,\"y\":393}],\"text\":\"No of Papers\"},{\"boundingBox\":[{\"x\":65,\"y\":298},{\"x\":95,\"y\":299},{\"x\":95,\"y\":320},{\"x\":64,\"y\":320}],\"text\":\"80\"},{\"boundingBox\":[{\"x\":1130,\"y\":300},{\"x\":1176,\"y\":300},{\"x\":1176,\"y\":327},{\"x\":1131,\"y\":327}],\"text\":\"156\"},{\"boundingBox\":[{\"x\":67,\"y\":342},{\"x\":93,\"y\":342},{\"x\":93,\"y\":364},{\"x\":67,\"y\":364}],\"text\":\"60\"},{\"boundingBox\":[{\"x\":1061,\"y\":368},{\"x\":1092,\"y\":368},{\"x\":1092,\"y\":389},{\"x\":1061,\"y\":389}],\"text\":\"98\"},{\"boundingBox\":[{\"x\":66,\"y\":381},{\"x\":98,\"y\":383},{\"x\":96,\"y\":409},{\"x\":65,\"y\":408}],\"text\":\"40\"},{\"boundingBox\":[{\"x\":67,\"y\":431},{\"x\":93,\"y\":430},{\"x\":94,\"y\":453},{\"x\":66,\"y\":453}],\"text\":\"20\"},{\"boundingBox\":[{\"x\":838,\"y\":449},{\"x\":871,\"y\":448},{\"x\":872,\"y\":474},{\"x\":839,\"y\":474}],\"text\":\"22\"},{\"boundingBox\":[{\"x\":913,\"y\":444},{\"x\":944,\"y\":444},{\"x\":944,\"y\":467},{\"x\":913,\"y\":467}],\"text\":\"28\"},{\"boundingBox\":[{\"x\":989,\"y\":430},{\"x\":1019,\"y\":431},{\"x\":1019,\"y\":457},{\"x\":988,\"y\":456}],\"text\":\"38\"},{\"boundingBox\":[{\"x\":79,\"y\":474},{\"x\":123,\"y\":472},{\"x\":123,\"y\":494},{\"x\":79,\"y\":496}],\"text\":\"0 2\"},{\"boundingBox\":[{\"x\":767,\"y\":464},{\"x\":796,\"y\":463},{\"x\":797,\"y\":484},{\"x\":768,\"y\":485}],\"text\":\"10\"},{\"boundingBox\":[{\"x\":82,\"y\":506},{\"x\":1179,\"y\":506},{\"x\":1180,\"y\":529},{\"x\":82,\"y\":529}],\"text\":\"2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018\"},{\"boundingBox\":[{\"x\":591,\"y\":555},{\"x\":678,\"y\":556},{\"x\":677,\"y\":580},{\"x\":591,\"y\":579}],\"text\":\"Years\"}],\"words\":[{\"boundingBox\":[{\"x\":204,\"y\":1},{\"x\":384,\"y\":2},{\"x\":384,\"y\":33},{\"x\":204,\"y\":31}],\"text\":\"Magnitude\"},{\"boundingBox\":[{\"x\":390,\"y\":2},{\"x\":424,\"y\":2},{\"x\":424,\"y\":33},{\"x\":390,\"y\":33}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":430,\"y\":2},{\"x\":546,\"y\":2},{\"x\":546,\"y\":34},{\"x\":430,\"y\":33}],\"text\":\"change\"},{\"boundingBox\":[{\"x\":552,\"y\":2},{\"x\":586,\"y\":2},{\"x\":586,\"y\":34},{\"x\":552,\"y\":34}],\"text\":\"in\"},{\"boundingBox\":[{\"x\":592,\"y\":2},{\"x\":692,\"y\":2},{\"x\":692,\"y\":33},{\"x\":592,\"y\":34}],\"text\":\"paper\"},{\"boundingBox\":[{\"x\":698,\"y\":2},{\"x\":892,\"y\":1},{\"x\":892,\"y\":30},{\"x\":698,\"y\":33}],\"text\":\"distribution\"},{\"boundingBox\":[{\"x\":898,\"y\":1},{\"x\":972,\"y\":0},{\"x\":972,\"y\":29},{\"x\":898,\"y\":30}],\"text\":\"over\"},{\"boundingBox\":[{\"x\":978,\"y\":0},{\"x\":1035,\"y\":0},{\"x\":1034,\"y\":28},{\"x\":978,\"y\":29}],\"text\":\"the\"},{\"boundingBox\":[{\"x\":512,\"y\":44},{\"x\":633,\"y\":44},{\"x\":633,\"y\":76},{\"x\":513,\"y\":75}],\"text\":\"studied\"},{\"boundingBox\":[{\"x\":639,\"y\":44},{\"x\":727,\"y\":46},{\"x\":726,\"y\":77},{\"x\":639,\"y\":76}],\"text\":\"years\"},{\"boundingBox\":[{\"x\":52,\"y\":79},{\"x\":95,\"y\":79},{\"x\":95,\"y\":100},{\"x\":52,\"y\":100}],\"text\":\"180\"},{\"boundingBox\":[{\"x\":51,\"y\":122},{\"x\":96,\"y\":122},{\"x\":96,\"y\":143},{\"x\":51,\"y\":144}],\"text\":\"160\"},{\"boundingBox\":[{\"x\":52,\"y\":166},{\"x\":95,\"y\":166},{\"x\":95,\"y\":187},{\"x\":52,\"y\":187}],\"text\":\"140\"},{\"boundingBox\":[{\"x\":52,\"y\":211},{\"x\":95,\"y\":211},{\"x\":95,\"y\":232},{\"x\":52,\"y\":232}],\"text\":\"120\"},{\"boundingBox\":[{\"x\":52,\"y\":256},{\"x\":96,\"y\":255},{\"x\":97,\"y\":276},{\"x\":52,\"y\":277}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":1,\"y\":391},{\"x\":1,\"y\":343},{\"x\":32,\"y\":343},{\"x\":32,\"y\":391}],\"text\":\"No\"},{\"boundingBox\":[{\"x\":1,\"y\":337},{\"x\":1,\"y\":303},{\"x\":32,\"y\":303},{\"x\":32,\"y\":337}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":1,\"y\":297},{\"x\":3,\"y\":187},{\"x\":33,\"y\":187},{\"x\":32,\"y\":297}],\"text\":\"Papers\"},{\"boundingBox\":[{\"x\":64,\"y\":298},{\"x\":95,\"y\":298},{\"x\":94,\"y\":320},{\"x\":64,\"y\":319}],\"text\":\"80\"},{\"boundingBox\":[{\"x\":1130,\"y\":300},{\"x\":1175,\"y\":300},{\"x\":1175,\"y\":327},{\"x\":1130,\"y\":327}],\"text\":\"156\"},{\"boundingBox\":[{\"x\":67,\"y\":342},{\"x\":92,\"y\":342},{\"x\":92,\"y\":364},{\"x\":67,\"y\":364}],\"text\":\"60\"},{\"boundingBox\":[{\"x\":1061,\"y\":368},{\"x\":1092,\"y\":368},{\"x\":1092,\"y\":389},{\"x\":1061,\"y\":389}],\"text\":\"98\"},{\"boundingBox\":[{\"x\":66,\"y\":381},{\"x\":97,\"y\":382},{\"x\":95,\"y\":409},{\"x\":65,\"y\":407}],\"text\":\"40\"},{\"boundingBox\":[{\"x\":66,\"y\":430},{\"x\":93,\"y\":430},{\"x\":94,\"y\":452},{\"x\":66,\"y\":453}],\"text\":\"20\"},{\"boundingBox\":[{\"x\":839,\"y\":448},{\"x\":871,\"y\":448},{\"x\":871,\"y\":473},{\"x\":840,\"y\":474}],\"text\":\"22\"},{\"boundingBox\":[{\"x\":916,\"y\":444},{\"x\":944,\"y\":444},{\"x\":944,\"y\":467},{\"x\":916,\"y\":467}],\"text\":\"28\"},{\"boundingBox\":[{\"x\":990,\"y\":430},{\"x\":1017,\"y\":431},{\"x\":1017,\"y\":457},{\"x\":989,\"y\":456}],\"text\":\"38\"},{\"boundingBox\":[{\"x\":79,\"y\":474},{\"x\":99,\"y\":473},{\"x\":100,\"y\":494},{\"x\":79,\"y\":495}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":106,\"y\":473},{\"x\":123,\"y\":472},{\"x\":124,\"y\":493},{\"x\":107,\"y\":494}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":767,\"y\":464},{\"x\":796,\"y\":463},{\"x\":796,\"y\":484},{\"x\":767,\"y\":485}],\"text\":\"10\"},{\"boundingBox\":[{\"x\":82,\"y\":507},{\"x\":147,\"y\":507},{\"x\":147,\"y\":530},{\"x\":82,\"y\":530}],\"text\":\"2004\"},{\"boundingBox\":[{\"x\":156,\"y\":507},{\"x\":221,\"y\":506},{\"x\":221,\"y\":530},{\"x\":156,\"y\":530}],\"text\":\"2005\"},{\"boundingBox\":[{\"x\":230,\"y\":506},{\"x\":295,\"y\":506},{\"x\":295,\"y\":530},{\"x\":230,\"y\":530}],\"text\":\"2006\"},{\"boundingBox\":[{\"x\":306,\"y\":506},{\"x\":372,\"y\":506},{\"x\":372,\"y\":530},{\"x\":306,\"y\":530}],\"text\":\"2007\"},{\"boundingBox\":[{\"x\":379,\"y\":506},{\"x\":444,\"y\":506},{\"x\":444,\"y\":530},{\"x\":379,\"y\":530}],\"text\":\"2008\"},{\"boundingBox\":[{\"x\":453,\"y\":506},{\"x\":520,\"y\":506},{\"x\":520,\"y\":530},{\"x\":453,\"y\":530}],\"text\":\"2009\"},{\"boundingBox\":[{\"x\":529,\"y\":506},{\"x\":594,\"y\":506},{\"x\":594,\"y\":530},{\"x\":529,\"y\":530}],\"text\":\"2010\"},{\"boundingBox\":[{\"x\":603,\"y\":506},{\"x\":669,\"y\":506},{\"x\":669,\"y\":530},{\"x\":603,\"y\":530}],\"text\":\"2011\"},{\"boundingBox\":[{\"x\":677,\"y\":506},{\"x\":744,\"y\":506},{\"x\":744,\"y\":530},{\"x\":676,\"y\":530}],\"text\":\"2012\"},{\"boundingBox\":[{\"x\":752,\"y\":506},{\"x\":815,\"y\":506},{\"x\":815,\"y\":530},{\"x\":752,\"y\":530}],\"text\":\"2013\"},{\"boundingBox\":[{\"x\":826,\"y\":506},{\"x\":891,\"y\":506},{\"x\":891,\"y\":530},{\"x\":826,\"y\":530}],\"text\":\"2014\"},{\"boundingBox\":[{\"x\":900,\"y\":506},{\"x\":965,\"y\":506},{\"x\":965,\"y\":530},{\"x\":900,\"y\":530}],\"text\":\"2015\"},{\"boundingBox\":[{\"x\":975,\"y\":506},{\"x\":1040,\"y\":507},{\"x\":1040,\"y\":530},{\"x\":975,\"y\":530}],\"text\":\"2016\"},{\"boundingBox\":[{\"x\":1049,\"y\":507},{\"x\":1115,\"y\":507},{\"x\":1115,\"y\":530},{\"x\":1049,\"y\":530}],\"text\":\"2017\"},{\"boundingBox\":[{\"x\":1123,\"y\":507},{\"x\":1180,\"y\":507},{\"x\":1180,\"y\":529},{\"x\":1123,\"y\":530}],\"text\":\"2018\"},{\"boundingBox\":[{\"x\":595,\"y\":556},{\"x\":677,\"y\":557},{\"x\":677,\"y\":580},{\"x\":595,\"y\":580}],\"text\":\"Years\"}]}", "{\"language\":\"en\",\"text\":\"Percentage of Publication Type 34, 9% -Journal .Conferences 192, 50% 155, 41% Workshop/Technical/Sym posium\",\"lines\":[{\"boundingBox\":[{\"x\":152,\"y\":1},{\"x\":611,\"y\":1},{\"x\":611,\"y\":30},{\"x\":152,\"y\":30}],\"text\":\"Percentage of Publication Type\"},{\"boundingBox\":[{\"x\":120,\"y\":79},{\"x\":211,\"y\":79},{\"x\":211,\"y\":105},{\"x\":120,\"y\":105}],\"text\":\"34, 9%\"},{\"boundingBox\":[{\"x\":501,\"y\":123},{\"x\":610,\"y\":122},{\"x\":610,\"y\":146},{\"x\":501,\"y\":147}],\"text\":\"-Journal\"},{\"boundingBox\":[{\"x\":502,\"y\":212},{\"x\":672,\"y\":212},{\"x\":672,\"y\":236},{\"x\":502,\"y\":236}],\"text\":\".Conferences\"},{\"boundingBox\":[{\"x\":281,\"y\":259},{\"x\":403,\"y\":259},{\"x\":403,\"y\":286},{\"x\":281,\"y\":286}],\"text\":\"192, 50%\"},{\"boundingBox\":[{\"x\":20,\"y\":279},{\"x\":142,\"y\":279},{\"x\":142,\"y\":305},{\"x\":20,\"y\":306}],\"text\":\"155, 41%\"},{\"boundingBox\":[{\"x\":499,\"y\":302},{\"x\":825,\"y\":302},{\"x\":825,\"y\":330},{\"x\":499,\"y\":330}],\"text\":\"Workshop/Technical/Sym\"},{\"boundingBox\":[{\"x\":523,\"y\":340},{\"x\":612,\"y\":338},{\"x\":612,\"y\":361},{\"x\":524,\"y\":364}],\"text\":\"posium\"}],\"words\":[{\"boundingBox\":[{\"x\":152,\"y\":4},{\"x\":319,\"y\":2},{\"x\":318,\"y\":30},{\"x\":152,\"y\":30}],\"text\":\"Percentage\"},{\"boundingBox\":[{\"x\":324,\"y\":2},{\"x\":357,\"y\":1},{\"x\":357,\"y\":31},{\"x\":323,\"y\":30}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":362,\"y\":1},{\"x\":533,\"y\":2},{\"x\":533,\"y\":31},{\"x\":362,\"y\":31}],\"text\":\"Publication\"},{\"boundingBox\":[{\"x\":540,\"y\":2},{\"x\":611,\"y\":3},{\"x\":610,\"y\":31},{\"x\":540,\"y\":31}],\"text\":\"Type\"},{\"boundingBox\":[{\"x\":121,\"y\":79},{\"x\":164,\"y\":80},{\"x\":164,\"y\":106},{\"x\":120,\"y\":105}],\"text\":\"34,\"},{\"boundingBox\":[{\"x\":169,\"y\":80},{\"x\":211,\"y\":79},{\"x\":211,\"y\":106},{\"x\":169,\"y\":106}],\"text\":\"9%\"},{\"boundingBox\":[{\"x\":502,\"y\":124},{\"x\":610,\"y\":122},{\"x\":610,\"y\":146},{\"x\":502,\"y\":147}],\"text\":\"-Journal\"},{\"boundingBox\":[{\"x\":502,\"y\":213},{\"x\":672,\"y\":213},{\"x\":672,\"y\":237},{\"x\":502,\"y\":237}],\"text\":\".Conferences\"},{\"boundingBox\":[{\"x\":281,\"y\":259},{\"x\":337,\"y\":260},{\"x\":337,\"y\":287},{\"x\":281,\"y\":286}],\"text\":\"192,\"},{\"boundingBox\":[{\"x\":343,\"y\":260},{\"x\":404,\"y\":260},{\"x\":404,\"y\":287},{\"x\":342,\"y\":287}],\"text\":\"50%\"},{\"boundingBox\":[{\"x\":20,\"y\":280},{\"x\":76,\"y\":280},{\"x\":76,\"y\":307},{\"x\":20,\"y\":307}],\"text\":\"155,\"},{\"boundingBox\":[{\"x\":81,\"y\":280},{\"x\":143,\"y\":280},{\"x\":142,\"y\":306},{\"x\":81,\"y\":307}],\"text\":\"41%\"},{\"boundingBox\":[{\"x\":522,\"y\":304},{\"x\":826,\"y\":303},{\"x\":826,\"y\":330},{\"x\":522,\"y\":330}],\"text\":\"Workshop/Technical/Sym\"},{\"boundingBox\":[{\"x\":524,\"y\":341},{\"x\":608,\"y\":339},{\"x\":608,\"y\":362},{\"x\":524,\"y\":365}],\"text\":\"posium\"}]}", "{\"language\":\"en\",\"text\":\"90 80 70 No of Researchers 60 50 40 30 20 10 O Italy Frequency of Researchers Canada China India USA Germany France Japan Turkey Republic of Korea Ireland Spain Countries Poland Los Angeles Switzerland Iran Greece Norway\",\"lines\":[{\"boundingBox\":[{\"x\":56,\"y\":38},{\"x\":89,\"y\":39},{\"x\":88,\"y\":68},{\"x\":55,\"y\":67}],\"text\":\"90\"},{\"boundingBox\":[{\"x\":54,\"y\":78},{\"x\":86,\"y\":78},{\"x\":86,\"y\":102},{\"x\":52,\"y\":101}],\"text\":\"80\"},{\"boundingBox\":[{\"x\":55,\"y\":112},{\"x\":87,\"y\":113},{\"x\":85,\"y\":138},{\"x\":53,\"y\":137}],\"text\":\"70\"},{\"boundingBox\":[{\"x\":0,\"y\":358},{\"x\":0,\"y\":57},{\"x\":30,\"y\":57},{\"x\":30,\"y\":358}],\"text\":\"No of Researchers\"},{\"boundingBox\":[{\"x\":55,\"y\":149},{\"x\":85,\"y\":149},{\"x\":85,\"y\":173},{\"x\":53,\"y\":172}],\"text\":\"60\"},{\"boundingBox\":[{\"x\":54,\"y\":187},{\"x\":85,\"y\":187},{\"x\":86,\"y\":210},{\"x\":53,\"y\":210}],\"text\":\"50\"},{\"boundingBox\":[{\"x\":55,\"y\":221},{\"x\":85,\"y\":222},{\"x\":84,\"y\":247},{\"x\":53,\"y\":245}],\"text\":\"40\"},{\"boundingBox\":[{\"x\":54,\"y\":259},{\"x\":86,\"y\":260},{\"x\":86,\"y\":284},{\"x\":53,\"y\":283}],\"text\":\"30\"},{\"boundingBox\":[{\"x\":54,\"y\":295},{\"x\":85,\"y\":295},{\"x\":86,\"y\":319},{\"x\":53,\"y\":319}],\"text\":\"20\"},{\"boundingBox\":[{\"x\":54,\"y\":334},{\"x\":85,\"y\":333},{\"x\":86,\"y\":358},{\"x\":54,\"y\":358}],\"text\":\"10\"},{\"boundingBox\":[{\"x\":69,\"y\":392},{\"x\":68,\"y\":365},{\"x\":85,\"y\":365},{\"x\":85,\"y\":392}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":91,\"y\":434},{\"x\":134,\"y\":398},{\"x\":149,\"y\":416},{\"x\":107,\"y\":453}],\"text\":\"Italy\"},{\"boundingBox\":[{\"x\":393,\"y\":2},{\"x\":835,\"y\":0},{\"x\":835,\"y\":32},{\"x\":393,\"y\":35}],\"text\":\"Frequency of Researchers\"},{\"boundingBox\":[{\"x\":123,\"y\":465},{\"x\":191,\"y\":396},{\"x\":207,\"y\":412},{\"x\":138,\"y\":481}],\"text\":\"Canada\"},{\"boundingBox\":[{\"x\":198,\"y\":446},{\"x\":251,\"y\":395},{\"x\":267,\"y\":412},{\"x\":214,\"y\":463}],\"text\":\"China\"},{\"boundingBox\":[{\"x\":264,\"y\":440},{\"x\":309,\"y\":394},{\"x\":325,\"y\":411},{\"x\":280,\"y\":456}],\"text\":\"India\"},{\"boundingBox\":[{\"x\":331,\"y\":429},{\"x\":370,\"y\":393},{\"x\":387,\"y\":414},{\"x\":348,\"y\":450}],\"text\":\"USA\"},{\"boundingBox\":[{\"x\":345,\"y\":480},{\"x\":431,\"y\":398},{\"x\":448,\"y\":415},{\"x\":360,\"y\":498}],\"text\":\"Germany\"},{\"boundingBox\":[{\"x\":428,\"y\":458},{\"x\":490,\"y\":396},{\"x\":505,\"y\":411},{\"x\":443,\"y\":473}],\"text\":\"France\"},{\"boundingBox\":[{\"x\":495,\"y\":448},{\"x\":549,\"y\":395},{\"x\":568,\"y\":416},{\"x\":514,\"y\":468}],\"text\":\"Japan\"},{\"boundingBox\":[{\"x\":545,\"y\":457},{\"x\":608,\"y\":398},{\"x\":625,\"y\":415},{\"x\":560,\"y\":475}],\"text\":\"Turkey\"},{\"boundingBox\":[{\"x\":504,\"y\":558},{\"x\":665,\"y\":395},{\"x\":683,\"y\":413},{\"x\":522,\"y\":576}],\"text\":\"Republic of Korea\"},{\"boundingBox\":[{\"x\":660,\"y\":460},{\"x\":726,\"y\":394},{\"x\":743,\"y\":411},{\"x\":677,\"y\":477}],\"text\":\"Ireland\"},{\"boundingBox\":[{\"x\":735,\"y\":445},{\"x\":784,\"y\":395},{\"x\":802,\"y\":413},{\"x\":752,\"y\":463}],\"text\":\"Spain\"},{\"boundingBox\":[{\"x\":504,\"y\":598},{\"x\":663,\"y\":599},{\"x\":662,\"y\":625},{\"x\":504,\"y\":625}],\"text\":\"Countries\"},{\"boundingBox\":[{\"x\":781,\"y\":458},{\"x\":845,\"y\":394},{\"x\":862,\"y\":411},{\"x\":797,\"y\":475}],\"text\":\"Poland\"},{\"boundingBox\":[{\"x\":796,\"y\":503},{\"x\":904,\"y\":392},{\"x\":925,\"y\":414},{\"x\":815,\"y\":524}],\"text\":\"Los Angeles\"},{\"boundingBox\":[{\"x\":855,\"y\":504},{\"x\":963,\"y\":394},{\"x\":981,\"y\":411},{\"x\":871,\"y\":521}],\"text\":\"Switzerland\"},{\"boundingBox\":[{\"x\":988,\"y\":432},{\"x\":1024,\"y\":397},{\"x\":1039,\"y\":412},{\"x\":1002,\"y\":446}],\"text\":\"Iran\"},{\"boundingBox\":[{\"x\":1018,\"y\":460},{\"x\":1082,\"y\":397},{\"x\":1097,\"y\":412},{\"x\":1033,\"y\":476}],\"text\":\"Greece\"},{\"boundingBox\":[{\"x\":1070,\"y\":467},{\"x\":1144,\"y\":398},{\"x\":1160,\"y\":416},{\"x\":1085,\"y\":484}],\"text\":\"Norway\"}],\"words\":[{\"boundingBox\":[{\"x\":55,\"y\":38},{\"x\":88,\"y\":39},{\"x\":87,\"y\":68},{\"x\":55,\"y\":67}],\"text\":\"90\"},{\"boundingBox\":[{\"x\":52,\"y\":78},{\"x\":86,\"y\":78},{\"x\":85,\"y\":102},{\"x\":52,\"y\":101}],\"text\":\"80\"},{\"boundingBox\":[{\"x\":55,\"y\":112},{\"x\":86,\"y\":113},{\"x\":86,\"y\":138},{\"x\":54,\"y\":137}],\"text\":\"70\"},{\"boundingBox\":[{\"x\":1,\"y\":359},{\"x\":1,\"y\":306},{\"x\":31,\"y\":306},{\"x\":31,\"y\":359}],\"text\":\"No\"},{\"boundingBox\":[{\"x\":1,\"y\":300},{\"x\":1,\"y\":265},{\"x\":31,\"y\":265},{\"x\":31,\"y\":300}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":1,\"y\":259},{\"x\":2,\"y\":58},{\"x\":30,\"y\":58},{\"x\":31,\"y\":259}],\"text\":\"Researchers\"},{\"boundingBox\":[{\"x\":53,\"y\":149},{\"x\":85,\"y\":149},{\"x\":85,\"y\":173},{\"x\":53,\"y\":172}],\"text\":\"60\"},{\"boundingBox\":[{\"x\":53,\"y\":187},{\"x\":85,\"y\":187},{\"x\":85,\"y\":210},{\"x\":53,\"y\":210}],\"text\":\"50\"},{\"boundingBox\":[{\"x\":54,\"y\":221},{\"x\":84,\"y\":222},{\"x\":83,\"y\":247},{\"x\":53,\"y\":245}],\"text\":\"40\"},{\"boundingBox\":[{\"x\":53,\"y\":259},{\"x\":85,\"y\":260},{\"x\":84,\"y\":284},{\"x\":53,\"y\":283}],\"text\":\"30\"},{\"boundingBox\":[{\"x\":53,\"y\":295},{\"x\":85,\"y\":295},{\"x\":85,\"y\":319},{\"x\":53,\"y\":319}],\"text\":\"20\"},{\"boundingBox\":[{\"x\":54,\"y\":333},{\"x\":85,\"y\":333},{\"x\":86,\"y\":357},{\"x\":54,\"y\":358}],\"text\":\"10\"},{\"boundingBox\":[{\"x\":68,\"y\":392},{\"x\":68,\"y\":375},{\"x\":85,\"y\":375},{\"x\":85,\"y\":392}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":91,\"y\":434},{\"x\":133,\"y\":398},{\"x\":149,\"y\":416},{\"x\":106,\"y\":453}],\"text\":\"Italy\"},{\"boundingBox\":[{\"x\":394,\"y\":3},{\"x\":577,\"y\":2},{\"x\":577,\"y\":35},{\"x\":394,\"y\":35}],\"text\":\"Frequency\"},{\"boundingBox\":[{\"x\":584,\"y\":2},{\"x\":619,\"y\":2},{\"x\":619,\"y\":34},{\"x\":583,\"y\":35}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":625,\"y\":2},{\"x\":836,\"y\":0},{\"x\":835,\"y\":30},{\"x\":625,\"y\":34}],\"text\":\"Researchers\"},{\"boundingBox\":[{\"x\":124,\"y\":465},{\"x\":192,\"y\":397},{\"x\":207,\"y\":413},{\"x\":138,\"y\":481}],\"text\":\"Canada\"},{\"boundingBox\":[{\"x\":199,\"y\":447},{\"x\":251,\"y\":395},{\"x\":268,\"y\":412},{\"x\":215,\"y\":463}],\"text\":\"China\"},{\"boundingBox\":[{\"x\":264,\"y\":440},{\"x\":308,\"y\":395},{\"x\":324,\"y\":411},{\"x\":280,\"y\":456}],\"text\":\"India\"},{\"boundingBox\":[{\"x\":331,\"y\":430},{\"x\":369,\"y\":393},{\"x\":387,\"y\":413},{\"x\":347,\"y\":450}],\"text\":\"USA\"},{\"boundingBox\":[{\"x\":345,\"y\":480},{\"x\":431,\"y\":398},{\"x\":449,\"y\":416},{\"x\":360,\"y\":498}],\"text\":\"Germany\"},{\"boundingBox\":[{\"x\":428,\"y\":459},{\"x\":491,\"y\":396},{\"x\":505,\"y\":413},{\"x\":443,\"y\":473}],\"text\":\"France\"},{\"boundingBox\":[{\"x\":495,\"y\":448},{\"x\":548,\"y\":395},{\"x\":568,\"y\":416},{\"x\":514,\"y\":469}],\"text\":\"Japan\"},{\"boundingBox\":[{\"x\":545,\"y\":458},{\"x\":609,\"y\":399},{\"x\":624,\"y\":417},{\"x\":560,\"y\":475}],\"text\":\"Turkey\"},{\"boundingBox\":[{\"x\":505,\"y\":558},{\"x\":585,\"y\":477},{\"x\":602,\"y\":496},{\"x\":522,\"y\":576}],\"text\":\"Republic\"},{\"boundingBox\":[{\"x\":588,\"y\":474},{\"x\":607,\"y\":455},{\"x\":625,\"y\":474},{\"x\":606,\"y\":493}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":611,\"y\":451},{\"x\":665,\"y\":397},{\"x\":682,\"y\":416},{\"x\":628,\"y\":470}],\"text\":\"Korea\"},{\"boundingBox\":[{\"x\":661,\"y\":461},{\"x\":727,\"y\":395},{\"x\":744,\"y\":412},{\"x\":678,\"y\":478}],\"text\":\"Ireland\"},{\"boundingBox\":[{\"x\":736,\"y\":447},{\"x\":784,\"y\":396},{\"x\":802,\"y\":414},{\"x\":752,\"y\":463}],\"text\":\"Spain\"},{\"boundingBox\":[{\"x\":507,\"y\":598},{\"x\":661,\"y\":601},{\"x\":662,\"y\":625},{\"x\":505,\"y\":625}],\"text\":\"Countries\"},{\"boundingBox\":[{\"x\":781,\"y\":459},{\"x\":845,\"y\":395},{\"x\":862,\"y\":413},{\"x\":797,\"y\":475}],\"text\":\"Poland\"},{\"boundingBox\":[{\"x\":796,\"y\":502},{\"x\":825,\"y\":473},{\"x\":845,\"y\":495},{\"x\":815,\"y\":524}],\"text\":\"Los\"},{\"boundingBox\":[{\"x\":829,\"y\":469},{\"x\":905,\"y\":393},{\"x\":925,\"y\":416},{\"x\":849,\"y\":491}],\"text\":\"Angeles\"},{\"boundingBox\":[{\"x\":856,\"y\":503},{\"x\":964,\"y\":394},{\"x\":981,\"y\":411},{\"x\":871,\"y\":521}],\"text\":\"Switzerland\"},{\"boundingBox\":[{\"x\":988,\"y\":431},{\"x\":1023,\"y\":397},{\"x\":1038,\"y\":413},{\"x\":1002,\"y\":447}],\"text\":\"Iran\"},{\"boundingBox\":[{\"x\":1019,\"y\":461},{\"x\":1083,\"y\":398},{\"x\":1097,\"y\":414},{\"x\":1033,\"y\":476}],\"text\":\"Greece\"},{\"boundingBox\":[{\"x\":1070,\"y\":467},{\"x\":1143,\"y\":399},{\"x\":1160,\"y\":418},{\"x\":1085,\"y\":484}],\"text\":\"Norway\"}]}", "{\"language\":\"en\",\"text\":\"Published online: 06 June 2019\",\"lines\":[{\"boundingBox\":[{\"x\":0,\"y\":16},{\"x\":894,\"y\":16},{\"x\":894,\"y\":69},{\"x\":0,\"y\":69}],\"text\":\"Published online: 06 June 2019\"}],\"words\":[{\"boundingBox\":[{\"x\":1,\"y\":18},{\"x\":284,\"y\":17},{\"x\":284,\"y\":70},{\"x\":0,\"y\":69}],\"text\":\"Published\"},{\"boundingBox\":[{\"x\":294,\"y\":17},{\"x\":496,\"y\":16},{\"x\":497,\"y\":70},{\"x\":294,\"y\":70}],\"text\":\"online:\"},{\"boundingBox\":[{\"x\":506,\"y\":16},{\"x\":587,\"y\":17},{\"x\":588,\"y\":70},{\"x\":507,\"y\":70}],\"text\":\"06\"},{\"boundingBox\":[{\"x\":597,\"y\":17},{\"x\":736,\"y\":17},{\"x\":737,\"y\":70},{\"x\":598,\"y\":70}],\"text\":\"June\"},{\"boundingBox\":[{\"x\":746,\"y\":17},{\"x\":891,\"y\":18},{\"x\":892,\"y\":69},{\"x\":747,\"y\":70}],\"text\":\"2019\"}]}" ] }, { "@search.score": 1.7473283, "content": "\nSänger et al. Journal of Trust Management (2015) 2:5 \nDOI 10.1186/s40493-015-0015-3\n\nRESEARCH Open Access\n\nReusable components for online reputation\nsystems\nJohannes Sänger*, Christian Richthammer and Günther Pernul\n\n*Correspondence:\njohannes.saenger@wiwi.\nuni-regensburg.de\nUniversity of Regensburg,\nUniversitätsstraße 31, 93053\nRegensburg, Germany\n\nAbstract\n\nReputation systems have been extensively explored in various disciplines and\napplication areas. A problem in this context is that the computation engines applied by\nmost reputation systems available are designed from scratch and rarely consider well\nestablished concepts and achievements made by others. Thus, approved models and\npromising approaches may get lost in the shuffle. In this work, we aim to foster reuse in\nrespect of trust and reputation systems by providing a hierarchical component\ntaxonomy of computation engines which serves as a natural framework for the design\nof new reputation systems. In order to assist the design process we, furthermore,\nprovide a component repository that contains design knowledge on both a\nconceptual and an implementation level. To evaluate our approach we conduct a\ndescriptive scenario-based analysis which shows that it has an obvious utility from a\npractical point of view. Matching the identified components and the properties of trust\nintroduced in literature, we finally show which properties of trust are widely covered by\ncommon models and which aspects have only rarely been considered so far.\n\nKeywords: Trust; Reputation; Reusability; Trust pattern\n\nIntroduction\nIn the last decade, trust and reputation have been extensively explored in various disci-\nplines and application areas. Thereby, a wide range of metrics and computation methods\nfor reputation-based trust has been proposed. While most common systems have been\nintroduced in e-commerce, such as eBay’s reputation system [1] that allows to rate sell-\ners and buyers, considerable research has also been done in the context of peer-to-peer\nnetworks, mobile ad hoc networks, social networks or ensuring data accuracy, relevance\nand quality in several environments [2]. Computation methods applied range from sim-\nple arithmetic over statistical approaches up to graph-based models involving multiple\nfactors such as context information, propagation or personal preferences. A general prob-\nlem is that most of the newly introduced trust and reputation models use computation\nmethods that are designed from scratch and rely on one novel idea which could lead to\nbetter solutions [3]. Only a few authors build on proposals of others. Therefore, approved\nmodels and promising approaches may get lost in the shuffle.\nIn this work, we aim to encourage reuse in the development of reputation systems by\n\nproviding a framework for creating reputation systems based on reusable components.\nDesign approaches for reuse have been given much attention in the software engineering\n\n© 2015 Sänger et al.; licensee Springer. This is an Open Access article distributed under the terms of the Creative Commons\nAttribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction\nin any medium, provided the original work is properly credited.\n\nmailto: johannes.saenger@wiwi.uni-regensburg.de\nmailto: johannes.saenger@wiwi.uni-regensburg.de\nhttp://creativecommons.org/licenses/by/4.0\n\n\nSänger et al. Journal of Trust Management (2015) 2:5 Page 2 of 21\n\ncommunity. The research in trust and reputation systems could also profit from ben-\nefits like effective use of specialists, accelerated development and increased reliability.\nToward this goal, we propose a hierarchical taxonomy for components of computation\nengines used in reputation systems. Thereto, we decompose the computation phase of\ncommon reputation models to derive single building blocks. The classification based on\ntheir functions serves as a natural framework for the design of new reputation systems.\nMoreover, we set up a component repository containing artifacts on both a conceptual\nand an implementation level to facilitate the reuse of the identified components. On the\nconceptual level, we describe each building block as a design pattern-like solution. On\nthe implementation level, we provide already implemented components by means of web\nservices.\nThe rest of this paper is based on the design science research paradigm involving the\n\nguidelines for conducting design science research by Hevner et al. [4] and organized as\nfollows: Firstly, we give an overview of the general problem context as well as the relevance\nand motivation of our work. Thereby, we identify the research gap and define the objec-\ntives of our research. In the following section, we introduce our hierarchical component\ntaxonomy of computation engines used in reputation systems. After that, we point out\nhow our component repository is conceptually designed and implemented. Subsequently,\nwe carry out a descriptive scenario-based analysis of our approach. At the same time, we\nmatch all components identified with the properties of trust introduced in literature. We\nshow which properties of trust are widely covered by common models and which aspects\nhave only rarely been considered so far. Finally, we summarize the contribution and name\nour plans for future work.\n\nProblem context andmotivation\nWith the success of the Internet and the increasing distribution and connectivity, trust\nand reputation systems have become important artifacts to support decision making in\nnetwork environments. To impart a common understanding, we firstly provide a defi-\nnition of the notion of trust. At the same time, we explain the properties of trust that\nare important with regard to this work. Then, we point out how trust can be established\napplying computational trust models. Focusing on reputation-based trust, we explain how\nand why the research in reputation models could profit from reuse. Thereby, we identify\nthe research gap and define the objectives of this work.\n\nThe notion of trust and its properties\n\nThe notion of trust is a topic that has been discussed in research for decades. Although\nit has been intensively examined in various fields, it still lacks a uniform and generally\naccepted definition. Reasons for this circumstance are the multifaceted terms trust is\nassociated with like credibility, reliability or confidence as well as the multidimension-\nality of trust as an abstract concept that has a cognitive, an emotional and a behavioral\ndimension. As pointed out by [5], trust has been described as being structural in\nnature by sociologists while psychologists viewed trust as an interpersonal phenomenon.\nEconomists, however, interpreted trust as a rational choice mechanism. The definition\noften cited in literature regarding trust and reputation online that is referred to as relia-\nbility trust was proposed by Gambetta in 1988 [6]: “Trust (or, symmetrically, distrust) is\na particular level of the subjective probability with which an agent assesses that another\n\n\n\nSänger et al. Journal of Trust Management (2015) 2:5 Page 3 of 21\n\nagent or group of agents will perform a particular action, both before he can monitor such\naction (or independently of his capacity ever to be able to monitor it) and in a context in\nwhich it affects his own action.”\nMultiple authors furthermore include security and risk which can lead to more com-\n\nplex definitions. Anyway, it is generally agreed that trust is multifaceted and dependent\non a variety of factors. Moreover, there are several properties of trust described in lit-\nerature (see Table 1). These properties are important with respect to this work because\nthey form the basis for many applied computation techniques in trust and reputation\nsystems described in Section ‘Hierarchical component taxonomy’. Reusable components\ncould extend current models by the ability to gradually include these properties.\n\nReputation-based trust\n\nIn recent years, several trust models have been developed to establish trust. Thereby,\ntwo common ways can be distinguished, namely policy-based and reputation-based trust\n\nTable 1 Overview of properties of trust described in literature [14,41-46]\n\nDynamic Trust can increase or decrease through gathering new experiences. Moreover,\ntrust is said to decay with time (time-based aging [45]). Because of these char-\nacteristics, trust values strongly depend on the time they are determined. The\ngreater importance of new experiences compared to old experiences has been\nwidely studied and considered in many trust models such as [32,47] or [30].\n\nContext-dependent Trust is bound to a specific context. For example, Alice trusts Bob as her doctor.\nHowever, she might not trust him as a cook to prepare a delicious meal for her.\n\nMulti-faceted Even in the same context, a trust value may not reflect all aspects of this context\n[43]. For example, a customer may trust a particular restaurant for its quality of\nfood but not for its quality of service. The overall trust on this restaurant depends\non the combination of the amount of trust in the specific aspects.\n\nPropagative One property of trust made use of in several models is its propagativity. If Alice\ntrusts Bob, who in turn trusts Claire, Alice can derive trust on Claire from the rela-\ntionships between her and Bob as well as between Bob and Claire. Because of\nthis propagative nature, it is possible to create trust chains passing trust from\none agent to another agent. As clarified by Christianson and Harbison [48], trust\nis not automatically transitive although trust transitivity was assumed proven for\na long time. If Alice trusts Bob, who in turn trusts Claire, it does not inherently\nmean that Alice trusts Claire. It follows from the foregoing that transitivity implies\npropagation. The reverse, though, is not the case.\n\nComposable When trust is propagated, a particular agent may be connected to multiple\ntrust chains. To come up with a final decision whether to trust or distrust this\nagent, the trust information received from the different chains need to be com-\nposed in order to build one aggregated picture. In this context, trust statements\npropagated from nodes close to oneself should have greater influence on the\naggregated value than the ones from distant nodes (distance-based aging [45]).\nComposition is potentially difficult if the trust statements are contradictory [14].\n\nSubjective The subjective nature of trust becomes clear if one thinks about a review on Ama-\nzon [26]. A book review that totally reflects Alice’s opinion will probably resolve\nin a high level of trust against the reviewer Rachel. Bob, however, who disagrees\nwith the review, will have a lower trust in Rachel although it bases on the same\nevidence.\n\nFine-grained Although trust is sometimes modeled in a binary manner (i.e. either trust or dis-\ntrust), it is possible that Alice trusts both Bob and Claire but that she trusts Bob\nmore than Claire. Hence, there may be multiple discrete levels of trust such as\nhigh, medium and low [41]. Mapped to numbers, trust may also be a continuous\nvariable taking values within a certain interval (e.g. between 0 and 1).\n\nEvent-sensitive It can take a long time to build trust. One negative experience, though, can\ndestroy it [23].\n\nReflexive Trust in oneself is always at the maximum value.\n\nSelf-reinforcing It is human nature to preferentially interact with other agents that are trusted.\nAnalogously, agents will avoid interacting with untrustworthy agents. Thus, the\ntrustworthiness of other agents is inherently taken into consideration.\n\n\n\nSänger et al. Journal of Trust Management (2015) 2:5 Page 4 of 21\n\nestablishment [7]. Policy-based trust is often referred to as a hard security mechanism due\nto the exchange of hard evidence (e.g. credentials). Reputation-based trust, in contrast, is\nderived from the history of interactions. Hence, it can be seen as an estimation of trust-\nworthiness (soft security). In this work, we focus on reputation-based trust. Reputation\nis defined as follows: “Reputation is what is generally said or believed about a person’s or\nthing’s character or standing.” [8].\nIt is based on referrals, ratings or reviews from members of a community. Therefore,\n\nit can be considered as a collective measure of trustworthiness [8]. Trustworthiness as a\nglobal value is objective. However, the trust an agent puts in someone or something as a\ncombination of personal experience and referrals is subjective.\n\nResearch gap: design of reputation systems with reuse\n\nIt has been argued (e.g. by [3]) that most reputation-based trust models proposed in the\nacademic community are built from scratch and do not rely on existing approaches. Only\na few authors continue their research on the ideas of others. Thus, many approvedmodels\nand promising thoughts go unregarded. The benefits of reuse, though, have been rec-\nognized in software engineering for years. However, there are only very few works that\nproposed single components to enhance existing approaches. Rehak et al. [9], for instance,\nintroduced a generic mechanism that can be combined with existing trust models to\nextend their capabilities by efficiently modeling context. The benefits of such a compo-\nnent that can easily be combined with existing systems are obvious. Nonetheless, research\nin trust and reputation still lacks in sound and accepted principles to foster reuse.\nTo gradually close this gap, we aim to provide a framework for the design of new\n\nreputation systems with reuse. As described above, we thereto propose a hierarchical\ncomponent taxonomy of computation engines used in reputation systems. Based on this\ntaxonomy, we set up a repository containing design knowledge on both a conceptual\nand an implementation level. On the one hand, the uniform and well-structured artifacts\ncollected in this repository can be used by developers to select, understand and apply\nexisting concepts. On the other hand, they may encourage researchers to provide novel\ncomponents on a conceptual and an implementation level. In this way, the reuse of ideas,\nconcepts and implemented components as well as the communication of reuse knowledge\nshould be achieved. Furthermore, we argue that the reusable components we identify in\nthis work could extend current reputation models by the ability to gradually include the\nproperties of trust described above. To evaluate whether our taxonomy/framework can\ncover all aspects of trust, we finally provide a table matching our component classes with\ntrust properties.\n\nA hierarchical component taxonomy for computationmethods in reputation\nsystems\nTo derive a taxonomy from existing models, our research includes two steps: (1) the\nanalysis of the generic process of reputation systems and (2) the identification of logical\ncomponents of the computation methods used in common trust and reputation models.\nA critical question is how to determine and classify single components. Thereto, we follow\nan approach to function-based component classification, which means that the taxonomy\nis derived from the functions the identified components fulfill.\n\n\n\nSänger et al. Journal of Trust Management (2015) 2:5 Page 5 of 21\n\nThe generic process of reputation systems\n\nThe generic process of reputation systems, as depicted in Figure 1, can be divided into\nthree steps: (1) collection & preparation, (2) computation and (3) storage & communica-\ntion. These steps are adapted from the three fundamental phases of reputation systems\nidentified by [10] and [11]: feedback generation/collection, feedback aggregation and\nfeedback distribution. Feedback aggregation as the central part of every trust and repu-\ntation system is furthermore divided into the three process steps filtering, weighting and\naggregation taken together as computation. The context setting consists of a trustor who\nwants to build a trust relation toward a trustee by providing context and personalization\nparameters and receiving a trustee’s reputation value.\n\nCollection and preparation\n\nIn the collection and preparation phase, the reputation system gleans information about\nthe past behavior of a trustee and prepares it for subsequent computing. Although per-\nsonal experience is the most reliable, it is often not sufficiently available or nonexistent.\nTherefore, data from other sources needs to be collected. These can be various, ranging\nfrom public or personal collections of data centrally stored to data requested from dif-\nferent peers in a distributed network. After all available data is gathered, it is prepared\nfor further use. Preparation techniques include normalization, for instance, which brings\nthe input data from different sources into a uniform format. Once the preparation is\ncompleted, the reputation data serves as input for the computation phase.\n\nComputation\n\nThe computation phase is the central part of every reputation system and takes the rep-\nutation information collected as input and generates a trust/reputation value as output.\nThis phase can be divided into the three generic process steps filtering, weighting and\naggregation. Depending on the computation engine, not all steps have to be implemented.\nThe first two steps (filtering and weighting) preprocess the data for the subsequent aggre-\ngation. The need for these steps is obvious: The first question to be answered is which\ninformation is useful for further processing (filtering). The second process step concerns\nthe question of how relevant the information is for the specific situation (weighting). In\nline with this, Zhang et al. [12] pointed out that current trust models can be classified into\nthe two broad categories filtering-based and discounting-based. The difference between\nfiltering and weighting is that the filtering process reduces the information amount while\n\nFigure 1 Generic process of a reputation system, inspired by [10].\n\n\n\n\n\nSänger et al. Journal of Trust Management (2015) 2:5 Page 6 of 21\n\nit is enriched by weight factors in the second case. Therefore, filtering can be seen as\nhard selection while weighting is more like a soft selection. Finally, the reputation values\nare aggregated to calculate one or several reputation scores. Depending on the algo-\nrithm, the whole computation process or single process steps can be run through for\nmultiple times.\n\nStorage and communication\n\nAfter reputation scores are calculated, they are either stored locally, in a public storage\nor both depending on the structure (centralized/decentralized/hybrid) of the reputation\nsystem. Common reputation systems not only provide the reputation scores but also offer\nextra information to help the end-users understand the meaning of a score. They should\nfurthermore reveal the computation process to accomplish transparency.\nIn this work, we focus on the computation phase, since the first phase (collection &\n\npreparation) and the last phase (storage & communication) strongly depend on the struc-\nture of the reputation system (centralized or decentralized). The computation phase,\nhowever, is independent of the structure and can look alike for systems implemented in\nboth centralized and decentralized environments. Therefore, it works well for design with\nreuse.\n\nHierarchical component taxonomy\n\nIn this section, the computation process is examined in detail. We introduce a novel\nhierarchical component taxonomy that is based on the functional blocks of common rep-\nutation systems identified in this work. Thereto, we clarify the objectives of the identified\nclasses (functions) and name common examples. Our analysis and selection of reputa-\ntion systems is based on different surveys [2,3,8,13,14]. Figure 2 gives an overview of the\nprimary and secondary classes identified.\n\nFigure 2 Classes of filtering-, weighting- and aggregation-techniques.\n\n\n\n\n\nSänger et al. Journal of Trust Management (2015) 2:5 Page 7 of 21\n\nBeginning with the filtering phase, the three broad classes attribute-based, statistic-\nbased and clustering-based filtering can be identified:\n\n1. Attribute-based filtering: In several trust models, input data is filtered based on a\nconstraint-factor defined for the value of single attributes. Attribute-based filters\nmostly implement a very simple logic, in which an attribute is usually compared to\na reference value. Due to their lightweight, they are proper for reducing huge\namounts of input data to the part necessary for the reputation calculation. Besides\nthe initial filtering of input data, it is often applied after the weighting phase in\norder to filter referrals that have been strongly discounted. Time is an example of\nan attribute that is often constrained because it is desirable to disregard very old\nratings. eBay’s reputation system, for instance, only considers transactions having\noccurred in the last 12 months for their overview of positive, neutral and negative\nratings. Other models such as Sporas [15] ignore every referral but the latest, if one\nparty rated another party more than once. In this way, simple ballot stuffing attacks\ncan be prevented. In ballot stuffing attacks, parties improve their reputation by\nmeans of positive ratings after fake transactions.\n\n2. Statistic-based filtering: Further techniques that are used to enhance the\nrobustness of trust models against the spread of false rumors apply statistical\npatterns. Whitby et al. [16], for example, proposed a statistical filter technique to\nfilter out unfair ratings in Bayesian reputation systems applying the majority rule.\nThe majority rule considers feedback that is far away from the majority’s referrals as\ndishonest. In this way, dishonest or false feedback can easily be detected and filtered.\n\n3. Clustering-based filtering: Clustering-based filter use cluster analysis approaches\nto identify unfair ratings. These approaches are comparatively expensive and\ntherefore rarely used as filtering techniques. An exemplary procedure is to analyze\nan advisor’s history. Since a rater never lies to himself, an obvious way to detect\nfalse ratings is to compare own experience with the advisor’s referrals. Thus, both\nfair and unfair ratings can be identified. iCLUB [17], for example, calculates\nclusters of advisors whose evaluations against other parties are alike. Then, the\ncluster being most similar to the own opinion is chosen as fair ratings. If there is no\ncommon experience (e.g. bootstrapping), the majority rule will be applied. Another\nexample for an approach using cluster filtering was proposed by Dellarocas [18].\n\nOnce all available information is reduced to those suitable for measuring trust and\nreputation in the current situation, it becomes clear that various data differ in their\ncharacteristics (e.g. context, reliability). Hence, the referrals are weighted in the second\nprocess step based on different factors. In contrast to the filtering step, applied techniques\ndiffer strongly. For that reason, our classification of weighting techniques is based on the\nproperties of referrals that are analyzed for the discounting. We distinguish between the\nfollowing classes:\n\n1. Context comparability: Reputation data is always bound to the specific context in\nwhich it is created. Ratings that are generated in one application area might not be\nautomatically applicable in another application area. In e-commerce, for instance,\ntransactions are accomplished involving different prices, product types, payment\n\n\n\nSänger et al. Journal of Trust Management (2015) 2:5 Page 8 of 21\n\nmethods, quality or time. The non-consideration of this context leads to the value\nimbalance problem where a malicious seller can build a high reputation by selling\ncheap products while cheating on expensive ones. To increase comparability and\navoid such situations, context has become a crucial attribute for many current\napproaches like [19] or [9].\n\n2. Criteria comparability: Besides the context in which feedback is created, the\ncriteria that underlie the evaluation are important. Particularly, if referrals from\ndifferent application areas or communities are integrated, criteria comparability\ncan be crucial. In file-sharing networks, for instance, a positive rating is often\ngranted with a successful transaction independent of the quality of service. On\ne-commerce platforms, in contrast, quality may be a critical factor for customer\nsatisfaction. Other distinctions could be the costs of reviews, the level of\nanonymity or the number of peers in different communities or application\nareas. Weighting based on criteria comparability can compensate these\ndifferences.\n\n3. Credibility/propagation: In network structures such as in the web-of-trust, trust\ncan be established along a recommendation or trust chain. Obviously, referrals that\nhave first-hand information about the trustworthiness of an agent are more\ncredible than referrals received at second-hand (with propagation degree of two) or\nhigher. Therefore, several models apply a propagation (transitivity) rate to discount\nreferrals based on their distance. The biometric identity trust model [20], for\ninstance, derives the reputation-factor from the distance of nodes in a web-of-trust.\n\n4. Reliability: Reliability or honesty of referrals can strongly affect the weight of\nreviews. The concept of feedback reputation that measures the agents’ reliability in\nterms of providing honest feedback is often applied. As a consequence, referrals\ncreated by agents having a low feedback reputation have a low impact on the\naggregated reputation. The bases for this calculation can be various. Google’s\nPageRank [21], for instance, involves the position of every website connected to the\ntrustee in the web graph in their recursive algorithm. Epinions [22], on the other\nhand, allows users to directly rate reviews and reviewers. In this way, the effects of\nunfair ratings are diminished.\n\n5. Rating value: Trust is event sensitive. For stronger punishment of bad behavior,\nthe weight of positive ratings compared to negative ratings can be calculated\nasymmetrically. An example for a model using an “adaptive forgetting scheme” was\nproposed by Sun et al. [23], in which good reputation can be built slowly through\ngood behavior but easily be ruined through bad behavior.\n\n6. Time: Due to the dynamic nature of trust, it has been widely recognized that time\nis one important factor for the weighting of referrals. Old feedback might not be as\nrelevant for reputation scoring as new referrals. An example measure for\ntime-based weighting is the “forgetting factor” proposed by Jøsang [24].\n\n7. Personal preferences: Reputation systems are used by various end-users (e.g.\nhuman decision makers, services). Therefore, a reputation system must allow the\nadaptation of its techniques to subjective personal preferences. Different actors\nmight have different perceptions regarding the importance of direct experience\nand referrals, the significance of distinct information sources or the rating of\nnewcomers.\n\n\n\nSänger et al. Journal of Trust Management (2015) 2:5 Page 9 of 21\n\nThe tuple of reputation data and weight-factor(s) serve as input for the third step of\nthe computation process - the aggregation. In this phase, one or several trust/reputa-\ntion values are calculated by composing the available information. In some cases, the\nweighting and the aggregation process are run through repetitively in an iterative manner.\nHowever, the single steps can still be logically separated. The list of proposed algorithms\nto aggregate trust and reputation values has become very long during the last decade.\nHere, we summarize the most common aggregation techniques and classify them into the\nfour blocks simple arithmetic, statistic, fuzzy and graph-based models:\n\n1. Simple arithmetic: The first class includes simple aggregation techniques like\nranking, summation or average. Ranking is a very basic way to measure\ntrustworthiness. In ranking algorithms, ratings are counted and organized in a\ndescending order based on that value. This measure has no exact reputation score.\nInstead, it is frequently used as a proxy for the relative importance/trustworthiness.\nExamples for systems using ranking algorithms are message boards like Slashdot\n[25] or citation counts used to calculate the impact factor in academic literature.\nOther aggregation techniques that are well known due to the implementation on\neBay or Amazon [26] are the summation (adding up positive and negative ratings)\nor the average of ratings. Summation, though, can easily be misleading, since a\nvalue of 90 does not reveal the composition of positive and negative ratings (e.g.\n+100,-10 or +90,0). The average, on the other hand, is a very intuitive and easily\nunderstandable algorithm.\n\n2. Statistic: Many of the prominent trust models proposed in the last years use a\nstatistical approach to provide a solid mathematical basis for trust management.\nApplied techniques range from Bayesian probability over belief models to Hidden\nMarkov Models. All models based on the beta probability density function (beta\nPDF) are examples for models simply using Bayesian probability. The beta PDF\nrepresents the probability distributions of binary events. The a priori reputation\nscore is thereby gradually updated by new ratings. The result is a reputation score\nthat is described in a beta PDF function parameter tuple (α, β), whereby α\n\nrepresents positive and β represents negative ratings. A well known model using\nthe beta PDF is the Beta Reputation system [24]. A weakness of Bayesian\nprobabilistic models, however, is that they cannot handle uncertainty. Therefore,\nbelief models extend the probabilistic approach by Dempster-Shafer theory (DST)\nor subjective logic to include the notion of uncertainty. Trust and reputation\nmodels involving a belief model were proposed by Jøsang [27] or Yu and Singh [28].\nMore complex solutions that are based on machine learning, use the Hidden\nMarkov Model, a generalization of the beta model, to better cope with the dynamic\nbehavior. An example was introduced by Malik et al. [29].\n\n3. Fuzzy: Aggregation techniques classified as fuzzy models use fuzzy logic to\ncalculate a reputation value. In contrast to classical logic, fuzzy logic allows to\nmodel truth or falsity within an interval of [0,1]. Thus, it can describe the degree to\nwhich an agent/resource is trustworthy or not trustworthy. Fuzzy logic has been\nproven to deal well with uncertainty and mimic the human decision making\nprocess [30]. Thereby, a linguistic approach is often applied. REGRET [31] is one\nprominent example of a trust model making use of fuzzy logic.\n\n\n\nSänger et al. Journal of Trust Management (2015) 2:5 Page 10 of 21\n\n4. Graph-based: A variety of trust models employ a graph-based approach. They rely\non different measures describing the position of nodes in a network involving the\nflow of transitive trust along trust chains in network structures. As online social\nnetworks have become popular as a medium for disseminating information and\nconnecting people, many models regarding trust in social networks have lately\nbeen proposed. Graph-based approaches use measures from the field of graph\ntheory such as centrality (e.g. Eigenvector, betweenness), distance or node-degree.\nReputation values, for instance, grow with the number of incoming edges (in-\ndegree) and increase or decrease with the number of outgoing edges (out-degree).\nThe impact of one edge on the overall reputation can depend on several factors like\nthe reputation of the node an edge comes from or the distance of two nodes.\nPopular algorithms using graph-based flow model are Google’s PageRank [21] as\nwell as the Eigentrust Algorithm [32]. Other examples are the web-of-trust or trust\nmodels particularly designed for social networks as described in [14]. As mentioned\nabove, the weighting and aggregation phases are incrementally run through for\nseveral times due to the incremental nature of these algorithms.\n\nThe classification of the computation engine’s components used in different trust mod-\nels in this taxonomy is not limited to one component of each primary class. Depending\non the computation process, several filtering, weighting and aggregation techniques can\nbe combined and run through more than once. Malik et al. [29], for instance, introduced\na hybrid model combining heuristic and statistical approaches. However, our taxonomy\ncan reveal the single logical components a computation engine is built on. Moreover,\nit serves as an overview of existing approaches. Since every currently known reputa-\ntion system can find its position, to the best of our knowledge, this taxonomy can be\nseen as complete. Though, an extension by new classes driven by novel models and\nideas is possible. Our hierarchic", "metadata_storage_path": "aHR0cHM6Ly9zdG9yYWdlYWNjb3VudGxpZ2lyay5ibG9iLmNvcmUud2luZG93cy5uZXQvcGFwZXIvczQwNDkzLTAxNS0wMDE1LTMucGRm0", "metadata_author": null, "metadata_title": null, "keyphrases": [ "Creative Commons Attribution License", "mobile ad hoc networks", "Günther Pernul", "Universitätsstraße", "descriptive scenario-based analysis", "general prob- lem", "one novel idea", "Open Access article", "single building blocks", "RESEARCH Open Access", "online reputation systems", "new reputation systems", "hierarchical component taxonomy", "most reputation systems", "common reputation models", "Johannes Sänger", "hierarchical taxonomy", "common systems", "peer networks", "social networks", "component repository", "common models", "Christian Richthammer", "various disciplines", "application areas", "promising approaches", "implementation level", "obvious utility", "practical point", "last decade", "wide range", "considerable research", "data accuracy", "several environments", "statistical approaches", "personal preferences", "software engineering", "unrestricted use", "effective use", "computation engines", "computation phase", "graph-based models", "natural framework", "design process", "design knowledge", "Design approaches", "computation methods", "Reusable components", "Trust Management", "Trust pattern", "reputation-based trust", "context information", "original work", "Journal", "DOI", "Correspondence", "saenger", "wiwi", "regensburg", "University", "Germany", "Abstract", "problem", "scratch", "concepts", "achievements", "others", "shuffle", "reuse", "respect", "order", "conceptual", "view", "properties", "literature", "aspects", "Keywords", "Reusability", "Introduction", "metrics", "commerce", "eBay", "buyers", "relevance", "quality", "arithmetic", "multiple", "factors", "propagation", "solutions", "authors", "proposals", "development", "attention", "licensee", "Springer", "terms", "creativecommons", "licenses", "distribution", "reproduction", "medium", "mailto", "Page", "community", "efits", "specialists", "reliability", "goal", "classification", "functions", "many applied computation techniques", "design science research paradigm", "design pattern-like solution", "rational choice mechanism", "general problem context", "multifaceted terms trust", "computational trust models", "current models", "reputation models", "building block", "web services", "research gap", "objec- tives", "same time", "increasing distribution", "decision making", "network environments", "common understanding", "various fields", "multidimension- ality", "abstract concept", "behavioral dimension", "interpersonal phenomenon", "particular level", "subjective probability", "Sänger", "Multiple authors", "plex definitions", "reputation systems", "conceptual level", "following section", "important artifacts", "bility trust", "particular action", "future work", "several properties", "means", "rest", "paper", "guidelines", "Hevner", "overview", "motivation", "approach", "contribution", "name", "plans", "success", "Internet", "connectivity", "notion", "regard", "objectives", "topic", "decades", "uniform", "Reasons", "circumstance", "credibility", "confidence", "emotional", "nature", "sociologists", "psychologists", "Economists", "online", "Gambetta", "distrust", "agent", "group", "capacity", "security", "risk", "variety", "Table", "basis", "two common ways", "multiple discrete levels", "hard security mechanism", "One negative experience", "Propagative One property", "one aggregated picture", "many trust models", "multiple trust chains", "several trust models", "several models", "propagative nature", "aggregated value", "different chains", "recent years", "new experiences", "time-based aging", "greater importance", "old experiences", "delicious meal", "rela- tionships", "final decision", "greater influence", "distance-based aging", "subjective nature", "high level", "binary manner", "maximum value", "human nature", "hard evidence", "one agent", "other agents", "untrustworthy agents", "Dynamic Trust", "Context-dependent Trust", "trust value", "overall trust", "trust information", "trust statements", "lower trust", "Reflexive Trust", "specific context", "particular restaurant", "specific aspects", "long time", "particular agent", "distant nodes", "reviewer Rachel", "book review", "trust transitivity", "Policy-based trust", "same context", "values", "Table 1", "Overview", "acteristics", "example", "Alice", "Bob", "doctor", "cook", "customer", "food", "service", "combination", "amount", "use", "propagativity", "turn", "Claire", "Christianson", "Harbison", "transitive", "reverse", "case", "Composable", "Composition", "zon", "opinion", "numbers", "continuous", "variable", "interval", "reinforcing", "trustworthiness", "consideration", "establishment", "exchange", "credentials", "contrast", "history", "interactions", "estimation", "three process steps filtering", "most reputation-based trust models", "three fundamental phases", "repu- tation system", "function-based component classification", "current reputation models", "existing trust models", "three steps", "existing models", "component classes", "generic process", "two steps", "existing approaches", "existing systems", "soft security", "collective measure", "global value", "personal experience", "many approvedmodels", "promising thoughts", "generic mechanism", "compo- nent", "accepted principles", "one hand", "other hand", "critical question", "communica- tion", "feedback generation/collection", "feedback distribution", "central part", "common trust", "feedback aggregation", "existing concepts", "academic community", "context setting", "single components", "reusable components", "trust properties", "Research gap", "worthiness", "work", "thing", "character", "referrals", "ratings", "reviews", "members", "someone", "ideas", "benefits", "years", "Rehak", "instance", "capabilities", "sound", "new", "repository", "artifacts", "developers", "researchers", "novel", "way", "implemented", "communication", "ability", "table", "computationmethods", "analysis", "identification", "logical", "Figure", "preparation", "storage", "weighting", "trustor", "personalisation trust relation reputation value", "filtering weighting aggregation communication input context", "novel hierarchical component taxonomy", "common rep- utation systems", "output trustor trustee transaction", "three generic process steps", "current trust models", "two broad categories", "subsequent aggre- gation", "situation Sänger", "reputation system computation collection", "Common reputation systems", "second process step", "single process steps", "several reputation scores", "first two steps", "trust/reputation value", "common examples", "filtering process", "computation process", "subsequent computing", "utation information", "specific situation", "second case", "reputation values", "first phase", "computation engine", "personalization parameters", "past behavior", "sonal experience", "other sources", "personal collections", "ferent peers", "distributed network", "different sources", "uniform format", "weight factors", "struc- ture", "decentralized environments", "functional blocks", "input data", "last phase", "reputation data", "first question", "hard selection", "soft selection", "information amount", "extra information", "available data", "Preparation techniques", "preparation phase", "public storage", "normalization", "need", "processing", "line", "Zhang", "difference", "algo", "rithm", "structure", "decentralized/hybrid", "meaning", "transparency", "design", "section", "detail", "classes", "propagation statistic statistic-based reliability rating value fuzzy time clustering-based graph-based personal preferences Sänger", "simple arithmetic attribute-based criteria compatibility credibility", "three broad classes attribute-based, statistic- based", "simple ballot stuffing attacks", "weighting aggregation context comparability", "Statistic-based filtering", "statistical filter technique", "one application area", "reference value", "Clustering-based filter", "Bayesian reputation systems", "cluster analysis approaches", "Attribute-based filtering", "Attribute-based filters", "simple logic", "secondary classes", "following classes", "weighting phase", "Figure 2 Classes", "statistical patterns", "filtering step", "weighting techniques", "Other models", "different surveys", "filtering phase", "single attributes", "huge amounts", "initial filtering", "last 12 months", "positive, neutral", "false rumors", "exemplary procedure", "cluster filtering", "available information", "current situation", "various data", "different factors", "reputation calculation", "Reputation data", "negative ratings", "positive ratings", "unfair ratings", "false ratings", "Further techniques", "majority rule", "filtering techniques", "fake transactions", "false feedback", "other parties", "common experience", "obvious way", "primary", "aggregation-techniques", "constraint-factor", "lightweight", "Sporas", "party", "robustness", "spread", "Whitby", "dishonest", "advisor", "rater", "iCLUB", "clusters", "evaluations", "bootstrapping", "Dellarocas", "characteristics", "reason", "discounting", "1.", "several trust/reputa- tion values", "biometric identity trust model", "human decision makers", "adaptive forgetting scheme", "distinct information sources", "payment Sänger", "one important factor", "different application areas", "low feedback reputation", "forgetting factor", "low impact", "critical factor", "first-hand information", "weight-factor(s", "different prices", "Different actors", "different perceptions", "high reputation", "aggregated reputation", "good reputation", "reputation scoring", "Reputation systems", "product types", "imbalance problem", "malicious seller", "cheap products", "crucial attribute", "many current", "file-sharing networks", "successful transaction", "customer satisfaction", "network structures", "transitivity) rate", "honest feedback", "recursive algorithm", "stronger punishment", "bad behavior", "good behavior", "dynamic nature", "Old feedback", "Jøsang", "Personal preferences", "various end-users", "direct experience", "third step", "different communities", "trust chain", "2. Criteria comparability", "Other distinctions", "propagation degree", "example measure", "web graph", "time-based weighting", "Rating value", "new referrals", "agents’ reliability", "transactions", "methods", "non", "context", "expensive", "situations", "approaches", "evaluation", "costs", "level", "number", "peers", "differences", "recommendation", "second", "distance", "reputation-factor", "nodes", "honesty", "concept", "consequence", "bases", "calculation", "Google", "position", "website", "trustee", "Epinions", "reviewers", "effects", "Sun", "adaptation", "techniques", "importance", "significance", "newcomers", "tuple", "input", "aggregation", "phase", "cases", "4.", "5.", "beta PDF function parameter tuple", "human decision making process", "beta probability density function", "solid mathematical basis", "More complex solutions", "The beta PDF", "common aggregation techniques", "Beta Reputation system", "online social networks", "exact reputation score", "priori reputation score", "simple aggregation techniques", "Other aggregation techniques", "Bayesian probabilistic models", "Hidden Markov Model", "prominent trust models", "aggregation process", "beta model", "Bayesian probability", "Applied techniques", "Simple arithmetic", "probability distributions", "probabilistic approach", "Markov Models", "iterative manner", "single steps", "first class", "basic way", "descending order", "message boards", "citation counts", "impact factor", "academic literature", "understandable algorithm", "last years", "statistical approach", "binary events", "Dempster-Shafer theory", "subjective logic", "belief model", "machine learning", "dynamic behavior", "classical logic", "model truth", "linguistic approach", "prominent example", "graph-based approach", "graph theory", "many models", "trust management", "transitive trust", "trust chains", "fuzzy logic", "new ratings", "fuzzy models", "different measures", "ranking algorithms", "summation", "proxy", "Examples", "systems", "Slashdot", "implementation", "Amazon", "positive", "average", "composition", "result", "weakness", "uncertainty", "DST", "Yu", "Singh", "generalization", "Malik", "falsity", "degree", "agent/resource", "REGRET", "flow", "information", "people", "field", "centrality", "Eigenvector", "betweenness", "α", "β", "different trust mod- els", "reputa- tion system", "graph-based flow model", "single logical components", "hybrid model", "trust models", "incoming edges", "outgoing edges", "several factors", "two nodes", "Eigentrust Algorithm", "Other examples", "aggregation phases", "incremental nature", "one component", "primary class", "aggregation techniques", "new classes", "novel models", "Reputation values", "overall reputation", "one edge", "Popular algorithms", "impact", "PageRank", "taxonomy", "heuristic", "knowledge", "extension", "hierarchic" ], "merged_content": "\nSänger et al. Journal of Trust Management (2015) 2:5 \nDOI 10.1186/s40493-015-0015-3\n\nRESEARCH Open Access\n\nReusable components for online reputation\nsystems\nJohannes Sänger*, Christian Richthammer and Günther Pernul\n\n*Correspondence:\njohannes.saenger@wiwi.\nuni-regensburg.de\nUniversity of Regensburg,\nUniversitätsstraße 31, 93053\nRegensburg, Germany\n\nAbstract\n\nReputation systems have been extensively explored in various disciplines and\napplication areas. A problem in this context is that the computation engines applied by\nmost reputation systems available are designed from scratch and rarely consider well\nestablished concepts and achievements made by others. Thus, approved models and\npromising approaches may get lost in the shuffle. In this work, we aim to foster reuse in\nrespect of trust and reputation systems by providing a hierarchical component\ntaxonomy of computation engines which serves as a natural framework for the design\nof new reputation systems. In order to assist the design process we, furthermore,\nprovide a component repository that contains design knowledge on both a\nconceptual and an implementation level. To evaluate our approach we conduct a\ndescriptive scenario-based analysis which shows that it has an obvious utility from a\npractical point of view. Matching the identified components and the properties of trust\nintroduced in literature, we finally show which properties of trust are widely covered by\ncommon models and which aspects have only rarely been considered so far.\n\nKeywords: Trust; Reputation; Reusability; Trust pattern\n\nIntroduction\nIn the last decade, trust and reputation have been extensively explored in various disci-\nplines and application areas. Thereby, a wide range of metrics and computation methods\nfor reputation-based trust has been proposed. While most common systems have been\nintroduced in e-commerce, such as eBay’s reputation system [1] that allows to rate sell-\ners and buyers, considerable research has also been done in the context of peer-to-peer\nnetworks, mobile ad hoc networks, social networks or ensuring data accuracy, relevance\nand quality in several environments [2]. Computation methods applied range from sim-\nple arithmetic over statistical approaches up to graph-based models involving multiple\nfactors such as context information, propagation or personal preferences. A general prob-\nlem is that most of the newly introduced trust and reputation models use computation\nmethods that are designed from scratch and rely on one novel idea which could lead to\nbetter solutions [3]. Only a few authors build on proposals of others. Therefore, approved\nmodels and promising approaches may get lost in the shuffle.\nIn this work, we aim to encourage reuse in the development of reputation systems by\n\nproviding a framework for creating reputation systems based on reusable components.\nDesign approaches for reuse have been given much attention in the software engineering\n\n© 2015 Sänger et al.; licensee Springer. This is an Open Access article distributed under the terms of the Creative Commons\nAttribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction\nin any medium, provided the original work is properly credited.\n\nmailto: johannes.saenger@wiwi.uni-regensburg.de\nmailto: johannes.saenger@wiwi.uni-regensburg.de\nhttp://creativecommons.org/licenses/by/4.0\n\n\nSänger et al. Journal of Trust Management (2015) 2:5 Page 2 of 21\n\ncommunity. The research in trust and reputation systems could also profit from ben-\nefits like effective use of specialists, accelerated development and increased reliability.\nToward this goal, we propose a hierarchical taxonomy for components of computation\nengines used in reputation systems. Thereto, we decompose the computation phase of\ncommon reputation models to derive single building blocks. The classification based on\ntheir functions serves as a natural framework for the design of new reputation systems.\nMoreover, we set up a component repository containing artifacts on both a conceptual\nand an implementation level to facilitate the reuse of the identified components. On the\nconceptual level, we describe each building block as a design pattern-like solution. On\nthe implementation level, we provide already implemented components by means of web\nservices.\nThe rest of this paper is based on the design science research paradigm involving the\n\nguidelines for conducting design science research by Hevner et al. [4] and organized as\nfollows: Firstly, we give an overview of the general problem context as well as the relevance\nand motivation of our work. Thereby, we identify the research gap and define the objec-\ntives of our research. In the following section, we introduce our hierarchical component\ntaxonomy of computation engines used in reputation systems. After that, we point out\nhow our component repository is conceptually designed and implemented. Subsequently,\nwe carry out a descriptive scenario-based analysis of our approach. At the same time, we\nmatch all components identified with the properties of trust introduced in literature. We\nshow which properties of trust are widely covered by common models and which aspects\nhave only rarely been considered so far. Finally, we summarize the contribution and name\nour plans for future work.\n\nProblem context andmotivation\nWith the success of the Internet and the increasing distribution and connectivity, trust\nand reputation systems have become important artifacts to support decision making in\nnetwork environments. To impart a common understanding, we firstly provide a defi-\nnition of the notion of trust. At the same time, we explain the properties of trust that\nare important with regard to this work. Then, we point out how trust can be established\napplying computational trust models. Focusing on reputation-based trust, we explain how\nand why the research in reputation models could profit from reuse. Thereby, we identify\nthe research gap and define the objectives of this work.\n\nThe notion of trust and its properties\n\nThe notion of trust is a topic that has been discussed in research for decades. Although\nit has been intensively examined in various fields, it still lacks a uniform and generally\naccepted definition. Reasons for this circumstance are the multifaceted terms trust is\nassociated with like credibility, reliability or confidence as well as the multidimension-\nality of trust as an abstract concept that has a cognitive, an emotional and a behavioral\ndimension. As pointed out by [5], trust has been described as being structural in\nnature by sociologists while psychologists viewed trust as an interpersonal phenomenon.\nEconomists, however, interpreted trust as a rational choice mechanism. The definition\noften cited in literature regarding trust and reputation online that is referred to as relia-\nbility trust was proposed by Gambetta in 1988 [6]: “Trust (or, symmetrically, distrust) is\na particular level of the subjective probability with which an agent assesses that another\n\n\n\nSänger et al. Journal of Trust Management (2015) 2:5 Page 3 of 21\n\nagent or group of agents will perform a particular action, both before he can monitor such\naction (or independently of his capacity ever to be able to monitor it) and in a context in\nwhich it affects his own action.”\nMultiple authors furthermore include security and risk which can lead to more com-\n\nplex definitions. Anyway, it is generally agreed that trust is multifaceted and dependent\non a variety of factors. Moreover, there are several properties of trust described in lit-\nerature (see Table 1). These properties are important with respect to this work because\nthey form the basis for many applied computation techniques in trust and reputation\nsystems described in Section ‘Hierarchical component taxonomy’. Reusable components\ncould extend current models by the ability to gradually include these properties.\n\nReputation-based trust\n\nIn recent years, several trust models have been developed to establish trust. Thereby,\ntwo common ways can be distinguished, namely policy-based and reputation-based trust\n\nTable 1 Overview of properties of trust described in literature [14,41-46]\n\nDynamic Trust can increase or decrease through gathering new experiences. Moreover,\ntrust is said to decay with time (time-based aging [45]). Because of these char-\nacteristics, trust values strongly depend on the time they are determined. The\ngreater importance of new experiences compared to old experiences has been\nwidely studied and considered in many trust models such as [32,47] or [30].\n\nContext-dependent Trust is bound to a specific context. For example, Alice trusts Bob as her doctor.\nHowever, she might not trust him as a cook to prepare a delicious meal for her.\n\nMulti-faceted Even in the same context, a trust value may not reflect all aspects of this context\n[43]. For example, a customer may trust a particular restaurant for its quality of\nfood but not for its quality of service. The overall trust on this restaurant depends\non the combination of the amount of trust in the specific aspects.\n\nPropagative One property of trust made use of in several models is its propagativity. If Alice\ntrusts Bob, who in turn trusts Claire, Alice can derive trust on Claire from the rela-\ntionships between her and Bob as well as between Bob and Claire. Because of\nthis propagative nature, it is possible to create trust chains passing trust from\none agent to another agent. As clarified by Christianson and Harbison [48], trust\nis not automatically transitive although trust transitivity was assumed proven for\na long time. If Alice trusts Bob, who in turn trusts Claire, it does not inherently\nmean that Alice trusts Claire. It follows from the foregoing that transitivity implies\npropagation. The reverse, though, is not the case.\n\nComposable When trust is propagated, a particular agent may be connected to multiple\ntrust chains. To come up with a final decision whether to trust or distrust this\nagent, the trust information received from the different chains need to be com-\nposed in order to build one aggregated picture. In this context, trust statements\npropagated from nodes close to oneself should have greater influence on the\naggregated value than the ones from distant nodes (distance-based aging [45]).\nComposition is potentially difficult if the trust statements are contradictory [14].\n\nSubjective The subjective nature of trust becomes clear if one thinks about a review on Ama-\nzon [26]. A book review that totally reflects Alice’s opinion will probably resolve\nin a high level of trust against the reviewer Rachel. Bob, however, who disagrees\nwith the review, will have a lower trust in Rachel although it bases on the same\nevidence.\n\nFine-grained Although trust is sometimes modeled in a binary manner (i.e. either trust or dis-\ntrust), it is possible that Alice trusts both Bob and Claire but that she trusts Bob\nmore than Claire. Hence, there may be multiple discrete levels of trust such as\nhigh, medium and low [41]. Mapped to numbers, trust may also be a continuous\nvariable taking values within a certain interval (e.g. between 0 and 1).\n\nEvent-sensitive It can take a long time to build trust. One negative experience, though, can\ndestroy it [23].\n\nReflexive Trust in oneself is always at the maximum value.\n\nSelf-reinforcing It is human nature to preferentially interact with other agents that are trusted.\nAnalogously, agents will avoid interacting with untrustworthy agents. Thus, the\ntrustworthiness of other agents is inherently taken into consideration.\n\n\n\nSänger et al. Journal of Trust Management (2015) 2:5 Page 4 of 21\n\nestablishment [7]. Policy-based trust is often referred to as a hard security mechanism due\nto the exchange of hard evidence (e.g. credentials). Reputation-based trust, in contrast, is\nderived from the history of interactions. Hence, it can be seen as an estimation of trust-\nworthiness (soft security). In this work, we focus on reputation-based trust. Reputation\nis defined as follows: “Reputation is what is generally said or believed about a person’s or\nthing’s character or standing.” [8].\nIt is based on referrals, ratings or reviews from members of a community. Therefore,\n\nit can be considered as a collective measure of trustworthiness [8]. Trustworthiness as a\nglobal value is objective. However, the trust an agent puts in someone or something as a\ncombination of personal experience and referrals is subjective.\n\nResearch gap: design of reputation systems with reuse\n\nIt has been argued (e.g. by [3]) that most reputation-based trust models proposed in the\nacademic community are built from scratch and do not rely on existing approaches. Only\na few authors continue their research on the ideas of others. Thus, many approvedmodels\nand promising thoughts go unregarded. The benefits of reuse, though, have been rec-\nognized in software engineering for years. However, there are only very few works that\nproposed single components to enhance existing approaches. Rehak et al. [9], for instance,\nintroduced a generic mechanism that can be combined with existing trust models to\nextend their capabilities by efficiently modeling context. The benefits of such a compo-\nnent that can easily be combined with existing systems are obvious. Nonetheless, research\nin trust and reputation still lacks in sound and accepted principles to foster reuse.\nTo gradually close this gap, we aim to provide a framework for the design of new\n\nreputation systems with reuse. As described above, we thereto propose a hierarchical\ncomponent taxonomy of computation engines used in reputation systems. Based on this\ntaxonomy, we set up a repository containing design knowledge on both a conceptual\nand an implementation level. On the one hand, the uniform and well-structured artifacts\ncollected in this repository can be used by developers to select, understand and apply\nexisting concepts. On the other hand, they may encourage researchers to provide novel\ncomponents on a conceptual and an implementation level. In this way, the reuse of ideas,\nconcepts and implemented components as well as the communication of reuse knowledge\nshould be achieved. Furthermore, we argue that the reusable components we identify in\nthis work could extend current reputation models by the ability to gradually include the\nproperties of trust described above. To evaluate whether our taxonomy/framework can\ncover all aspects of trust, we finally provide a table matching our component classes with\ntrust properties.\n\nA hierarchical component taxonomy for computationmethods in reputation\nsystems\nTo derive a taxonomy from existing models, our research includes two steps: (1) the\nanalysis of the generic process of reputation systems and (2) the identification of logical\ncomponents of the computation methods used in common trust and reputation models.\nA critical question is how to determine and classify single components. Thereto, we follow\nan approach to function-based component classification, which means that the taxonomy\nis derived from the functions the identified components fulfill.\n\n\n\nSänger et al. Journal of Trust Management (2015) 2:5 Page 5 of 21\n\nThe generic process of reputation systems\n\nThe generic process of reputation systems, as depicted in Figure 1, can be divided into\nthree steps: (1) collection & preparation, (2) computation and (3) storage & communica-\ntion. These steps are adapted from the three fundamental phases of reputation systems\nidentified by [10] and [11]: feedback generation/collection, feedback aggregation and\nfeedback distribution. Feedback aggregation as the central part of every trust and repu-\ntation system is furthermore divided into the three process steps filtering, weighting and\naggregation taken together as computation. The context setting consists of a trustor who\nwants to build a trust relation toward a trustee by providing context and personalization\nparameters and receiving a trustee’s reputation value.\n\nCollection and preparation\n\nIn the collection and preparation phase, the reputation system gleans information about\nthe past behavior of a trustee and prepares it for subsequent computing. Although per-\nsonal experience is the most reliable, it is often not sufficiently available or nonexistent.\nTherefore, data from other sources needs to be collected. These can be various, ranging\nfrom public or personal collections of data centrally stored to data requested from dif-\nferent peers in a distributed network. After all available data is gathered, it is prepared\nfor further use. Preparation techniques include normalization, for instance, which brings\nthe input data from different sources into a uniform format. Once the preparation is\ncompleted, the reputation data serves as input for the computation phase.\n\nComputation\n\nThe computation phase is the central part of every reputation system and takes the rep-\nutation information collected as input and generates a trust/reputation value as output.\nThis phase can be divided into the three generic process steps filtering, weighting and\naggregation. Depending on the computation engine, not all steps have to be implemented.\nThe first two steps (filtering and weighting) preprocess the data for the subsequent aggre-\ngation. The need for these steps is obvious: The first question to be answered is which\ninformation is useful for further processing (filtering). The second process step concerns\nthe question of how relevant the information is for the specific situation (weighting). In\nline with this, Zhang et al. [12] pointed out that current trust models can be classified into\nthe two broad categories filtering-based and discounting-based. The difference between\nfiltering and weighting is that the filtering process reduces the information amount while\n\nFigure 1 Generic process of a reputation system, inspired by [10].\n\n reputation system computation collection & storage & preparation filtering weighting aggregation communication input context, personalisation trust relation reputation value(s) output trustor trustee transaction/situation \n\n\n\nSänger et al. Journal of Trust Management (2015) 2:5 Page 6 of 21\n\nit is enriched by weight factors in the second case. Therefore, filtering can be seen as\nhard selection while weighting is more like a soft selection. Finally, the reputation values\nare aggregated to calculate one or several reputation scores. Depending on the algo-\nrithm, the whole computation process or single process steps can be run through for\nmultiple times.\n\nStorage and communication\n\nAfter reputation scores are calculated, they are either stored locally, in a public storage\nor both depending on the structure (centralized/decentralized/hybrid) of the reputation\nsystem. Common reputation systems not only provide the reputation scores but also offer\nextra information to help the end-users understand the meaning of a score. They should\nfurthermore reveal the computation process to accomplish transparency.\nIn this work, we focus on the computation phase, since the first phase (collection &\n\npreparation) and the last phase (storage & communication) strongly depend on the struc-\nture of the reputation system (centralized or decentralized). The computation phase,\nhowever, is independent of the structure and can look alike for systems implemented in\nboth centralized and decentralized environments. Therefore, it works well for design with\nreuse.\n\nHierarchical component taxonomy\n\nIn this section, the computation process is examined in detail. We introduce a novel\nhierarchical component taxonomy that is based on the functional blocks of common rep-\nutation systems identified in this work. Thereto, we clarify the objectives of the identified\nclasses (functions) and name common examples. Our analysis and selection of reputa-\ntion systems is based on different surveys [2,3,8,13,14]. Figure 2 gives an overview of the\nprimary and secondary classes identified.\n\nFigure 2 Classes of filtering-, weighting- and aggregation-techniques.\n\n filtering weighting aggregation context comparability simple arithmetic attribute-based criteria compatibility credibility/propagation statistic statistic-based reliability rating value fuzzy time clustering-based graph-based personal preferences \n\n\n\nSänger et al. Journal of Trust Management (2015) 2:5 Page 7 of 21\n\nBeginning with the filtering phase, the three broad classes attribute-based, statistic-\nbased and clustering-based filtering can be identified:\n\n1. Attribute-based filtering: In several trust models, input data is filtered based on a\nconstraint-factor defined for the value of single attributes. Attribute-based filters\nmostly implement a very simple logic, in which an attribute is usually compared to\na reference value. Due to their lightweight, they are proper for reducing huge\namounts of input data to the part necessary for the reputation calculation. Besides\nthe initial filtering of input data, it is often applied after the weighting phase in\norder to filter referrals that have been strongly discounted. Time is an example of\nan attribute that is often constrained because it is desirable to disregard very old\nratings. eBay’s reputation system, for instance, only considers transactions having\noccurred in the last 12 months for their overview of positive, neutral and negative\nratings. Other models such as Sporas [15] ignore every referral but the latest, if one\nparty rated another party more than once. In this way, simple ballot stuffing attacks\ncan be prevented. In ballot stuffing attacks, parties improve their reputation by\nmeans of positive ratings after fake transactions.\n\n2. Statistic-based filtering: Further techniques that are used to enhance the\nrobustness of trust models against the spread of false rumors apply statistical\npatterns. Whitby et al. [16], for example, proposed a statistical filter technique to\nfilter out unfair ratings in Bayesian reputation systems applying the majority rule.\nThe majority rule considers feedback that is far away from the majority’s referrals as\ndishonest. In this way, dishonest or false feedback can easily be detected and filtered.\n\n3. Clustering-based filtering: Clustering-based filter use cluster analysis approaches\nto identify unfair ratings. These approaches are comparatively expensive and\ntherefore rarely used as filtering techniques. An exemplary procedure is to analyze\nan advisor’s history. Since a rater never lies to himself, an obvious way to detect\nfalse ratings is to compare own experience with the advisor’s referrals. Thus, both\nfair and unfair ratings can be identified. iCLUB [17], for example, calculates\nclusters of advisors whose evaluations against other parties are alike. Then, the\ncluster being most similar to the own opinion is chosen as fair ratings. If there is no\ncommon experience (e.g. bootstrapping), the majority rule will be applied. Another\nexample for an approach using cluster filtering was proposed by Dellarocas [18].\n\nOnce all available information is reduced to those suitable for measuring trust and\nreputation in the current situation, it becomes clear that various data differ in their\ncharacteristics (e.g. context, reliability). Hence, the referrals are weighted in the second\nprocess step based on different factors. In contrast to the filtering step, applied techniques\ndiffer strongly. For that reason, our classification of weighting techniques is based on the\nproperties of referrals that are analyzed for the discounting. We distinguish between the\nfollowing classes:\n\n1. Context comparability: Reputation data is always bound to the specific context in\nwhich it is created. Ratings that are generated in one application area might not be\nautomatically applicable in another application area. In e-commerce, for instance,\ntransactions are accomplished involving different prices, product types, payment\n\n\n\nSänger et al. Journal of Trust Management (2015) 2:5 Page 8 of 21\n\nmethods, quality or time. The non-consideration of this context leads to the value\nimbalance problem where a malicious seller can build a high reputation by selling\ncheap products while cheating on expensive ones. To increase comparability and\navoid such situations, context has become a crucial attribute for many current\napproaches like [19] or [9].\n\n2. Criteria comparability: Besides the context in which feedback is created, the\ncriteria that underlie the evaluation are important. Particularly, if referrals from\ndifferent application areas or communities are integrated, criteria comparability\ncan be crucial. In file-sharing networks, for instance, a positive rating is often\ngranted with a successful transaction independent of the quality of service. On\ne-commerce platforms, in contrast, quality may be a critical factor for customer\nsatisfaction. Other distinctions could be the costs of reviews, the level of\nanonymity or the number of peers in different communities or application\nareas. Weighting based on criteria comparability can compensate these\ndifferences.\n\n3. Credibility/propagation: In network structures such as in the web-of-trust, trust\ncan be established along a recommendation or trust chain. Obviously, referrals that\nhave first-hand information about the trustworthiness of an agent are more\ncredible than referrals received at second-hand (with propagation degree of two) or\nhigher. Therefore, several models apply a propagation (transitivity) rate to discount\nreferrals based on their distance. The biometric identity trust model [20], for\ninstance, derives the reputation-factor from the distance of nodes in a web-of-trust.\n\n4. Reliability: Reliability or honesty of referrals can strongly affect the weight of\nreviews. The concept of feedback reputation that measures the agents’ reliability in\nterms of providing honest feedback is often applied. As a consequence, referrals\ncreated by agents having a low feedback reputation have a low impact on the\naggregated reputation. The bases for this calculation can be various. Google’s\nPageRank [21], for instance, involves the position of every website connected to the\ntrustee in the web graph in their recursive algorithm. Epinions [22], on the other\nhand, allows users to directly rate reviews and reviewers. In this way, the effects of\nunfair ratings are diminished.\n\n5. Rating value: Trust is event sensitive. For stronger punishment of bad behavior,\nthe weight of positive ratings compared to negative ratings can be calculated\nasymmetrically. An example for a model using an “adaptive forgetting scheme” was\nproposed by Sun et al. [23], in which good reputation can be built slowly through\ngood behavior but easily be ruined through bad behavior.\n\n6. Time: Due to the dynamic nature of trust, it has been widely recognized that time\nis one important factor for the weighting of referrals. Old feedback might not be as\nrelevant for reputation scoring as new referrals. An example measure for\ntime-based weighting is the “forgetting factor” proposed by Jøsang [24].\n\n7. Personal preferences: Reputation systems are used by various end-users (e.g.\nhuman decision makers, services). Therefore, a reputation system must allow the\nadaptation of its techniques to subjective personal preferences. Different actors\nmight have different perceptions regarding the importance of direct experience\nand referrals, the significance of distinct information sources or the rating of\nnewcomers.\n\n\n\nSänger et al. Journal of Trust Management (2015) 2:5 Page 9 of 21\n\nThe tuple of reputation data and weight-factor(s) serve as input for the third step of\nthe computation process - the aggregation. In this phase, one or several trust/reputa-\ntion values are calculated by composing the available information. In some cases, the\nweighting and the aggregation process are run through repetitively in an iterative manner.\nHowever, the single steps can still be logically separated. The list of proposed algorithms\nto aggregate trust and reputation values has become very long during the last decade.\nHere, we summarize the most common aggregation techniques and classify them into the\nfour blocks simple arithmetic, statistic, fuzzy and graph-based models:\n\n1. Simple arithmetic: The first class includes simple aggregation techniques like\nranking, summation or average. Ranking is a very basic way to measure\ntrustworthiness. In ranking algorithms, ratings are counted and organized in a\ndescending order based on that value. This measure has no exact reputation score.\nInstead, it is frequently used as a proxy for the relative importance/trustworthiness.\nExamples for systems using ranking algorithms are message boards like Slashdot\n[25] or citation counts used to calculate the impact factor in academic literature.\nOther aggregation techniques that are well known due to the implementation on\neBay or Amazon [26] are the summation (adding up positive and negative ratings)\nor the average of ratings. Summation, though, can easily be misleading, since a\nvalue of 90 does not reveal the composition of positive and negative ratings (e.g.\n+100,-10 or +90,0). The average, on the other hand, is a very intuitive and easily\nunderstandable algorithm.\n\n2. Statistic: Many of the prominent trust models proposed in the last years use a\nstatistical approach to provide a solid mathematical basis for trust management.\nApplied techniques range from Bayesian probability over belief models to Hidden\nMarkov Models. All models based on the beta probability density function (beta\nPDF) are examples for models simply using Bayesian probability. The beta PDF\nrepresents the probability distributions of binary events. The a priori reputation\nscore is thereby gradually updated by new ratings. The result is a reputation score\nthat is described in a beta PDF function parameter tuple (α, β), whereby α\n\nrepresents positive and β represents negative ratings. A well known model using\nthe beta PDF is the Beta Reputation system [24]. A weakness of Bayesian\nprobabilistic models, however, is that they cannot handle uncertainty. Therefore,\nbelief models extend the probabilistic approach by Dempster-Shafer theory (DST)\nor subjective logic to include the notion of uncertainty. Trust and reputation\nmodels involving a belief model were proposed by Jøsang [27] or Yu and Singh [28].\nMore complex solutions that are based on machine learning, use the Hidden\nMarkov Model, a generalization of the beta model, to better cope with the dynamic\nbehavior. An example was introduced by Malik et al. [29].\n\n3. Fuzzy: Aggregation techniques classified as fuzzy models use fuzzy logic to\ncalculate a reputation value. In contrast to classical logic, fuzzy logic allows to\nmodel truth or falsity within an interval of [0,1]. Thus, it can describe the degree to\nwhich an agent/resource is trustworthy or not trustworthy. Fuzzy logic has been\nproven to deal well with uncertainty and mimic the human decision making\nprocess [30]. Thereby, a linguistic approach is often applied. REGRET [31] is one\nprominent example of a trust model making use of fuzzy logic.\n\n\n\nSänger et al. Journal of Trust Management (2015) 2:5 Page 10 of 21\n\n4. Graph-based: A variety of trust models employ a graph-based approach. They rely\non different measures describing the position of nodes in a network involving the\nflow of transitive trust along trust chains in network structures. As online social\nnetworks have become popular as a medium for disseminating information and\nconnecting people, many models regarding trust in social networks have lately\nbeen proposed. Graph-based approaches use measures from the field of graph\ntheory such as centrality (e.g. Eigenvector, betweenness), distance or node-degree.\nReputation values, for instance, grow with the number of incoming edges (in-\ndegree) and increase or decrease with the number of outgoing edges (out-degree).\nThe impact of one edge on the overall reputation can depend on several factors like\nthe reputation of the node an edge comes from or the distance of two nodes.\nPopular algorithms using graph-based flow model are Google’s PageRank [21] as\nwell as the Eigentrust Algorithm [32]. Other examples are the web-of-trust or trust\nmodels particularly designed for social networks as described in [14]. As mentioned\nabove, the weighting and aggregation phases are incrementally run through for\nseveral times due to the incremental nature of these algorithms.\n\nThe classification of the computation engine’s components used in different trust mod-\nels in this taxonomy is not limited to one component of each primary class. Depending\non the computation process, several filtering, weighting and aggregation techniques can\nbe combined and run through more than once. Malik et al. [29], for instance, introduced\na hybrid model combining heuristic and statistical approaches. However, our taxonomy\ncan reveal the single logical components a computation engine is built on. Moreover,\nit serves as an overview of existing approaches. Since every currently known reputa-\ntion system can find its position, to the best of our knowledge, this taxonomy can be\nseen as complete. Though, an extension by new classes driven by novel models and\nideas is possible. Our hierarchic", "text": [ "reputation system computation collection & storage & preparation filtering weighting aggregation communication input context, personalisation trust relation reputation value(s) output trustor trustee transaction/situation", "filtering weighting aggregation context comparability simple arithmetic attribute-based criteria compatibility credibility/propagation statistic statistic-based reliability rating value fuzzy time clustering-based graph-based personal preferences", "context comparability simple arithmetic attribute-based criteria compatibility credibility/propagation statistic statistic-based reliability rating value fuzzy time clustering-based service repository (implementation level)- graph-based personal preferences knowledge repository (conceptual level) filtering weighting aggregation", "Client Server filtering AgeBasedAbsolute Clustering implement/ inherit weighting CongruenceAbsolute Webservice Webservice abstract CallHelper CallHandler Component TimeDiscounting Relative aggregation CongruenceAbsolute TimeDiscounting Relative", "Filter: Weighting: referral reduced set age-based context similarity weighted referral Aggregation: referral reputation filter (absolut) (absolute set average value congruence) set", "Published online: 13 May 2015" ], "layoutText": [ "{\"language\":\"en\",\"text\":\"reputation system computation collection & storage & preparation filtering weighting aggregation communication input context, personalisation trust relation reputation value(s) output trustor trustee transaction/situation\",\"lines\":[{\"boundingBox\":[{\"x\":1075,\"y\":9},{\"x\":1242,\"y\":8},{\"x\":1242,\"y\":32},{\"x\":1075,\"y\":33}],\"text\":\"reputation system\"},{\"boundingBox\":[{\"x\":612,\"y\":98},{\"x\":746,\"y\":97},{\"x\":746,\"y\":121},{\"x\":612,\"y\":122}],\"text\":\"computation\"},{\"boundingBox\":[{\"x\":182,\"y\":148},{\"x\":308,\"y\":146},{\"x\":308,\"y\":169},{\"x\":182,\"y\":171}],\"text\":\"collection &\"},{\"boundingBox\":[{\"x\":1065,\"y\":148},{\"x\":1174,\"y\":147},{\"x\":1174,\"y\":172},{\"x\":1066,\"y\":173}],\"text\":\"storage &\"},{\"boundingBox\":[{\"x\":181,\"y\":179},{\"x\":305,\"y\":177},{\"x\":305,\"y\":200},{\"x\":181,\"y\":202}],\"text\":\"preparation\"},{\"boundingBox\":[{\"x\":442,\"y\":170},{\"x\":521,\"y\":173},{\"x\":520,\"y\":197},{\"x\":441,\"y\":194}],\"text\":\"filtering\"},{\"boundingBox\":[{\"x\":636,\"y\":172},{\"x\":742,\"y\":172},{\"x\":741,\"y\":198},{\"x\":636,\"y\":197}],\"text\":\"weighting\"},{\"boundingBox\":[{\"x\":817,\"y\":174},{\"x\":945,\"y\":172},{\"x\":945,\"y\":195},{\"x\":818,\"y\":197}],\"text\":\"aggregation\"},{\"boundingBox\":[{\"x\":1036,\"y\":177},{\"x\":1200,\"y\":177},{\"x\":1200,\"y\":199},{\"x\":1036,\"y\":200}],\"text\":\"communication\"},{\"boundingBox\":[{\"x\":6,\"y\":425},{\"x\":54,\"y\":424},{\"x\":54,\"y\":445},{\"x\":6,\"y\":446}],\"text\":\"input\"},{\"boundingBox\":[{\"x\":261,\"y\":417},{\"x\":479,\"y\":416},{\"x\":479,\"y\":438},{\"x\":261,\"y\":439}],\"text\":\"context, personalisation\"},{\"boundingBox\":[{\"x\":619,\"y\":417},{\"x\":735,\"y\":417},{\"x\":735,\"y\":436},{\"x\":619,\"y\":436}],\"text\":\"trust relation\"},{\"boundingBox\":[{\"x\":918,\"y\":416},{\"x\":1094,\"y\":415},{\"x\":1094,\"y\":439},{\"x\":918,\"y\":439}],\"text\":\"reputation value(s)\"},{\"boundingBox\":[{\"x\":1220,\"y\":423},{\"x\":1284,\"y\":422},{\"x\":1284,\"y\":443},{\"x\":1220,\"y\":443}],\"text\":\"output\"},{\"boundingBox\":[{\"x\":490,\"y\":473},{\"x\":552,\"y\":473},{\"x\":552,\"y\":492},{\"x\":490,\"y\":492}],\"text\":\"trustor\"},{\"boundingBox\":[{\"x\":802,\"y\":475},{\"x\":869,\"y\":475},{\"x\":869,\"y\":494},{\"x\":802,\"y\":494}],\"text\":\"trustee\"},{\"boundingBox\":[{\"x\":1058,\"y\":467},{\"x\":1244,\"y\":467},{\"x\":1244,\"y\":487},{\"x\":1058,\"y\":487}],\"text\":\"transaction/situation\"}],\"words\":[{\"boundingBox\":[{\"x\":1076,\"y\":11},{\"x\":1168,\"y\":10},{\"x\":1169,\"y\":33},{\"x\":1077,\"y\":32}],\"text\":\"reputation\"},{\"boundingBox\":[{\"x\":1172,\"y\":10},{\"x\":1240,\"y\":9},{\"x\":1241,\"y\":32},{\"x\":1173,\"y\":33}],\"text\":\"system\"},{\"boundingBox\":[{\"x\":613,\"y\":99},{\"x\":746,\"y\":97},{\"x\":746,\"y\":121},{\"x\":613,\"y\":121}],\"text\":\"computation\"},{\"boundingBox\":[{\"x\":182,\"y\":149},{\"x\":283,\"y\":147},{\"x\":284,\"y\":171},{\"x\":183,\"y\":171}],\"text\":\"collection\"},{\"boundingBox\":[{\"x\":288,\"y\":147},{\"x\":308,\"y\":147},{\"x\":308,\"y\":170},{\"x\":288,\"y\":171}],\"text\":\"&\"},{\"boundingBox\":[{\"x\":1066,\"y\":149},{\"x\":1149,\"y\":148},{\"x\":1149,\"y\":173},{\"x\":1066,\"y\":173}],\"text\":\"storage\"},{\"boundingBox\":[{\"x\":1154,\"y\":148},{\"x\":1174,\"y\":148},{\"x\":1174,\"y\":173},{\"x\":1154,\"y\":173}],\"text\":\"&\"},{\"boundingBox\":[{\"x\":181,\"y\":180},{\"x\":305,\"y\":177},{\"x\":305,\"y\":200},{\"x\":183,\"y\":203}],\"text\":\"preparation\"},{\"boundingBox\":[{\"x\":442,\"y\":171},{\"x\":521,\"y\":173},{\"x\":520,\"y\":197},{\"x\":441,\"y\":194}],\"text\":\"filtering\"},{\"boundingBox\":[{\"x\":637,\"y\":174},{\"x\":742,\"y\":173},{\"x\":742,\"y\":199},{\"x\":637,\"y\":196}],\"text\":\"weighting\"},{\"boundingBox\":[{\"x\":818,\"y\":176},{\"x\":945,\"y\":172},{\"x\":945,\"y\":195},{\"x\":819,\"y\":197}],\"text\":\"aggregation\"},{\"boundingBox\":[{\"x\":1037,\"y\":179},{\"x\":1200,\"y\":177},{\"x\":1200,\"y\":200},{\"x\":1038,\"y\":200}],\"text\":\"communication\"},{\"boundingBox\":[{\"x\":6,\"y\":425},{\"x\":53,\"y\":424},{\"x\":54,\"y\":445},{\"x\":6,\"y\":446}],\"text\":\"input\"},{\"boundingBox\":[{\"x\":261,\"y\":418},{\"x\":332,\"y\":418},{\"x\":333,\"y\":439},{\"x\":262,\"y\":438}],\"text\":\"context,\"},{\"boundingBox\":[{\"x\":336,\"y\":418},{\"x\":479,\"y\":417},{\"x\":479,\"y\":438},{\"x\":337,\"y\":439}],\"text\":\"personalisation\"},{\"boundingBox\":[{\"x\":619,\"y\":417},{\"x\":659,\"y\":417},{\"x\":659,\"y\":436},{\"x\":619,\"y\":436}],\"text\":\"trust\"},{\"boundingBox\":[{\"x\":663,\"y\":417},{\"x\":735,\"y\":417},{\"x\":734,\"y\":436},{\"x\":663,\"y\":436}],\"text\":\"relation\"},{\"boundingBox\":[{\"x\":919,\"y\":418},{\"x\":1014,\"y\":417},{\"x\":1014,\"y\":440},{\"x\":920,\"y\":439}],\"text\":\"reputation\"},{\"boundingBox\":[{\"x\":1018,\"y\":417},{\"x\":1094,\"y\":416},{\"x\":1093,\"y\":439},{\"x\":1018,\"y\":440}],\"text\":\"value(s)\"},{\"boundingBox\":[{\"x\":1220,\"y\":425},{\"x\":1284,\"y\":423},{\"x\":1284,\"y\":444},{\"x\":1222,\"y\":443}],\"text\":\"output\"},{\"boundingBox\":[{\"x\":491,\"y\":474},{\"x\":552,\"y\":474},{\"x\":552,\"y\":492},{\"x\":491,\"y\":492}],\"text\":\"trustor\"},{\"boundingBox\":[{\"x\":803,\"y\":476},{\"x\":869,\"y\":477},{\"x\":869,\"y\":494},{\"x\":802,\"y\":495}],\"text\":\"trustee\"},{\"boundingBox\":[{\"x\":1059,\"y\":468},{\"x\":1244,\"y\":467},{\"x\":1244,\"y\":488},{\"x\":1059,\"y\":488}],\"text\":\"transaction/situation\"}]}", "{\"language\":\"en\",\"text\":\"filtering weighting aggregation context comparability simple arithmetic attribute-based criteria compatibility credibility/propagation statistic statistic-based reliability rating value fuzzy time clustering-based graph-based personal preferences\",\"lines\":[{\"boundingBox\":[{\"x\":175,\"y\":16},{\"x\":318,\"y\":17},{\"x\":317,\"y\":62},{\"x\":175,\"y\":60}],\"text\":\"filtering\"},{\"boundingBox\":[{\"x\":640,\"y\":16},{\"x\":824,\"y\":16},{\"x\":824,\"y\":62},{\"x\":640,\"y\":61}],\"text\":\"weighting\"},{\"boundingBox\":[{\"x\":1072,\"y\":21},{\"x\":1292,\"y\":16},{\"x\":1293,\"y\":60},{\"x\":1073,\"y\":64}],\"text\":\"aggregation\"},{\"boundingBox\":[{\"x\":559,\"y\":130},{\"x\":869,\"y\":130},{\"x\":869,\"y\":167},{\"x\":559,\"y\":167}],\"text\":\"context comparability\"},{\"boundingBox\":[{\"x\":1055,\"y\":181},{\"x\":1307,\"y\":179},{\"x\":1307,\"y\":211},{\"x\":1055,\"y\":213}],\"text\":\"simple arithmetic\"},{\"boundingBox\":[{\"x\":137,\"y\":209},{\"x\":360,\"y\":208},{\"x\":360,\"y\":238},{\"x\":137,\"y\":239}],\"text\":\"attribute-based\"},{\"boundingBox\":[{\"x\":568,\"y\":250},{\"x\":859,\"y\":250},{\"x\":859,\"y\":287},{\"x\":568,\"y\":286}],\"text\":\"criteria compatibility\"},{\"boundingBox\":[{\"x\":554,\"y\":371},{\"x\":876,\"y\":370},{\"x\":876,\"y\":406},{\"x\":554,\"y\":407}],\"text\":\"credibility/propagation\"},{\"boundingBox\":[{\"x\":1127,\"y\":396},{\"x\":1240,\"y\":396},{\"x\":1240,\"y\":425},{\"x\":1128,\"y\":425}],\"text\":\"statistic\"},{\"boundingBox\":[{\"x\":141,\"y\":506},{\"x\":352,\"y\":506},{\"x\":352,\"y\":536},{\"x\":141,\"y\":537}],\"text\":\"statistic-based\"},{\"boundingBox\":[{\"x\":650,\"y\":495},{\"x\":780,\"y\":496},{\"x\":780,\"y\":530},{\"x\":650,\"y\":529}],\"text\":\"reliability\"},{\"boundingBox\":[{\"x\":628,\"y\":624},{\"x\":800,\"y\":623},{\"x\":801,\"y\":656},{\"x\":628,\"y\":656}],\"text\":\"rating value\"},{\"boundingBox\":[{\"x\":1141,\"y\":606},{\"x\":1220,\"y\":610},{\"x\":1219,\"y\":638},{\"x\":1140,\"y\":635}],\"text\":\"fuzzy\"},{\"boundingBox\":[{\"x\":681,\"y\":750},{\"x\":749,\"y\":751},{\"x\":748,\"y\":779},{\"x\":680,\"y\":779}],\"text\":\"time\"},{\"boundingBox\":[{\"x\":123,\"y\":799},{\"x\":370,\"y\":798},{\"x\":370,\"y\":831},{\"x\":123,\"y\":832}],\"text\":\"clustering-based\"},{\"boundingBox\":[{\"x\":1085,\"y\":826},{\"x\":1276,\"y\":820},{\"x\":1277,\"y\":851},{\"x\":1086,\"y\":857}],\"text\":\"graph-based\"},{\"boundingBox\":[{\"x\":557,\"y\":873},{\"x\":872,\"y\":872},{\"x\":872,\"y\":903},{\"x\":557,\"y\":905}],\"text\":\"personal preferences\"}],\"words\":[{\"boundingBox\":[{\"x\":175,\"y\":17},{\"x\":316,\"y\":18},{\"x\":316,\"y\":63},{\"x\":176,\"y\":61}],\"text\":\"filtering\"},{\"boundingBox\":[{\"x\":641,\"y\":17},{\"x\":823,\"y\":17},{\"x\":823,\"y\":63},{\"x\":641,\"y\":62}],\"text\":\"weighting\"},{\"boundingBox\":[{\"x\":1073,\"y\":22},{\"x\":1292,\"y\":17},{\"x\":1294,\"y\":61},{\"x\":1073,\"y\":65}],\"text\":\"aggregation\"},{\"boundingBox\":[{\"x\":560,\"y\":133},{\"x\":665,\"y\":132},{\"x\":665,\"y\":165},{\"x\":561,\"y\":164}],\"text\":\"context\"},{\"boundingBox\":[{\"x\":671,\"y\":132},{\"x\":869,\"y\":130},{\"x\":868,\"y\":168},{\"x\":671,\"y\":165}],\"text\":\"comparability\"},{\"boundingBox\":[{\"x\":1055,\"y\":181},{\"x\":1150,\"y\":181},{\"x\":1150,\"y\":213},{\"x\":1056,\"y\":214}],\"text\":\"simple\"},{\"boundingBox\":[{\"x\":1157,\"y\":181},{\"x\":1306,\"y\":180},{\"x\":1305,\"y\":212},{\"x\":1156,\"y\":213}],\"text\":\"arithmetic\"},{\"boundingBox\":[{\"x\":137,\"y\":210},{\"x\":359,\"y\":208},{\"x\":359,\"y\":239},{\"x\":138,\"y\":240}],\"text\":\"attribute-based\"},{\"boundingBox\":[{\"x\":569,\"y\":253},{\"x\":666,\"y\":252},{\"x\":667,\"y\":286},{\"x\":569,\"y\":284}],\"text\":\"criteria\"},{\"boundingBox\":[{\"x\":672,\"y\":252},{\"x\":859,\"y\":250},{\"x\":859,\"y\":288},{\"x\":673,\"y\":286}],\"text\":\"compatibility\"},{\"boundingBox\":[{\"x\":555,\"y\":372},{\"x\":875,\"y\":370},{\"x\":875,\"y\":406},{\"x\":554,\"y\":405}],\"text\":\"credibility/propagation\"},{\"boundingBox\":[{\"x\":1128,\"y\":397},{\"x\":1240,\"y\":396},{\"x\":1240,\"y\":425},{\"x\":1129,\"y\":426}],\"text\":\"statistic\"},{\"boundingBox\":[{\"x\":142,\"y\":507},{\"x\":351,\"y\":507},{\"x\":350,\"y\":536},{\"x\":142,\"y\":537}],\"text\":\"statistic-based\"},{\"boundingBox\":[{\"x\":651,\"y\":497},{\"x\":781,\"y\":497},{\"x\":779,\"y\":531},{\"x\":651,\"y\":528}],\"text\":\"reliability\"},{\"boundingBox\":[{\"x\":628,\"y\":625},{\"x\":713,\"y\":624},{\"x\":713,\"y\":657},{\"x\":628,\"y\":656}],\"text\":\"rating\"},{\"boundingBox\":[{\"x\":719,\"y\":624},{\"x\":801,\"y\":624},{\"x\":801,\"y\":656},{\"x\":719,\"y\":657}],\"text\":\"value\"},{\"boundingBox\":[{\"x\":1141,\"y\":606},{\"x\":1218,\"y\":609},{\"x\":1217,\"y\":638},{\"x\":1140,\"y\":634}],\"text\":\"fuzzy\"},{\"boundingBox\":[{\"x\":680,\"y\":750},{\"x\":748,\"y\":750},{\"x\":747,\"y\":779},{\"x\":680,\"y\":778}],\"text\":\"time\"},{\"boundingBox\":[{\"x\":124,\"y\":799},{\"x\":371,\"y\":799},{\"x\":370,\"y\":831},{\"x\":125,\"y\":832}],\"text\":\"clustering-based\"},{\"boundingBox\":[{\"x\":1086,\"y\":826},{\"x\":1277,\"y\":821},{\"x\":1276,\"y\":852},{\"x\":1087,\"y\":858}],\"text\":\"graph-based\"},{\"boundingBox\":[{\"x\":558,\"y\":875},{\"x\":684,\"y\":873},{\"x\":684,\"y\":905},{\"x\":559,\"y\":905}],\"text\":\"personal\"},{\"boundingBox\":[{\"x\":690,\"y\":873},{\"x\":872,\"y\":873},{\"x\":871,\"y\":903},{\"x\":690,\"y\":905}],\"text\":\"preferences\"}]}", "{\"language\":\"en\",\"text\":\"context comparability simple arithmetic attribute-based criteria compatibility credibility/propagation statistic statistic-based reliability rating value fuzzy time clustering-based service repository (implementation level)- graph-based personal preferences knowledge repository (conceptual level) filtering weighting aggregation\",\"lines\":[{\"boundingBox\":[{\"x\":805,\"y\":101},{\"x\":1022,\"y\":100},{\"x\":1022,\"y\":126},{\"x\":805,\"y\":126}],\"text\":\"context comparability\"},{\"boundingBox\":[{\"x\":1097,\"y\":142},{\"x\":1272,\"y\":141},{\"x\":1272,\"y\":163},{\"x\":1097,\"y\":164}],\"text\":\"simple arithmetic\"},{\"boundingBox\":[{\"x\":565,\"y\":160},{\"x\":719,\"y\":160},{\"x\":719,\"y\":182},{\"x\":565,\"y\":182}],\"text\":\"attribute-based\"},{\"boundingBox\":[{\"x\":812,\"y\":192},{\"x\":1014,\"y\":192},{\"x\":1014,\"y\":220},{\"x\":812,\"y\":220}],\"text\":\"criteria compatibility\"},{\"boundingBox\":[{\"x\":801,\"y\":287},{\"x\":1026,\"y\":287},{\"x\":1026,\"y\":313},{\"x\":801,\"y\":312}],\"text\":\"credibility/propagation\"},{\"boundingBox\":[{\"x\":1145,\"y\":308},{\"x\":1223,\"y\":308},{\"x\":1223,\"y\":329},{\"x\":1145,\"y\":330}],\"text\":\"statistic\"},{\"boundingBox\":[{\"x\":569,\"y\":379},{\"x\":715,\"y\":379},{\"x\":715,\"y\":401},{\"x\":570,\"y\":401}],\"text\":\"statistic-based\"},{\"boundingBox\":[{\"x\":867,\"y\":378},{\"x\":959,\"y\":378},{\"x\":959,\"y\":403},{\"x\":867,\"y\":402}],\"text\":\"reliability\"},{\"boundingBox\":[{\"x\":853,\"y\":473},{\"x\":973,\"y\":473},{\"x\":973,\"y\":497},{\"x\":854,\"y\":497}],\"text\":\"rating value\"},{\"boundingBox\":[{\"x\":1156,\"y\":470},{\"x\":1212,\"y\":474},{\"x\":1211,\"y\":495},{\"x\":1155,\"y\":492}],\"text\":\"fuzzy\"},{\"boundingBox\":[{\"x\":890,\"y\":566},{\"x\":936,\"y\":567},{\"x\":935,\"y\":587},{\"x\":890,\"y\":586}],\"text\":\"time\"},{\"boundingBox\":[{\"x\":558,\"y\":603},{\"x\":728,\"y\":602},{\"x\":728,\"y\":626},{\"x\":558,\"y\":626}],\"text\":\"clustering-based\"},{\"boundingBox\":[{\"x\":0,\"y\":646},{\"x\":409,\"y\":645},{\"x\":409,\"y\":670},{\"x\":0,\"y\":672}],\"text\":\"service repository (implementation level)-\"},{\"boundingBox\":[{\"x\":1119,\"y\":630},{\"x\":1249,\"y\":627},{\"x\":1250,\"y\":650},{\"x\":1119,\"y\":654}],\"text\":\"graph-based\"},{\"boundingBox\":[{\"x\":806,\"y\":663},{\"x\":1021,\"y\":661},{\"x\":1021,\"y\":685},{\"x\":806,\"y\":687}],\"text\":\"personal preferences\"},{\"boundingBox\":[{\"x\":40,\"y\":689},{\"x\":438,\"y\":688},{\"x\":438,\"y\":715},{\"x\":40,\"y\":716}],\"text\":\"knowledge repository (conceptual level)\"},{\"boundingBox\":[{\"x\":611,\"y\":765},{\"x\":689,\"y\":766},{\"x\":689,\"y\":788},{\"x\":611,\"y\":787}],\"text\":\"filtering\"},{\"boundingBox\":[{\"x\":863,\"y\":767},{\"x\":961,\"y\":767},{\"x\":961,\"y\":787},{\"x\":863,\"y\":788}],\"text\":\"weighting\"},{\"boundingBox\":[{\"x\":1114,\"y\":768},{\"x\":1238,\"y\":767},{\"x\":1238,\"y\":786},{\"x\":1114,\"y\":788}],\"text\":\"aggregation\"}],\"words\":[{\"boundingBox\":[{\"x\":806,\"y\":103},{\"x\":879,\"y\":102},{\"x\":879,\"y\":126},{\"x\":807,\"y\":125}],\"text\":\"context\"},{\"boundingBox\":[{\"x\":883,\"y\":102},{\"x\":1022,\"y\":101},{\"x\":1020,\"y\":127},{\"x\":883,\"y\":126}],\"text\":\"comparability\"},{\"boundingBox\":[{\"x\":1098,\"y\":142},{\"x\":1162,\"y\":142},{\"x\":1163,\"y\":165},{\"x\":1098,\"y\":165}],\"text\":\"simple\"},{\"boundingBox\":[{\"x\":1167,\"y\":142},{\"x\":1271,\"y\":141},{\"x\":1272,\"y\":164},{\"x\":1167,\"y\":165}],\"text\":\"arithmetic\"},{\"boundingBox\":[{\"x\":566,\"y\":161},{\"x\":720,\"y\":161},{\"x\":719,\"y\":183},{\"x\":566,\"y\":183}],\"text\":\"attribute-based\"},{\"boundingBox\":[{\"x\":813,\"y\":194},{\"x\":879,\"y\":194},{\"x\":880,\"y\":219},{\"x\":814,\"y\":218}],\"text\":\"criteria\"},{\"boundingBox\":[{\"x\":884,\"y\":194},{\"x\":1015,\"y\":192},{\"x\":1014,\"y\":221},{\"x\":884,\"y\":219}],\"text\":\"compatibility\"},{\"boundingBox\":[{\"x\":802,\"y\":288},{\"x\":1026,\"y\":288},{\"x\":1026,\"y\":313},{\"x\":803,\"y\":311}],\"text\":\"credibility/propagation\"},{\"boundingBox\":[{\"x\":1145,\"y\":308},{\"x\":1222,\"y\":308},{\"x\":1222,\"y\":330},{\"x\":1145,\"y\":331}],\"text\":\"statistic\"},{\"boundingBox\":[{\"x\":570,\"y\":380},{\"x\":715,\"y\":380},{\"x\":714,\"y\":401},{\"x\":570,\"y\":401}],\"text\":\"statistic-based\"},{\"boundingBox\":[{\"x\":867,\"y\":379},{\"x\":960,\"y\":379},{\"x\":959,\"y\":403},{\"x\":867,\"y\":403}],\"text\":\"reliability\"},{\"boundingBox\":[{\"x\":854,\"y\":475},{\"x\":911,\"y\":473},{\"x\":911,\"y\":498},{\"x\":854,\"y\":497}],\"text\":\"rating\"},{\"boundingBox\":[{\"x\":915,\"y\":473},{\"x\":973,\"y\":473},{\"x\":973,\"y\":498},{\"x\":915,\"y\":498}],\"text\":\"value\"},{\"boundingBox\":[{\"x\":1156,\"y\":470},{\"x\":1212,\"y\":473},{\"x\":1210,\"y\":495},{\"x\":1155,\"y\":491}],\"text\":\"fuzzy\"},{\"boundingBox\":[{\"x\":890,\"y\":566},{\"x\":935,\"y\":567},{\"x\":935,\"y\":587},{\"x\":890,\"y\":586}],\"text\":\"time\"},{\"boundingBox\":[{\"x\":559,\"y\":603},{\"x\":729,\"y\":603},{\"x\":728,\"y\":626},{\"x\":560,\"y\":626}],\"text\":\"clustering-based\"},{\"boundingBox\":[{\"x\":0,\"y\":649},{\"x\":69,\"y\":648},{\"x\":69,\"y\":671},{\"x\":0,\"y\":670}],\"text\":\"service\"},{\"boundingBox\":[{\"x\":73,\"y\":647},{\"x\":176,\"y\":646},{\"x\":176,\"y\":672},{\"x\":73,\"y\":671}],\"text\":\"repository\"},{\"boundingBox\":[{\"x\":180,\"y\":646},{\"x\":341,\"y\":645},{\"x\":342,\"y\":671},{\"x\":180,\"y\":672}],\"text\":\"(implementation\"},{\"boundingBox\":[{\"x\":345,\"y\":645},{\"x\":409,\"y\":646},{\"x\":409,\"y\":670},{\"x\":346,\"y\":671}],\"text\":\"level)-\"},{\"boundingBox\":[{\"x\":1119,\"y\":631},{\"x\":1249,\"y\":628},{\"x\":1249,\"y\":651},{\"x\":1121,\"y\":654}],\"text\":\"graph-based\"},{\"boundingBox\":[{\"x\":806,\"y\":665},{\"x\":893,\"y\":662},{\"x\":893,\"y\":687},{\"x\":807,\"y\":687}],\"text\":\"personal\"},{\"boundingBox\":[{\"x\":897,\"y\":662},{\"x\":1021,\"y\":663},{\"x\":1019,\"y\":686},{\"x\":897,\"y\":687}],\"text\":\"preferences\"},{\"boundingBox\":[{\"x\":41,\"y\":690},{\"x\":150,\"y\":690},{\"x\":149,\"y\":716},{\"x\":41,\"y\":714}],\"text\":\"knowledge\"},{\"boundingBox\":[{\"x\":154,\"y\":690},{\"x\":255,\"y\":689},{\"x\":255,\"y\":716},{\"x\":154,\"y\":716}],\"text\":\"repository\"},{\"boundingBox\":[{\"x\":260,\"y\":689},{\"x\":377,\"y\":689},{\"x\":377,\"y\":716},{\"x\":259,\"y\":716}],\"text\":\"(conceptual\"},{\"boundingBox\":[{\"x\":382,\"y\":689},{\"x\":438,\"y\":689},{\"x\":438,\"y\":714},{\"x\":382,\"y\":715}],\"text\":\"level)\"},{\"boundingBox\":[{\"x\":611,\"y\":766},{\"x\":688,\"y\":767},{\"x\":689,\"y\":788},{\"x\":611,\"y\":788}],\"text\":\"filtering\"},{\"boundingBox\":[{\"x\":864,\"y\":768},{\"x\":962,\"y\":768},{\"x\":962,\"y\":787},{\"x\":864,\"y\":788}],\"text\":\"weighting\"},{\"boundingBox\":[{\"x\":1115,\"y\":769},{\"x\":1237,\"y\":767},{\"x\":1237,\"y\":787},{\"x\":1115,\"y\":788}],\"text\":\"aggregation\"}]}", "{\"language\":\"en\",\"text\":\"Client Server filtering AgeBasedAbsolute Clustering implement/ inherit weighting CongruenceAbsolute Webservice Webservice abstract CallHelper CallHandler Component TimeDiscounting Relative aggregation CongruenceAbsolute TimeDiscounting Relative\",\"lines\":[{\"boundingBox\":[{\"x\":204,\"y\":10},{\"x\":274,\"y\":11},{\"x\":273,\"y\":36},{\"x\":204,\"y\":34}],\"text\":\"Client\"},{\"boundingBox\":[{\"x\":289,\"y\":11},{\"x\":369,\"y\":12},{\"x\":369,\"y\":35},{\"x\":289,\"y\":34}],\"text\":\"Server\"},{\"boundingBox\":[{\"x\":693,\"y\":78},{\"x\":762,\"y\":81},{\"x\":762,\"y\":102},{\"x\":692,\"y\":99}],\"text\":\"filtering\"},{\"boundingBox\":[{\"x\":822,\"y\":116},{\"x\":1002,\"y\":114},{\"x\":1002,\"y\":136},{\"x\":822,\"y\":138}],\"text\":\"AgeBasedAbsolute\"},{\"boundingBox\":[{\"x\":865,\"y\":216},{\"x\":960,\"y\":218},{\"x\":959,\"y\":240},{\"x\":865,\"y\":238}],\"text\":\"Clustering\"},{\"boundingBox\":[{\"x\":1114,\"y\":220},{\"x\":1221,\"y\":218},{\"x\":1222,\"y\":240},{\"x\":1114,\"y\":242}],\"text\":\"implement/\"},{\"boundingBox\":[{\"x\":1137,\"y\":244},{\"x\":1197,\"y\":244},{\"x\":1197,\"y\":265},{\"x\":1137,\"y\":265}],\"text\":\"inherit\"},{\"boundingBox\":[{\"x\":691,\"y\":315},{\"x\":783,\"y\":316},{\"x\":782,\"y\":338},{\"x\":691,\"y\":337}],\"text\":\"weighting\"},{\"boundingBox\":[{\"x\":813,\"y\":351},{\"x\":1011,\"y\":351},{\"x\":1011,\"y\":374},{\"x\":813,\"y\":373}],\"text\":\"CongruenceAbsolute\"},{\"boundingBox\":[{\"x\":42,\"y\":380},{\"x\":182,\"y\":381},{\"x\":182,\"y\":406},{\"x\":42,\"y\":404}],\"text\":\"Webservice\"},{\"boundingBox\":[{\"x\":388,\"y\":380},{\"x\":525,\"y\":381},{\"x\":524,\"y\":406},{\"x\":388,\"y\":405}],\"text\":\"Webservice\"},{\"boundingBox\":[{\"x\":1251,\"y\":385},{\"x\":1346,\"y\":385},{\"x\":1346,\"y\":408},{\"x\":1251,\"y\":408}],\"text\":\"abstract\"},{\"boundingBox\":[{\"x\":50,\"y\":411},{\"x\":173,\"y\":413},{\"x\":172,\"y\":440},{\"x\":49,\"y\":437}],\"text\":\"CallHelper\"},{\"boundingBox\":[{\"x\":388,\"y\":411},{\"x\":526,\"y\":411},{\"x\":526,\"y\":436},{\"x\":388,\"y\":436}],\"text\":\"CallHandler\"},{\"boundingBox\":[{\"x\":1230,\"y\":416},{\"x\":1367,\"y\":416},{\"x\":1367,\"y\":442},{\"x\":1230,\"y\":442}],\"text\":\"Component\"},{\"boundingBox\":[{\"x\":833,\"y\":441},{\"x\":990,\"y\":441},{\"x\":990,\"y\":464},{\"x\":833,\"y\":463}],\"text\":\"TimeDiscounting\"},{\"boundingBox\":[{\"x\":874,\"y\":467},{\"x\":951,\"y\":466},{\"x\":951,\"y\":485},{\"x\":874,\"y\":486}],\"text\":\"Relative\"},{\"boundingBox\":[{\"x\":690,\"y\":554},{\"x\":802,\"y\":551},{\"x\":803,\"y\":572},{\"x\":690,\"y\":575}],\"text\":\"aggregation\"},{\"boundingBox\":[{\"x\":814,\"y\":588},{\"x\":1010,\"y\":587},{\"x\":1010,\"y\":611},{\"x\":814,\"y\":611}],\"text\":\"CongruenceAbsolute\"},{\"boundingBox\":[{\"x\":834,\"y\":676},{\"x\":992,\"y\":677},{\"x\":991,\"y\":700},{\"x\":834,\"y\":698}],\"text\":\"TimeDiscounting\"},{\"boundingBox\":[{\"x\":874,\"y\":702},{\"x\":951,\"y\":702},{\"x\":951,\"y\":721},{\"x\":874,\"y\":722}],\"text\":\"Relative\"}],\"words\":[{\"boundingBox\":[{\"x\":204,\"y\":10},{\"x\":274,\"y\":11},{\"x\":273,\"y\":36},{\"x\":204,\"y\":34}],\"text\":\"Client\"},{\"boundingBox\":[{\"x\":290,\"y\":11},{\"x\":370,\"y\":12},{\"x\":369,\"y\":36},{\"x\":290,\"y\":35}],\"text\":\"Server\"},{\"boundingBox\":[{\"x\":694,\"y\":79},{\"x\":762,\"y\":82},{\"x\":761,\"y\":102},{\"x\":693,\"y\":100}],\"text\":\"filtering\"},{\"boundingBox\":[{\"x\":822,\"y\":116},{\"x\":1002,\"y\":115},{\"x\":1001,\"y\":137},{\"x\":822,\"y\":139}],\"text\":\"AgeBasedAbsolute\"},{\"boundingBox\":[{\"x\":866,\"y\":217},{\"x\":960,\"y\":219},{\"x\":958,\"y\":240},{\"x\":866,\"y\":238}],\"text\":\"Clustering\"},{\"boundingBox\":[{\"x\":1114,\"y\":220},{\"x\":1221,\"y\":218},{\"x\":1220,\"y\":240},{\"x\":1115,\"y\":242}],\"text\":\"implement/\"},{\"boundingBox\":[{\"x\":1138,\"y\":245},{\"x\":1198,\"y\":245},{\"x\":1198,\"y\":265},{\"x\":1137,\"y\":265}],\"text\":\"inherit\"},{\"boundingBox\":[{\"x\":692,\"y\":316},{\"x\":782,\"y\":317},{\"x\":781,\"y\":339},{\"x\":692,\"y\":337}],\"text\":\"weighting\"},{\"boundingBox\":[{\"x\":814,\"y\":352},{\"x\":1011,\"y\":352},{\"x\":1011,\"y\":374},{\"x\":816,\"y\":374}],\"text\":\"CongruenceAbsolute\"},{\"boundingBox\":[{\"x\":43,\"y\":380},{\"x\":182,\"y\":382},{\"x\":181,\"y\":407},{\"x\":43,\"y\":405}],\"text\":\"Webservice\"},{\"boundingBox\":[{\"x\":389,\"y\":380},{\"x\":524,\"y\":381},{\"x\":524,\"y\":405},{\"x\":388,\"y\":404}],\"text\":\"Webservice\"},{\"boundingBox\":[{\"x\":1251,\"y\":386},{\"x\":1345,\"y\":386},{\"x\":1344,\"y\":409},{\"x\":1252,\"y\":409}],\"text\":\"abstract\"},{\"boundingBox\":[{\"x\":50,\"y\":412},{\"x\":173,\"y\":414},{\"x\":173,\"y\":440},{\"x\":50,\"y\":438}],\"text\":\"CallHelper\"},{\"boundingBox\":[{\"x\":389,\"y\":411},{\"x\":527,\"y\":412},{\"x\":526,\"y\":437},{\"x\":388,\"y\":436}],\"text\":\"CallHandler\"},{\"boundingBox\":[{\"x\":1231,\"y\":416},{\"x\":1367,\"y\":417},{\"x\":1367,\"y\":442},{\"x\":1231,\"y\":442}],\"text\":\"Component\"},{\"boundingBox\":[{\"x\":834,\"y\":441},{\"x\":990,\"y\":441},{\"x\":989,\"y\":465},{\"x\":835,\"y\":462}],\"text\":\"TimeDiscounting\"},{\"boundingBox\":[{\"x\":875,\"y\":467},{\"x\":951,\"y\":466},{\"x\":950,\"y\":486},{\"x\":874,\"y\":486}],\"text\":\"Relative\"},{\"boundingBox\":[{\"x\":691,\"y\":554},{\"x\":802,\"y\":552},{\"x\":802,\"y\":573},{\"x\":691,\"y\":575}],\"text\":\"aggregation\"},{\"boundingBox\":[{\"x\":815,\"y\":589},{\"x\":1011,\"y\":588},{\"x\":1010,\"y\":611},{\"x\":815,\"y\":611}],\"text\":\"CongruenceAbsolute\"},{\"boundingBox\":[{\"x\":834,\"y\":676},{\"x\":991,\"y\":678},{\"x\":991,\"y\":701},{\"x\":834,\"y\":698}],\"text\":\"TimeDiscounting\"},{\"boundingBox\":[{\"x\":874,\"y\":702},{\"x\":951,\"y\":702},{\"x\":951,\"y\":722},{\"x\":875,\"y\":722}],\"text\":\"Relative\"}]}", "{\"language\":\"en\",\"text\":\"Filter: Weighting: referral reduced set age-based context similarity weighted referral Aggregation: referral reputation filter (absolut) (absolute set average value congruence) set\",\"lines\":[{\"boundingBox\":[{\"x\":245,\"y\":20},{\"x\":303,\"y\":20},{\"x\":302,\"y\":41},{\"x\":244,\"y\":39}],\"text\":\"Filter:\"},{\"boundingBox\":[{\"x\":669,\"y\":4},{\"x\":783,\"y\":6},{\"x\":783,\"y\":31},{\"x\":669,\"y\":29}],\"text\":\"Weighting:\"},{\"boundingBox\":[{\"x\":26,\"y\":35},{\"x\":103,\"y\":34},{\"x\":103,\"y\":55},{\"x\":26,\"y\":56}],\"text\":\"referral\"},{\"boundingBox\":[{\"x\":450,\"y\":26},{\"x\":539,\"y\":25},{\"x\":540,\"y\":46},{\"x\":450,\"y\":47}],\"text\":\"reduced\"},{\"boundingBox\":[{\"x\":47,\"y\":65},{\"x\":83,\"y\":64},{\"x\":84,\"y\":83},{\"x\":48,\"y\":84}],\"text\":\"set\"},{\"boundingBox\":[{\"x\":216,\"y\":51},{\"x\":331,\"y\":48},{\"x\":331,\"y\":70},{\"x\":216,\"y\":72}],\"text\":\"age-based\"},{\"boundingBox\":[{\"x\":638,\"y\":34},{\"x\":815,\"y\":33},{\"x\":815,\"y\":57},{\"x\":638,\"y\":58}],\"text\":\"context similarity\"},{\"boundingBox\":[{\"x\":916,\"y\":26},{\"x\":1011,\"y\":25},{\"x\":1011,\"y\":47},{\"x\":916,\"y\":48}],\"text\":\"weighted\"},{\"boundingBox\":[{\"x\":456,\"y\":55},{\"x\":533,\"y\":53},{\"x\":534,\"y\":73},{\"x\":456,\"y\":75}],\"text\":\"referral\"},{\"boundingBox\":[{\"x\":1110,\"y\":34},{\"x\":1247,\"y\":33},{\"x\":1247,\"y\":57},{\"x\":1110,\"y\":58}],\"text\":\"Aggregation:\"},{\"boundingBox\":[{\"x\":926,\"y\":54},{\"x\":1004,\"y\":52},{\"x\":1004,\"y\":74},{\"x\":926,\"y\":76}],\"text\":\"referral\"},{\"boundingBox\":[{\"x\":1329,\"y\":40},{\"x\":1436,\"y\":38},{\"x\":1437,\"y\":61},{\"x\":1329,\"y\":64}],\"text\":\"reputation\"},{\"boundingBox\":[{\"x\":201,\"y\":76},{\"x\":346,\"y\":76},{\"x\":346,\"y\":100},{\"x\":201,\"y\":99}],\"text\":\"filter (absolut)\"},{\"boundingBox\":[{\"x\":676,\"y\":63},{\"x\":777,\"y\":63},{\"x\":777,\"y\":85},{\"x\":677,\"y\":86}],\"text\":\"(absolute\"},{\"boundingBox\":[{\"x\":479,\"y\":84},{\"x\":512,\"y\":84},{\"x\":513,\"y\":102},{\"x\":479,\"y\":102}],\"text\":\"set\"},{\"boundingBox\":[{\"x\":1135,\"y\":65},{\"x\":1224,\"y\":65},{\"x\":1224,\"y\":87},{\"x\":1135,\"y\":86}],\"text\":\"average\"},{\"boundingBox\":[{\"x\":1353,\"y\":68},{\"x\":1413,\"y\":68},{\"x\":1413,\"y\":88},{\"x\":1353,\"y\":89}],\"text\":\"value\"},{\"boundingBox\":[{\"x\":660,\"y\":94},{\"x\":793,\"y\":93},{\"x\":793,\"y\":114},{\"x\":660,\"y\":115}],\"text\":\"congruence)\"},{\"boundingBox\":[{\"x\":947,\"y\":84},{\"x\":983,\"y\":83},{\"x\":983,\"y\":101},{\"x\":947,\"y\":102}],\"text\":\"set\"}],\"words\":[{\"boundingBox\":[{\"x\":244,\"y\":20},{\"x\":301,\"y\":20},{\"x\":301,\"y\":41},{\"x\":244,\"y\":40}],\"text\":\"Filter:\"},{\"boundingBox\":[{\"x\":669,\"y\":5},{\"x\":784,\"y\":6},{\"x\":783,\"y\":31},{\"x\":670,\"y\":29}],\"text\":\"Weighting:\"},{\"boundingBox\":[{\"x\":27,\"y\":36},{\"x\":103,\"y\":34},{\"x\":103,\"y\":56},{\"x\":28,\"y\":56}],\"text\":\"referral\"},{\"boundingBox\":[{\"x\":451,\"y\":27},{\"x\":540,\"y\":25},{\"x\":540,\"y\":46},{\"x\":451,\"y\":47}],\"text\":\"reduced\"},{\"boundingBox\":[{\"x\":47,\"y\":65},{\"x\":83,\"y\":64},{\"x\":84,\"y\":83},{\"x\":47,\"y\":84}],\"text\":\"set\"},{\"boundingBox\":[{\"x\":216,\"y\":51},{\"x\":331,\"y\":49},{\"x\":331,\"y\":71},{\"x\":217,\"y\":73}],\"text\":\"age-based\"},{\"boundingBox\":[{\"x\":639,\"y\":37},{\"x\":715,\"y\":35},{\"x\":715,\"y\":57},{\"x\":639,\"y\":57}],\"text\":\"context\"},{\"boundingBox\":[{\"x\":719,\"y\":35},{\"x\":815,\"y\":34},{\"x\":814,\"y\":58},{\"x\":719,\"y\":57}],\"text\":\"similarity\"},{\"boundingBox\":[{\"x\":916,\"y\":28},{\"x\":1012,\"y\":26},{\"x\":1012,\"y\":47},{\"x\":917,\"y\":47}],\"text\":\"weighted\"},{\"boundingBox\":[{\"x\":456,\"y\":56},{\"x\":534,\"y\":54},{\"x\":533,\"y\":74},{\"x\":457,\"y\":74}],\"text\":\"referral\"},{\"boundingBox\":[{\"x\":1111,\"y\":35},{\"x\":1247,\"y\":33},{\"x\":1246,\"y\":58},{\"x\":1110,\"y\":58}],\"text\":\"Aggregation:\"},{\"boundingBox\":[{\"x\":927,\"y\":55},{\"x\":1004,\"y\":52},{\"x\":1004,\"y\":75},{\"x\":927,\"y\":75}],\"text\":\"referral\"},{\"boundingBox\":[{\"x\":1329,\"y\":41},{\"x\":1436,\"y\":38},{\"x\":1436,\"y\":62},{\"x\":1329,\"y\":64}],\"text\":\"reputation\"},{\"boundingBox\":[{\"x\":201,\"y\":76},{\"x\":245,\"y\":77},{\"x\":245,\"y\":100},{\"x\":201,\"y\":100}],\"text\":\"filter\"},{\"boundingBox\":[{\"x\":250,\"y\":77},{\"x\":346,\"y\":77},{\"x\":345,\"y\":101},{\"x\":250,\"y\":100}],\"text\":\"(absolut)\"},{\"boundingBox\":[{\"x\":677,\"y\":64},{\"x\":777,\"y\":64},{\"x\":776,\"y\":85},{\"x\":677,\"y\":87}],\"text\":\"(absolute\"},{\"boundingBox\":[{\"x\":479,\"y\":84},{\"x\":513,\"y\":84},{\"x\":513,\"y\":102},{\"x\":479,\"y\":102}],\"text\":\"set\"},{\"boundingBox\":[{\"x\":1136,\"y\":65},{\"x\":1225,\"y\":65},{\"x\":1224,\"y\":88},{\"x\":1136,\"y\":86}],\"text\":\"average\"},{\"boundingBox\":[{\"x\":1354,\"y\":69},{\"x\":1413,\"y\":68},{\"x\":1414,\"y\":88},{\"x\":1354,\"y\":89}],\"text\":\"value\"},{\"boundingBox\":[{\"x\":661,\"y\":95},{\"x\":792,\"y\":93},{\"x\":792,\"y\":115},{\"x\":660,\"y\":115}],\"text\":\"congruence)\"},{\"boundingBox\":[{\"x\":947,\"y\":84},{\"x\":982,\"y\":83},{\"x\":983,\"y\":101},{\"x\":947,\"y\":102}],\"text\":\"set\"}]}", "{\"language\":\"en\",\"text\":\"Published online: 13 May 2015\",\"lines\":[{\"boundingBox\":[{\"x\":0,\"y\":15},{\"x\":897,\"y\":16},{\"x\":896,\"y\":72},{\"x\":0,\"y\":70}],\"text\":\"Published online: 13 May 2015\"}],\"words\":[{\"boundingBox\":[{\"x\":1,\"y\":18},{\"x\":284,\"y\":16},{\"x\":284,\"y\":71},{\"x\":0,\"y\":68}],\"text\":\"Published\"},{\"boundingBox\":[{\"x\":294,\"y\":16},{\"x\":503,\"y\":16},{\"x\":503,\"y\":73},{\"x\":294,\"y\":71}],\"text\":\"online:\"},{\"boundingBox\":[{\"x\":512,\"y\":16},{\"x\":587,\"y\":16},{\"x\":588,\"y\":73},{\"x\":513,\"y\":73}],\"text\":\"13\"},{\"boundingBox\":[{\"x\":597,\"y\":16},{\"x\":737,\"y\":16},{\"x\":738,\"y\":73},{\"x\":598,\"y\":73}],\"text\":\"May\"},{\"boundingBox\":[{\"x\":747,\"y\":16},{\"x\":894,\"y\":17},{\"x\":895,\"y\":73},{\"x\":748,\"y\":73}],\"text\":\"2015\"}]}" ] }, { "@search.score": 1.593202, "content": "\nComputational Visual Media\nhttps://doi.org/10.1007/s41095-020-0189-1 Vol. 7, No. 2, June 2021, 159–167\n\nReview Article\n\nMachine learning for digital try-on: Challenges and progress\n\nJunbang Liang1 (�), Ming C. Lin1\n\nc© The Author(s) 2020.\n\nAbstract Digital try-on systems for e-commerce have\nthe potential to change people’s lives and provide notable\neconomic benefits. However, their development is limited\nby practical constraints, such as accurate sizing of the\nbody and realism of demonstrations. We enumerate three\nopen challenges remaining for a complete and easy-to-use\ntry-on system that recent advances in machine learning\nmake increasingly tractable. For each, we describe\nthe problem, introduce state-of-the-art approaches, and\nprovide future directions.\n\nKeywords machine learning; digital try-on; garment\nmodeling; human body estimation; material\nmodeling\n\n1 Introduction\nE-commerce has grown at a rapid pace in recent\nyears. Consumers today are more likely to shop\nonline than to visit a retail store. The situation is\nmuch more complicated, however, when it comes to\nbuying clothes. People need to know how a garment\nfits on them, how it looks, and how it feels. Digital\ntry-on systems can potentially satisfy these needs,\nproviding a direct visual impression, and possibly\ncustomized clothes sizing as well. Therefore, it has\ndrawn much attention as an attractive alternative to\nimprove the user experience and popularize online\nfashion shopping.\n\nHowever, the technology is still far from practical,\neasy-to-use, and adequate to replace physical try-on.\nCurrently, most try-on systems rely on either image-\nediting, copy-pasting, or template demonstrations,\n\n1 University of Maryland, College Park, MD 20785, USA.\nE-mail: J. Liang, liangjb@cs.umd.edu (�); M. C. Lin,\nlin@cs.umd.edu.\n\nManuscript received: 2020-06-24; accepted: 2020-07-21\n\nwhile the ultimate goal is a fast and realistic try-on\nsystem adaptive to each customer’s body. There is\nstill a substantial technological gap between modeling\nand demonstrating garment fitting in the digital and\nreal worlds, including fast and realistic demonstration,\naccurate modeling of human body and garments,\nfaithful modeling of garment material, and lossless\ntransformation of garments between virtual and\nphysical worlds.\n\nIn this paper, we present some open research issues\nthat contribute to this technological gap, including:\n1. accurate estimation of human shapes and sizes\n\nusing consumer devices,\n2. faithful recovery of garment materials via (online)\n\nimages, and\n3. ease of design and manipulation of sewing patterns\n\nand garment pieces by end-users.\nAlthough traditional methods have made important\n\nprogress on these under-constrained problems,\nlearning-based approaches have shown tremendous\npotential to make a notable impact. Compared to\ntraditional methods, machine-learning algorithms are\nusually much faster since training and optimization\nare performed offline. They are also good at\ngeneralizing to unseen images without the need for\ntedious data pre-processing. While extensive research\nexists on 2D image learning, machine learning of\nhighly variable 3D human body shapes is still far\nfrom mature, which is the reason why the open issues\ndescribed above remain elusive.\n\nFor each problem listed above, we motivate its\nimportance, provide a problem description, and\npresent state-of-the-art approaches with potential\nfor improvements. We believe that solutions to\nthese challenging problems will lead to significant\nadvances in digital try-on, as well as other areas of\ne-commerce.\n\n159\n\n\n\n\n\n\n\n160 J. Liang, M. C. Lin\n\n2 Open problems\nIn this section we first introduce three major\nchallenges that limit digital try-on technology from\nbeing widely adopted and accepted by shoppers.\nThere are several reasons why shoppers still prefer\nphysical try-on. Firstly, consumers are unsure if what\nthey buy online will fit them well. Although general\nsizing systems exist, their lack of consistency and\nstandardization across different brands and garment\nmaterials can often make it difficult to size clothes,\nespecially for persons with non-standard body shapes\nand proportion. Accurate estimation of human body\nshape is the key to successful digital try-on. Secondly,\nfabric is usually a key consideration when shopping\nfor clothes. Different fabric affects how garments\nlook and fit, how consumers would wear them, and\nwhether or not they would buy them. However, the\ncorrespondence between the actual material and its\ndigital representation are not well understood. It is\nalso challenging to acquire a full fabric digital model\nfrom real-world examples.\n\nFor the customers, appearance is as critical as other\nfactors. There are two approaches to displaying\ngarments: 2D image-based, and 3D mesh-based\nwith photo-realistic rendering. They have different\nadvantages and drawbacks, but both need a large\ngarment database for support. While creating a 3D\ngarment takes considerable effort, 2D images often\nsuffer from a lack of variation and are much more\ndifficult to customize. In either case, the try-on\nsystem needs a user-friendly design and manipulation\nbackend. Last, but not least, a fast and realistic\nanimation of the garments in motion along with\nbody movements greatly improves the user experience.\nAlthough it is not so critical as other factors, it\nwould effectively reduce the perceptual gap between\nthe real and digital worlds for online shopping.\nPrevious work has proposed using cloud computing\nto improve the animation speed, but there is still a\nnotable technology gap for high-quality, interactive\n3D animation of clothes.\n\n3 Human shape estimation\nAs noted, accurate human shape estimation is key to\nenabling digital try-on. Human body reconstruction,\nconsisting of pose and shape estimation, has been\nwidely studied in a variety of areas, including digital\n\nsurveillance, computer animation, special effects, and\nvirtual and augmented environments. Yet, it remains\na challenging and popular topic of interest. While\ndirect 3D body scanning can provide excellent and\naccurate results, its adoption is somewhat limited by\nthe required specialized hardware. RGB images are\nwidely available for input to digital try-on and can\nbe easily captured using commodity mobile devices.\nAlthough purely image-based try-on methods have\nbeen proposed [1], learning-based 3D body estimation\nis more widely applicable in that the 3D body can be\narticulated and so re-posed and re-targeted.\n\nWe define the human-body reconstruction problem\ninformally as, given one or more RGB images, to\nestimate the human body geometry and size, and\noutput (preferably) a 3D humanoid mesh. Traditional\nalgorithms often formulate it as an optimization\nproblem, in which the silhouette difference is a major\npart of the objective function [2]. Therefore, these\nmethods either require the human to wear tight\nclothes, or alternatively relax the target function\nto be unilateral on uncovered body parts [3], or\nto point correspondences [4]. The use of machine\nlearning methods in this problem has led to significant\nadvances. Firstly, it has moved the algorithm from\nonline to offline, significantly reducing response time.\nSecond, by using a parametric human model [5],\none can easily construct a regression network for\nthe parameters while the losses needed can also be\ninferred from them. While early works proposed\nnetwork models for only 2D/3D body skeletons [6–\n8], more recent works have introduced techniques to\nperform regression for the entire human body—either\nusing a parametric human model [9, 10] or a voxel-\nbased representation [11–13]. As annotations in most\nreal-world datasets contain only joint positions, the\nlearning process has been refined in various ways [14–\n17]. The current state of the art is the recent work\nby Ref. [18] �. It emphasizes shape learning, while\nmany other works often focus on body-joint losses,\nbut neglect the effect of body shapes.\n\nThe key contribution of Ref. [18] is a multi-view,\nmulti-stage framework to address ambiguity caused by\ncamera projection (see Fig. 1). Their model performs\nseveral stages of error correction. Each of the image\ninputs is passed on step by step; at each step, a shared-\n\n� Liang and Lin’s data and code are available at https://gamma.umd.edu/\nresearchdirections/virtualtryon/humanmultiview\n\n\n\n\n\nMachine learning for digital try-on: Challenges and progress 161\n\nFig. 1 Network structure from Ref. [18]. By using an iterative value correction structure, visual information from different views is effectively\nintegrated to provide a unified human shape. Reproduced with permission from Ref. [18], c© The Author(s) 2019.\n\nparameter prediction block computes the correction\nbased on the image feature and the input guesses.\nThe camera and the human body parameters are\nestimated at the same time, projecting the predicted\n3D joints back to 2D for loss computation. The\nestimated pose and shape parameters are shared\namong all views, while each view maintains its own\ncamera calibration and global orientation. Their\nproposed framework uses a recurrent structure,\nmaking it a universal model applicable to any number\nof views. At the same time, it couples shareable\ninformation across different views so that the human\nbody pose and shape are optimized using image\nfeatures from all views. Unlike static multi-view\nCNNs which have a fixed number of inputs, they\nmake use of the RNN-like structure in a cyclic form to\naccept any number of views, and prevent the gradient\nvanishing by predicting corrective values instead of\nupdating parameters in each regression block.\n\nExperiments have shown that, after training, this\nmodel can form a single view image, provide equally\ngood pose estimation as the state of the art, and\nprovide considerably improved pose estimation when\nusing multi-view inputs, leading to better shape\nestimation across all datasets. An example is\ndemonstrated in Fig. 2. Moreover, a physically-based\nsynthetic data generation pipeline is introduced to\nenrich the training data, which is very helpful for\n\nshape estimation and regularization in cases that\ntraditional datasets do not capture. While synthetic\ndata improves the diversity of human bodies with\nground-truth parameters, a larger garment dataset\nand a more convenient registration process are needed\nto minimize the performance gap between real-world\nimages and synthetic data. In addition, other\nvariables such as hair, skin color, and 3D backgrounds\nare subtle elements that can influence the perceived\nrealism of the synthetic data at the higher expense of\na more complex data generation pipeline. With the\nrecent progress in image style transfer using GAN, a\n\nFig. 2 Prediction results using the state of the art [18]. The model\ncaptures the shape of the human body by learning from synthetic\ndata. The recovered legs and chest are close to those of the person\nin the image. Reproduced with permission from Ref. [18], c© The\nAuthor(s) 2019.\n\n\n\n\n\n\n\n162 J. Liang, M. C. Lin\n\npromising direction is to transfer the synthetic result\nto more realistic images to further improve the result.\n\n4 Garment material modeling\n4.1 Introduction\nGarment material plays an important role in digital\ntry-on systems. Physical recreation of the fabric not\nonly gives a compelling visual simulation of the cloth,\nbut also affects how the garment feels and fits on\nthe body. However, fabric modeling is a challenging\ntask: the appearance and physical properties of\nthe garment are determined not only by the type\nof materials the clothes are made of, but also by\nsewing and weave. Thus, researchers often focus on\nthe physical behaviour, rather than the underlying\nsemantic primitives.\n\nHence, we state the garment material modeling\nproblem as follows. Given a sufficient amount of data,\nmodel the material’s physical behavior and physical\nproperties, so that visual effects the same as or similar\nto those of the real material can be reproduced by a\ncomputer. This has two implications: firstly, we need\nto define a physical model of the material, and then\nwe must estimate the parameters in the model.\n\nThere are many ways to model clothes, including\nspring–mass systems and finite elements. The latter is\nthe most popular model since it can produce realistic\nresults. While one can use isotropic properties such\nas Young’s modulus and Poisson ratio, an anisotropic\nmodel is a better choice since it can support different\nbehaviors caused by the weave of the material.\n\n4.2 Learning-based estimation\nWhile traditional optimization methods [19] often\ntake a long time to compute material parameters,\nmachine-learning methods can make predictions in\nreal time by a simple feed-forward operation, which\nis more useful in applications that need fast feedback,\nsuch as garment prototyping. The state-of-the-art\nmodel from Yang et al. [20] � uses CNNs combined\nwith LSTM to recover material parameters from\nvideos. To constrain both the input and solution\nspace, they choose one of the materials as a basis;\nthe material sub-space is constructed by multiplying\nthis material basis with a positive coefficient. To\nconstruct an optimal material parameter sub-space, a\n\n� Yang et al.’s data and code are available at http://gamma.cs.unc.edu/\nVideoCloth\n\nmaterial parameter sensitivity analysis is conducted\nto examine the sensitivity of the material parameters\nκ with respect to the amount of deformation D(κ).\nPhysically based cloth simulations are used to\ngenerate a much larger number of data samples within\nthese sub-spaces, which would otherwise be difficult\nor time-consuming to capture. The cloth meshes are\ngenerated through physically-based simulation, and\nthen rendered as 2D images with a randomly assigned\ntexture. Using the data samples, they combine the\nimage signal feature extraction method, a CNN, with\nthe temporal sequence learning method, LSTM, to\nlearn the mapping from visual appearance to material.\nAs shown in Fig. 3, the CNN layer is used to extract\nboth low- and high-level visual features, while the\nLSTM layer focuses on learning the mapping between\nthe material properties of the cloth and its consequent\nmovement.\n\nThey demonstrated the proposed framework with\nthe application of “material cloning”. With the\ntrained deep neural network model being able to\ncapture the cloth motion (Fig. 4), the material type\ncan be inferred from a video recording of the motion\nof the cloth in a fairly small amount of time. The\nrecovered material type can be “cloned” onto another\npiece of cloth or garment as shown in Fig. 5.\n\nIn this work, the videos contain only a single piece\nof cloth which does not interact with any other object.\nWhile this is not applicable to all real-world scenarios,\nthis method provides new insights into addressing this\nchallenging problem. A natural extension would be to\nlearn from videos of clothing directly interacting with\nthe human body, under varying lighting conditions\nand partial occlusion.\n\n4.3 Optimization using differentiable physics\nAnother approach to modeling the fabric is to measure\ngeometric differences directly during parameter\n\nFig. 3 Network model from Ref. [20]. The material is modeled\nby learning motion patterns of image features given by CNNs.\nReproduced with permission from Ref. [20], c© The Author(s) 2017.\n\n\n\n\n\nMachine learning for digital try-on: Challenges and progress 163\n\nFig. 4 Learned CNN conv5-layer activation visualization from\nRef. [20]. Experiments show that the trained model is able to capture\nmoving parts of the cloth even in an unseen video. Reproduced with\npermission from Ref. [20], c© The Author(s) 2017.\n\nFig. 5 Yang et al. [20] modeled clothes materials in input videos (left),\nand applied those materials to a simulated skirt (right). Reproduced\nwith permission from Ref. [20], c© The Author(s) 2017.\n\noptimization. Assuming that the environment is\nknown to the system, computation of the estimated\nmotion and its gradient with respect to the material\nparameters can be achieved using differentiable\nsimulation. A typical usage of differentiable\nsimulation is motion control (see Fig. 6), where the\ndifference to the target is measured and the loss\nbackpropagated to the network. Similar processes\ncan be applied to material parameter estimation as\nwell. By measuring the distance to the target as the\nloss and computing corresponding gradients, either in\npixel space or in 3D space, the material parameters\n\ncan be learned or optimized to achieve the desired\ncloth motion or visual effect. Recent differentiable\nphysics work covers rigid bodies [22, 23], cloth [24],\nand particle-grid systems [25, 26]. The state-of-\nthe-art is Ref. [24] �, which proposes a method for\ndifferentiable cloth simulation. It is the first work\nto tackle a high dimensional simulation problem\nand to propose a general differentiable collision\nhandling algorithm. Later, a follow-up work [21]\nextended the algorithm to be applicable to coupled\ndynamics with rigid bodies. Overall, they follow\nthe computational flow of the common approach\nto cloth simulation: discretization using the finite\nelement method, integration using an implicit Euler\nmethod, and collision response on impact zones. They\nuse implicit differentiation in the linear solver and\noptimization in order to compute the gradient with\nrespect to the input parameters. The discontinuity\nintroduced by collision response is negligible because\nthe discontinuous states constitute a zero-measure\nset. During backpropagation in the optimization,\ngradient values can be directly computed after QR\ndecomposition of the constraint matrix. Their\npipeline contains several techniques that can be\nemployed in other differentiable simulations.\n4.3.1 Derivatives of the physical solution\nIn modern simulation algorithms, an implicit Euler\nmethod is often used for stable integration results.\nThus the mass matrix M often includes the Jacobian\nof the forces, and is denoted as M̂ to indicate this\ndifference. A linear solver is needed to compute the\nacceleration since it is time-consuming to compute\nM̂−1. Implicit differentiation is used to compute the\ngradients of the linear solution. Given an equation\nM̂a = f with a solution z and propagated gradient\n∂L/∂a|a=z, where L is the task-specific loss function,\nimplicit differentiation is used to derive the gradients.\nWe refer readers to the original paper [24] for more\ndetails.\n4.3.2 Derivatives of the collision response\nA general approach using LCP to integrate collision\nconstraints into physics simulations has been\nproposed, but constructing a static LCP is often\nimpractical in cloth simulation due to the high\ndimensionality. Collisions and contacts which happen\nat each step are very sparse compared to the complete\n\n� Liang et al.’s data and code are available at https://gamma.umd.edu/\nresearchdirections/virtualtryon/differentiablecloth\n\n\n\n\n\n\n\n164 J. Liang, M. C. Lin\n\nFig. 6 Differentiable simulation embedding example from Ref. [21]. The loss can be backpropagated through the physics simulator to the\nneural network, enabling learning tasks such as material modeling and motion control.\n\ndata. Therefore, a dynamic approach is used that\nincorporates collision detection and response.\n\nCollision handling in their implementation is based\non impact zone optimization. It finds all colliding\ninstances using continuous collision detection and\nsets up the constraints for all collisions. In order\nto introduce minimum change to the original mesh\nstate, a QP problem is developed to determine the\nconstraints. Since the signed distance function is\nlinear in x, the optimization takes a quadratic form,\nas shown originally in Ref. [24]:\n\nminimize\nz\n\n1\n2\n\n(z − x)TW (z − x),\n\nsubject to Gz + h � 0\nwhere W is a constant diagonal weight matrix related\nto the mass of each vertex, and G and h are constraint\nparameters. The numbers of variables and constraints\nare n and m, i.e. x ∈ R\n\nn, h ∈ R\nm, and G ∈ R\n\nm×n.\nNote that this optimization problem has inputs x,\nG, and h, and output z. The goal here is to derive\n∂L/∂x, ∂L/∂G, and ∂L/∂h given ∂L/∂z, where L\nis the loss function.\n\nWhen computing the gradient using implicit\ndifferentiation, the dimensionality of the linear system\ncan be very high. Their key observation here is that\nn >> m > rank(G), since one contact often involves 4\nvertices (thus 12 variables) and some contacts may be\nlinearly dependent (e.g., multiple adjacent collision\n\npairs). They minimize the size of the linear equation\nbased on the QR decomposition of G, which is the key\nto accelerating backpropagation of high dimensional\nQP problems.\n\nOne of their experiments shows its ability to\noptimize material parameters from observation. The\nscene features a piece of cloth hanging under gravity\nand subjected to a constant wind force. The material\nmodel consists of three parts: density d, stretching\nstiffness S, and bending stiffness B. The stretching\nstiffness quantifies the reaction force when the cloth\nis stretched; the bending stiffness models how easily\nthe cloth can be bent and folded. Table 1 shows\nresults. They achieve a much smaller error in most\nmeasurements in comparison to the baselines; the\nlinear part of the stiffness matrix is modeled well.\nWith the computed gradient using their model, one\ncan effectively optimize the unknown parameters that\ndominate cloth movement to fit the observed data.\n\nIn follow-up work, Qiao et al. extended the\ndifferentiable simulation pipeline to couple with\nrigid body dynamics, formulated using generalized\ncoordinates:\n\nd\ndt\n\n⎛\n⎝ q\n\nq̇\n\n⎞\n⎠ =\n\n⎛\n⎝ q̇\n\nq̈\n\n⎞\n⎠ =\n\n⎛\n⎝ q̇\n\nM−1f(q, q̇)\n\n⎞\n⎠\n\nand update the optimization formulation for collision\nresponse accordingly (see Ref. [21] for details):\n\nTable 1 Material parameter estimation results from Ref. [24]. Their proposed method runs faster than L-BFGS. Values of material parameters\nare Frobenius norms of the difference normalized by the Frobenius norm of the target. Values of the simulated result are the average pairwise\nvertex distances normalized by the size of the cloth. The gradient-based method yields much smaller errors than the baselines\n\nMethod\nRuntime\n\n(sec/step/iter)\n\nDensity\n\nerror (%)\n\nLinear stretching\n\nstiffness error (%)\n\nBending stiffness\n\nerror (%)\n\nSimulation\n\nerror (%)\n\nBaseline — 68 ± 46 160 ± 119 70 ± 42 12 ± 3.0\n\nL-BFGS 2.89 ± 0.02 4.2 ± 5.6 72 ± 90 70 ± 43 4.9 ± 3.3\n\nLiang et al. [24] 2.03 ± 0.06 1.8 ± 2.0 45 ± 41 77 ± 36 1.6 ± 1.4\n\n\n\n\n\nMachine learning for digital try-on: Challenges and progress 165\n\nminimize\nq′\n\n1\n2\n\n(q − q′)TM̂(q − q′)\n\nsubject to Gf(q′) + h � 0\n\nDue to the inclusion of rigid bodies, the constraints\nused in the optimization are no longer linear. When\ncomputing gradients, they linearize the constraints\naround a neighborhood as an approximation to enable\nQR decomposition for acceleration as previously\nmentioned.\n\n5 Garment modeling and design\nRealistic apparel model generation has become\nincreasingly popular, due to the rapid changes in\nfashion trends and the growing need for garment\nmodels in different applications such as virtual try-\non. It is already used even for state-of-the-art\ninteractive apparel design systems [27]. Application\nrequirements mean that it is important to have a\ngeneral cloth model that can represent a diverse set\nof garments. However, there are many challenges\nin automatic garment model generation. Firstly,\ngarments usually have different types of topology,\nespecially for fashion apparel, that makes it difficult\nto design a universal pipeline. Moreover, it is often\nnot straightforward for general garments design to\nbe retargeted onto another body shape, making\ncustomization difficult.\n\nPrevious work has addressed this problem to some\nextent. Huang et al. [28] proposed a realistic 3D\ngarment generation algorithm based on front and\nback image sketches, but it cannot readily retarget\ngenerated garments to other body shapes. Wang et\nal. [29] proposed an algorithm which can conveniently\nperform retargeting, but permits limited topology\nlike T-shirts or skirts. There is no recent work that\naddresses these two problems at the same time.\n\nWe introduce a learning-based parametric\ngenerative model to overcome the above difficulties,\ngiven garment sewing patterns and human body\nshapes as input. One possible approach would be to\ncompute a displacement image on the U–V space\nof the human body as a unified representation of\nthe garment mesh. Different topology and sizes\nof the garment are represented by different values\nin the image. The 2D displacement image, as the\nrepresentation of the 3D garment mesh data, can\n\nthen be fed into a conditional generative adversarial\nnetwork (cGAN) for latent space learning. The 2D\nrepresentation for the garment mesh can transfer\nthe irregular 3D mesh data to regular image data\nwhere a traditional CNN can easily learn. It can also\nextract relative geometric information with respect\nto the human body, enabling garment retargeting to\na different person.\n\n6 Conclusions\nAlthough virtual reality and digital try-on have\nexcellent potential and are rapidly developing, there\nremain open problems before online try-on systems\ncan be widely adopted. We have listed three major\nchallenges, all of which can be addressed or further\nimproved using machine learning algorithms. For\ngarment material prediction, state-of-the-art methods\nare still limited in that the training data is highly\nconstrained: the scenario contains only a piece\nof cloth floating in the wind. To improve its\napplicability to daily tasks, it is necessary to focus\non solving the problem on a more diverse set of\ninputs. Predicting the material from a garment\non a fixed human body could be a good start,\nbefore generalizing to arbitrary human motions and\npredicting multiple garments on the same body. In\nthe area of human shape estimation, it would be\ninteresting to learn how external constraints could\nimprove estimation accuracy. For example, the shape\nand size of the garment are hard constraints to\nwhich the predicted body should conform. While\noptimization-based methods can integrate these\nconstraints fairly easily, doing so remains elusive\nfor learning-based approaches. One possibility is\nto jointly estimate body and garment together and\nintroduce an intersection loss. This approach would\nrequire a new solution to the open problem of unified\ndeep garment representation, if we do not want to\ntrain one model for every garment type, which could\nbe even more challenging. We believe that substantial\nbreakthroughs in digital try-on are achievable with\nmore investigation in these directions.\n\nAcknowledgements\nThis research was supported in part by the Iribe\nProfessorship and the National Science Foundation.\n\n\n\n166 J. Liang, M. C. Lin\n\nReferences\n\n[1] Zheng, Z. H.; Zhang, H. T.; Zhang, F. L.; Mu, T. J.\nImage-based clothes changing system. Computational\nVisual Media Vol. 3, No. 4, 337–347, 2017.\n\n[2] Dibra, E.; Jain, H.; Öztireli, C.; Ziegler, R.; Gross,\nM. HS-Nets: Estimating human body shape from\nsilhouettes with convolutional neural networks. In:\nProceedings of the 4th International Conference on\n3D Vision, 108–117, 2016.\n\n[3] Bălan, A. O.; Black, M. J. The naked truth: Estimating\nbody shape under clothing. In: Computer Vision –\nECCV 2008. Lecture Notes in Computer Science, Vol.\n5303. Forsyth, D.; Torr, P.; Zisserman, A. Eds. Springer\nBerlin, 15–29, 2008.\n\n[4] Lassner, C.; Romero, J.; Kiefel, M.; Bogo, F.; Black,\nM. J.; Gehler, P. V. Unite the people: Closing the\nloop between 3D and 2D human representations. In:\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 4704–4713, 2017.\n\n[5] Loper, M.; Mahmood, N.; Romero, J.; Pons-Moll, G.;\nBlack, M. J. SMPL: A skinned multi-person linear\nmodel. ACM Transactions on Graphics Vol. 34, No. 6,\nArticle No. 248, 2015.\n\n[6] Wei, S.-E.; Ramakrishna, V.; Kanade, T.; Sheikh, Y.\nConvolutional pose machines. In: Proceedings of the\nIEEE conference on Computer Vision and Pattern\nRecognition, 4724–4732, 2016.\n\n[7] Cao, Z.; Simon, T.; Wei, S.; Sheikh, Y. Realtime multi-\nperson 2D pose estimation using part affinity fields.\nIn: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 1302–1310, 2017.\n\n[8] Mehta, D.; Sridhar, S.; Sotnychenko, O.; Rhodin, H.;\nShafiei, M.; Seidel, H.-P.; Xu, W.; Casas, D.; Theobalt,\nC. VNect: Realtime 3D human pose estimation with\na single RGB camera. ACM Transactions on Graphics\nVol. 36, No. 4, Article No. 44, 2017.\n\n[9] Alldieck, T.; Magnor, M.; Xu, W.; Theobalt, C.; Pons-\nMoll, G. Video based reconstruction of 3D people\nmodels. In: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 8387–8397,\n2018.\n\n[10] Kanazawa, A.; Black, M. J.; Jacobs, D. W.; Malik, J.\nEnd-to-end recovery of human shape and pose. In:\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 7122–7131, 2018.\n\n[11] Varol, G.; Ceylan, D.; Russell, B.; Yang, J.; Yumer,\nE.; Laptev, I. Bodynet: Volumetric inference of 3D\nhuman body shapes. In: Proceedings of the European\nConference on Computer Vision, 20–36, 2018.\n\n[12] Zheng, Z.; Yu, T.; Wei, Y.; Dai, Q.; Liu, Y. Deephuman:\n3D human reconstruction from a single image. In:\n\nProceedings of the IEEE International Conference on\nComputer Vision, 7739–7749, 2019.\n\n[13] Saito, S.; Huang, Z.; Natsume, R.; Morishima, S.;\nKanazawa, A.; Li, H. PIFu: Pixel-aligned implicit\nfunction for high-resolution clothed human digitization.\nIn: Proceedings of the IEEE International Conference\non Computer Vision, 2304–2314, 2019.\n\n[14] Xu, Y.; Zhu, S.-C.; Tung, T. Denserac: Joint 3D pose\nand shape estimation by dense render-and-compare. In:\nProceedings of the IEEE International Conference on\nComputer Vision, 7760–7770, 2019.\n\n[15] Smith, D.; Loper, M.; Hu, X.; Mavroidis, P.; Romero,\nJ. FACSIMILE: Fast and accurate scans from an image\nin less than a second. In: Proceedings of the IEEE\nInternational Conference on Computer Vision, 5329–\n5338, 2019.\n\n[16] Alldieck, T.; Magnor, M.; Bhatnagar, B. L.; Theobalt,\nC.; Pons-Moll, G. Learning to reconstruct people in\nclothing from a single RGB camera. In: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern\nRecognition, 1175–1186, 2019.\n\n[17] Kolotouros, N.; Pavlakos, G.; Black, M. J.; Daniilidis,\nK. Learning to reconstruct 3D human pose and shape\nvia modelfitting in the loop. In: Proceedings of the\nIEEE International Conference on Computer Vision,\n2252–2261, 2019.\n\n[18] Liang, J.; Lin, M. C. Shape-aware human pose and\nshape reconstruction using multi-view images. In:\nProceedings of the IEEE International Conference on\nComputer Vision, 4352–4362, 2019.\n\n[19] Yang, S.; Pan, Z. R.; Amert, T.; Wang, K.; Yu, L.\nC.; Berg, T.; Lin, M. C. Physics-inspired garment\nrecovery from a single-view image. ACM Transactions\non Graphics Vol. 37, No. 5, Article No. 170, 2018.\n\n[20] Yang, S.; Liang, J.; Lin, M. C.; Learning-based cloth\nmaterial recovery from video. In: Proceedings of the\nIEEE International Conference on Computer Vision,\n4383–4393, 2017.\n\n[21] Qiao, Y. L.; Liang, J. B.; Koltun, V.; Lin, M. C.\nScalable differentiable physics for learning and control.\narXiv preprint arXiv:2007.02168, 2020.\n\n[22] De Avila Belbute-Peres, F.; Smith, K. A.; Allen, K.;\nTenenbaum, J.; Kolter, J. Z. End-to-end differentiable\nphysics for learning and control. In: Proceedings of the\nAdvances in Neural Information Processing Systems,\n2018.\n\n[23] Degrave, J.; Hermans, M.; Dambre, J.; Wyffels, F.\nA differentiable physics engine for deep learning in\nrobotics. Frontiers in Neurorobotics Vol. 13, 6, 2019.\n\n[24] Liang, J.; Lin, M.; Koltun, V. Differentiable cloth\nsimulation for inverse problems. In: Proceedings of\nthe 33rd Conference on Neural Information Processing\nSystems, 2019.\n\n\n\nMachine learning for digital try-on: Challenges and progress 167\n\n[25] Hu, Y.; Liu, J.; Spielberg, A.; Tenenbaum, J.\nB.; Freeman, W. T.; Wu, J.; Rus, D.; Matusik,\nW. ChainQueen: A real-time differentiable physical\nsimulator for soft robotics. In: Proceedings of the\nInternational Conference on Robotics and Automation,\n6265–6271, 2019.\n\n[26] Hu, Y. M.; Anderson, L.; Li, T. M.; Sun, Q.; Carr, N.;\nRagan-Kelley, J.; Durand, F. DiffTaichi: Differentiable\nprogramming for physical simulation. arXiv preprint\narXiv:1910.00935, 2019.\n\n[27] Liu, K. X.; Zeng, X. Y.; Bruniaux, P.; Tao, X. Y.; Yao,\nX. F.; Li, V.; Wang, J. 3D interactive garment pattern-\nmaking technology. Computer-Aided Design Vol. 104,\n113–124, 2018.\n\n[28] Huang, P.; Yao, J.; Zhao, H. Automatic realistic\n3D garment generation based on two images. In:\nProceedings of the International Conference on Virtual\nReality and Visualization, 250–257, 2016.\n\n[29] Wang, T. Y.; Ceylan, D.; Popović, J.; Mitra, N. J.\nLearning a shared shape space for multimodal garment\ndesign. ACM Transactions on Graphics Vol. 37, No. 6", "metadata_storage_path": "aHR0cHM6Ly9zdG9yYWdlYWNjb3VudGxpZ2lyay5ibG9iLmNvcmUud2luZG93cy5uZXQvcGFwZXIvTGlhbmctTGluMjAyMV9BcnRpY2xlX01hY2hpbmVMZWFybmluZ0ZvckRpZ2l0YWxUcnktby5wZGY1", "metadata_author": "Administrator", "metadata_title": "01-CVM0189.pdf", "keyphrases": [ "variable 3D human body shapes", "TSINGHUA UNIVERSITY PRESS Springer", "Computational Visual Media", "Ming C. Lin1", "The Author(s", "direct visual impression", "M. C. Lin", "2D image learning", "substantial technological gap", "human body estimation", "open research issues", "three major challenges", "human shapes", "open issues", "accurate estimation", "extensive research", "open challenges", "2 Open problems", "Machine learning", "doi.org", "Review Article", "Junbang Liang1", "economic benefits", "practical constraints", "accurate sizing", "art approaches", "future directions", "rapid pace", "recent years", "retail store", "attractive alternative", "user experience", "fashion shopping", "physical try", "image- editing", "College Park", "J. Liang", "ultimate goal", "real worlds", "physical worlds", "consumer devices", "faithful recovery", "sewing patterns", "traditional methods", "constrained problems", "learning-based approaches", "machine-learning algorithms", "tedious data", "challenging problems", "other areas", "several reasons", "material modeling", "accurate modeling", "faithful modeling", "garment material", "garment pieces", "recent advances", "template demonstrations", "realistic demonstration", "notable impact", "unseen images", "garment modeling", "digital try", "Abstract Digital", "tremendous potential", "problem description", "1 University", "Vol.", "June", "progress", "systems", "commerce", "people", "lives", "development", "realism", "complete", "state", "Keywords", "1 Introduction", "Consumers", "situation", "clothes", "needs", "attention", "technology", "Maryland", "USA", "mail", "liangjb", "umd", "Manuscript", "fast", "customer", "garments", "lossless", "transformation", "virtual", "paper", "sizes", "ease", "design", "manipulation", "end-users", "training", "optimization", "processing", "importance", "present", "improvements", "solutions", "significant", "section", "shoppers", "direct 3D body scanning", "full fabric digital model", "accurate human shape estimation", "parametric human model", "commodity mobile devices", "standard body shapes", "uncovered body parts", "2D/3D body skeletons", "Human body reconstruction", "human body geometry", "entire human body", "notable technology gap", "voxel- based representation", "successful digital try", "3D humanoid mesh", "human-body reconstruction problem", "3 Human shape estimation", "many other works", "machine learning methods", "Accurate estimation", "digital representation", "body movements", "shape learning", "accurate results", "perceptual gap", "3D mesh-based", "digital worlds", "3D animation", "digital surveillance", "early works", "recent works", "learning process", "sizing systems", "Different fabric", "actual material", "real-world examples", "two approaches", "2D image-based", "photo-realistic rendering", "considerable effort", "2D images", "user-friendly design", "realistic animation", "Previous work", "cloud computing", "animation speed", "high-quality, interactive", "computer animation", "special effects", "augmented environments", "popular topic", "specialized hardware", "RGB images", "optimization problem", "silhouette difference", "objective function", "target function", "response time", "network models", "real-world datasets", "joint positions", "various ways", "current state", "different brands", "garment materials", "garment database", "key consideration", "other factors", "online shopping", "regression network", "body-joint losses", "lack", "consistency", "standardization", "persons", "proportion", "consumers", "correspondence", "customers", "appearance", "advantages", "drawbacks", "large", "support", "variation", "case", "backend", "motion", "pose", "variety", "areas", "challenging", "interest", "excellent", "adoption", "input", "one", "size", "output", "algorithms", "major", "tight", "advances", "parameters", "techniques", "annotations", "most", "Ref.", "St Regression Regression Regression", "complex data generation pipeline", "Regression Regression Regression View", "iterative value correction structure", "synthetic data generation pipeline", "Multi-View Multi-Stage Regression Network", "convenient registration process", "larger garment dataset", "4 Garment material modeling", "image style transfer", "single view image", "good pose estimation", "parameter prediction block", "unified human shape", "human body parameters", "Network structure", "regression block", "static multi-view", "multi-stage framework", "recurrent structure", "RNN-like structure", "Prediction results", "body pose", "error correction", "multi-view inputs", "estimated pose", "body shapes", "human bodies", "Recovered body", "key contribution", "several stages", "image feature", "input guesses", "same time", "3D joints", "loss computation", "global orientation", "cyclic form", "corrective values", "ground-truth parameters", "performance gap", "real-world images", "other variables", "skin color", "3D backgrounds", "subtle elements", "higher expense", "Tesi b", "Image Encoder", "Adreamstime dreamsbr", "Input image", "promising direction", "realistic images", "important role", "Physical recreation", "synthetic result", "training data", "camera projection", "image inputs", "shape parameters", "camera calibration", "shape estimation", "visual information", "traditional datasets", "recent progress", "162 J. Liang", "Block 3,2 Block", "Stage 2 Stage", "different views", "fixed number", "universal model", "Block 02", "01,2 Block 92", "Stage 1", "effect", "ambiguity", "Fig.", "step", "gamma", "researchdirections", "virtualtryon", "humanmultiview", "Challenges", "permission", "2D", "features", "CNNs", "use", "gradient", "Experiments", "art", "example", "regularization", "cases", "diversity", "addition", "hair", "GAN", "legs", "chest", "person", "f2", "NX", "fn", "Introduction", "fabric", "image signal feature extraction method", "temporal sequence learning method", "deep neural network model", "material parameter sensitivity analysis", "garment material modeling problem", "spring–mass systems", "simple feed-forward operation", "varying lighting conditions", "compelling visual simulation", "high-level visual features", "optimal material parameter", "traditional optimization methods", "image features", "challenging problem", "fabric modeling", "visual effects", "machine-learning methods", "based simulation", "physical model", "popular model", "anisotropic model", "art model", "challenging task", "physical properties", "physical behaviour", "semantic primitives", "physical behavior", "real material", "two implications", "many ways", "finite elements", "realistic results", "isotropic properties", "Poisson ratio", "different behaviors", "4.2 Learning-based estimation", "fast feedback", "solution space", "material sub-space", "positive coefficient", "larger number", "visual appearance", "material properties", "consequent movement", "material cloning", "video recording", "other object", "real-world scenarios", "new insights", "natural extension", "partial occlusion", "differentiable physics", "geometric differences", "garment prototyping", "material parameters", "sufficient amount", "long time", "real time", "material type", "small amount", "motion patterns", "material basis", "data samples", "CNN layer", "single piece", "human body", "cloth simulations", "cloth meshes", "LSTM layer", "cloth motion", "materials", "sewing", "researchers", "underlying", "computer", "Young", "modulus", "choice", "weave", "predictions", "applications", "Yang", "videos", "code", "unc", "VideoCloth", "respect", "deformation", "sub-spaces", "texture", "mapping", "framework", "clothing", "approach", "Tu2 Tun CNN CNN CNN Classifier Material type LSTM cell Concatenate op T1 T2 Lrgb Tn", "CNN conv5-layer activation visualization", "general differentiable collision handling algorithm", "high dimensional simulation problem", "material parameter estimation", "finite element method", "modern simulation algorithms", "other differentiable simulations", "stable integration results", "implicit Euler method", "task-specific loss function", "differentiable cloth simulation", "general approach", "high dimensionality", "Recent differentiable", "collision response", "collision constraints", "M̂a", "physics simulations", "implicit differentiation", "unseen video", "input videos", "typical usage", "Similar processes", "pixel space", "3D space", "visual effect", "physics work", "rigid bodies", "particle-grid systems", "first work", "follow-up work", "computational flow", "common approach", "impact zones", "linear solver", "input parameters", "discontinuous states", "constraint matrix", "several techniques", "physical solution", "mass matrix", "linear solution", "solution z", "original paper", "complete � Liang", "164 J. Liang", "motion control", "clothes materials", "static LCP", "corresponding gradients", "gradient values", "digital", "model", "parts", "simulated", "skirt", "environment", "difference", "target", "network", "distance", "computing", "dynamics", "discretization", "order", "discontinuity", "zero-measure", "backpropagation", "QR", "decomposition", "pipeline", "Derivatives", "Jacobian", "forces", "acceleration", "equation", "readers", "details", "Collisions", "contacts", "data", "edu", "differentiablecloth", "1 2", "3.1", "3.2", "New Observation Control Simulation Observations Signals Time Collision Results", "multiple adjacent collision pairs", "Material parameter estimation results", "constant diagonal weight matrix", "differentiable simulation pipeline", "Differentiable Simulation Layer", "continuous collision detection", "constant wind force", "rigid body dynamics", "Trainable Network Layers", "original mesh state", "impact zone optimization", "bending stiffness models", "baselines Method Runtime", "Integration Response Loss", "Collision handling", "key observation", "stiffness matrix", "neural network", "physics simulator", "dynamic approach", "minimum change", "QP problem", "distance function", "quadratic form", "constraint parameters", "loss function", "linear system", "one contact", "linear equation", "QR decomposition", "high dimensional", "three parts", "stiffness S", "stiffness B.", "reaction force", "most measurements", "linear part", "unknown parameters", "generalized coordinates", "Frobenius norms", "simulated result", "gradient-based method", "smaller errors", "optimization formulation", "The scene", "vertex distances", "Linear stretching", "stiffness error", "cloth movement", "tasks", "implementation", "instances", "constraints", "collisions", "Gz", "mass", "numbers", "variables", "inputs", "goal", "dimensionality", "rank", "4 vertices", "experiments", "ability", "piece", "gravity", "density", "Table 1", "comparison", "Qiao", "dt", "L-BFGS.", "Values", "iter", "Liang", "velocity", "TM", "Gf", "inclusion", "−", "conditional generative adversarial network", "learning-based parametric generative model", "irregular 3D mesh data", "Realistic apparel model generation", "automatic garment model generation", "interactive apparel design systems", "3D garment mesh data", "U–V space", "latent space learning", "relative geometric information", "machine learning algorithms", "arbitrary human motions", "regular image data", "garment sewing patterns", "general cloth model", "garment generation algorithm", "2D displacement image", "fixed human body", "deep garment representation", "One possible approach", "The 2D representation", "garment material prediction", "other body shapes", "human shape estimation", "realistic 3D", "one model", "fashion apparel", "many challenges", "estimation accuracy", "One possibility", "image sketches", "5 Garment modeling", "garment models", "garment type", "computing gradients", "rapid changes", "fashion trends", "growing need", "different applications", "virtual try", "Application requirements", "diverse set", "different types", "universal pipeline", "recent work", "two problems", "different values", "traditional CNN", "different person", "virtual reality", "excellent potential", "open problems", "daily tasks", "good start", "same body", "intersection loss", "new solution", "substantial breakthroughs", "general garments", "unified representation", "limited topology", "Different topology", "external constraints", "hard constraints", "optimization-based methods", "multiple garments", "neighborhood", "approximation", "customization", "extent", "Huang", "front", "Wang", "al.", "retargeting", "T-shirts", "skirts", "difficulties", "cGAN", "6 Conclusions", "scenario", "wind", "applicability", "area", "investigation", "directions", "Acknowledgements", "research", "part", "Iribe", "Y. Realtime multi- person 2D pose estimation", "Realtime 3D human pose estimation", "Image-based clothes changing system", "skinned multi-person linear model", "2D human representations", "M. C. Lin References", "Convolutional pose machines", "Shape-aware human pose", "Joint 3D pose", "convolutional neural networks", "part affinity fields", "single RGB camera", "Video based reconstruction", "human body shapes", "National Science Foundation", "4th International Conference", "3D human reconstruction", "IEEE International Conference", "A. Eds. Springer", "Visual Media Vol.", "M. J. SMPL", "IEEE Conference", "human shape", "human digitization", "shape reconstruction", "3D Vision", "Computer Science", "European Conference", "Y. Deephuman", "single image", "naked truth", "Computer Vision", "Lecture Notes", "ACM Transactions", "Article No.", "Pons- Moll", "I. Bodynet", "Volumetric inference", "high-resolution clothed", "accurate scans", "multi-view images", "3D people", "a second", "M. HS-Nets", "C. VNect", "J. FACSIMILE", "Öztireli", "H.-P.", "H. PIFu", "F. L.", "B. L.", "A. O.", "T. Denserac", "Pattern Recognition", "S.-E.", "S.-C.", "Graphics Vol.", "166 J. Liang", "T. J.", "H. T.", "Z. H.", "P. V.", "D. W.", "Professorship", "Zheng", "Zhang", "Computational", "Dibra", "Jain", "Ziegler", "R.", "Gross", "silhouettes", "Proceedings", "Black", "ECCV", "Forsyth", "Torr", "Zisserman", "Berlin", "Romero", "Kiefel", "Bogo", "Gehler", "loop", "Loper", "Mahmood", "N.", "Pons-Moll", "G.", "Wei", "Ramakrishna", "Kanade", "Sheikh", "Cao", "Simon", "Mehta", "Sridhar", "Sotnychenko", "Rhodin", "Shafiei", "Seidel", "Xu", "Casas", "Theobalt", "Alldieck", "Magnor", "models", "Kanazawa", "Jacobs", "Malik", "end", "recovery", "Varol", "Ceylan", "Russell", "Yumer", "Laptev", "Dai", "Q.", "Liu", "Saito", "Natsume", "Morishima", "Pixel-aligned", "function", "Zhu", "Tung", "Smith", "X.", "Mavroidis", "Fast", "less", "Bhatnagar", "Kolotouros", "Pavlakos", "Daniilidis", "K.", "modelfitting", "Neural Information Processing Systems", "real-time differentiable physical simulator", "V. Differentiable cloth simulation", "De Avila Belbute-Peres", "shared shape space", "Scalable differentiable physics", "differentiable physics engine", "3D garment generation", "multimodal garment design", "physical simulation", "Learning-based cloth", "Physics-inspired garment", "Computer-Aided Design", "33rd Conference", "Z. R.", "single-view image", "arXiv preprint", "inverse problems", "W. ChainQueen", "H. Automatic", "two images", "Virtual Reality", "X. Y.", "M. C.", "L. C.", "W. T.", "Y. M.", "material recovery", "Y. L.", "deep learning", "F. DiffTaichi", "K. X.", "X. F.", "T. Y.", "soft robotics", "T. M.", "J. B.", "J. Z.", "K. A.", "N. J.", "Pan", "Amert", "Yu", "Berg", "Lin", "S.", "video", "Koltun", "control", "Allen", "Tenenbaum", "Kolter", "Advances", "Degrave", "Hermans", "Dambre", "Wyffels", "Frontiers", "Neurorobotics", "Hu", "Freeman", "Wu", "Rus", "D.", "Matusik", "Automation", "Anderson", "Sun", "Carr", "Ragan-Kelley", "Durand", "programming", "Zeng", "Bruniaux", "P.", "Tao", "Yao", "Zhao", "Visualization", "Popović", "Mitra" ], "merged_content": "\nComputational Visual Media\nhttps://doi.org/10.1007/s41095-020-0189-1 Vol. 7, No. 2, June 2021, 159–167\n\nReview Article\n\nMachine learning for digital try-on: Challenges and progress\n\nJunbang Liang1 (�), Ming C. Lin1\n\nc© The Author(s) 2020.\n\nAbstract Digital try-on systems for e-commerce have\nthe potential to change people’s lives and provide notable\neconomic benefits. However, their development is limited\nby practical constraints, such as accurate sizing of the\nbody and realism of demonstrations. We enumerate three\nopen challenges remaining for a complete and easy-to-use\ntry-on system that recent advances in machine learning\nmake increasingly tractable. For each, we describe\nthe problem, introduce state-of-the-art approaches, and\nprovide future directions.\n\nKeywords machine learning; digital try-on; garment\nmodeling; human body estimation; material\nmodeling\n\n1 Introduction\nE-commerce has grown at a rapid pace in recent\nyears. Consumers today are more likely to shop\nonline than to visit a retail store. The situation is\nmuch more complicated, however, when it comes to\nbuying clothes. People need to know how a garment\nfits on them, how it looks, and how it feels. Digital\ntry-on systems can potentially satisfy these needs,\nproviding a direct visual impression, and possibly\ncustomized clothes sizing as well. Therefore, it has\ndrawn much attention as an attractive alternative to\nimprove the user experience and popularize online\nfashion shopping.\n\nHowever, the technology is still far from practical,\neasy-to-use, and adequate to replace physical try-on.\nCurrently, most try-on systems rely on either image-\nediting, copy-pasting, or template demonstrations,\n\n1 University of Maryland, College Park, MD 20785, USA.\nE-mail: J. Liang, liangjb@cs.umd.edu (�); M. C. Lin,\nlin@cs.umd.edu.\n\nManuscript received: 2020-06-24; accepted: 2020-07-21\n\nwhile the ultimate goal is a fast and realistic try-on\nsystem adaptive to each customer’s body. There is\nstill a substantial technological gap between modeling\nand demonstrating garment fitting in the digital and\nreal worlds, including fast and realistic demonstration,\naccurate modeling of human body and garments,\nfaithful modeling of garment material, and lossless\ntransformation of garments between virtual and\nphysical worlds.\n\nIn this paper, we present some open research issues\nthat contribute to this technological gap, including:\n1. accurate estimation of human shapes and sizes\n\nusing consumer devices,\n2. faithful recovery of garment materials via (online)\n\nimages, and\n3. ease of design and manipulation of sewing patterns\n\nand garment pieces by end-users.\nAlthough traditional methods have made important\n\nprogress on these under-constrained problems,\nlearning-based approaches have shown tremendous\npotential to make a notable impact. Compared to\ntraditional methods, machine-learning algorithms are\nusually much faster since training and optimization\nare performed offline. They are also good at\ngeneralizing to unseen images without the need for\ntedious data pre-processing. While extensive research\nexists on 2D image learning, machine learning of\nhighly variable 3D human body shapes is still far\nfrom mature, which is the reason why the open issues\ndescribed above remain elusive.\n\nFor each problem listed above, we motivate its\nimportance, provide a problem description, and\npresent state-of-the-art approaches with potential\nfor improvements. We believe that solutions to\nthese challenging problems will lead to significant\nadvances in digital try-on, as well as other areas of\ne-commerce.\n\n159\n\n \n\n TSINGHUA UNIVERSITY PRESS Springer \n\n\n\n160 J. Liang, M. C. Lin\n\n2 Open problems\nIn this section we first introduce three major\nchallenges that limit digital try-on technology from\nbeing widely adopted and accepted by shoppers.\nThere are several reasons why shoppers still prefer\nphysical try-on. Firstly, consumers are unsure if what\nthey buy online will fit them well. Although general\nsizing systems exist, their lack of consistency and\nstandardization across different brands and garment\nmaterials can often make it difficult to size clothes,\nespecially for persons with non-standard body shapes\nand proportion. Accurate estimation of human body\nshape is the key to successful digital try-on. Secondly,\nfabric is usually a key consideration when shopping\nfor clothes. Different fabric affects how garments\nlook and fit, how consumers would wear them, and\nwhether or not they would buy them. However, the\ncorrespondence between the actual material and its\ndigital representation are not well understood. It is\nalso challenging to acquire a full fabric digital model\nfrom real-world examples.\n\nFor the customers, appearance is as critical as other\nfactors. There are two approaches to displaying\ngarments: 2D image-based, and 3D mesh-based\nwith photo-realistic rendering. They have different\nadvantages and drawbacks, but both need a large\ngarment database for support. While creating a 3D\ngarment takes considerable effort, 2D images often\nsuffer from a lack of variation and are much more\ndifficult to customize. In either case, the try-on\nsystem needs a user-friendly design and manipulation\nbackend. Last, but not least, a fast and realistic\nanimation of the garments in motion along with\nbody movements greatly improves the user experience.\nAlthough it is not so critical as other factors, it\nwould effectively reduce the perceptual gap between\nthe real and digital worlds for online shopping.\nPrevious work has proposed using cloud computing\nto improve the animation speed, but there is still a\nnotable technology gap for high-quality, interactive\n3D animation of clothes.\n\n3 Human shape estimation\nAs noted, accurate human shape estimation is key to\nenabling digital try-on. Human body reconstruction,\nconsisting of pose and shape estimation, has been\nwidely studied in a variety of areas, including digital\n\nsurveillance, computer animation, special effects, and\nvirtual and augmented environments. Yet, it remains\na challenging and popular topic of interest. While\ndirect 3D body scanning can provide excellent and\naccurate results, its adoption is somewhat limited by\nthe required specialized hardware. RGB images are\nwidely available for input to digital try-on and can\nbe easily captured using commodity mobile devices.\nAlthough purely image-based try-on methods have\nbeen proposed [1], learning-based 3D body estimation\nis more widely applicable in that the 3D body can be\narticulated and so re-posed and re-targeted.\n\nWe define the human-body reconstruction problem\ninformally as, given one or more RGB images, to\nestimate the human body geometry and size, and\noutput (preferably) a 3D humanoid mesh. Traditional\nalgorithms often formulate it as an optimization\nproblem, in which the silhouette difference is a major\npart of the objective function [2]. Therefore, these\nmethods either require the human to wear tight\nclothes, or alternatively relax the target function\nto be unilateral on uncovered body parts [3], or\nto point correspondences [4]. The use of machine\nlearning methods in this problem has led to significant\nadvances. Firstly, it has moved the algorithm from\nonline to offline, significantly reducing response time.\nSecond, by using a parametric human model [5],\none can easily construct a regression network for\nthe parameters while the losses needed can also be\ninferred from them. While early works proposed\nnetwork models for only 2D/3D body skeletons [6–\n8], more recent works have introduced techniques to\nperform regression for the entire human body—either\nusing a parametric human model [9, 10] or a voxel-\nbased representation [11–13]. As annotations in most\nreal-world datasets contain only joint positions, the\nlearning process has been refined in various ways [14–\n17]. The current state of the art is the recent work\nby Ref. [18] �. It emphasizes shape learning, while\nmany other works often focus on body-joint losses,\nbut neglect the effect of body shapes.\n\nThe key contribution of Ref. [18] is a multi-view,\nmulti-stage framework to address ambiguity caused by\ncamera projection (see Fig. 1). Their model performs\nseveral stages of error correction. Each of the image\ninputs is passed on step by step; at each step, a shared-\n\n� Liang and Lin’s data and code are available at https://gamma.umd.edu/\nresearchdirections/virtualtryon/humanmultiview\n\n TSINGHUA UNIVERSITY PRESS Springer \n\n\n\nMachine learning for digital try-on: Challenges and progress 161\n\nFig. 1 Network structure from Ref. [18]. By using an iterative value correction structure, visual information from different views is effectively\nintegrated to provide a unified human shape. Reproduced with permission from Ref. [18], c© The Author(s) 2019.\n\nparameter prediction block computes the correction\nbased on the image feature and the input guesses.\nThe camera and the human body parameters are\nestimated at the same time, projecting the predicted\n3D joints back to 2D for loss computation. The\nestimated pose and shape parameters are shared\namong all views, while each view maintains its own\ncamera calibration and global orientation. Their\nproposed framework uses a recurrent structure,\nmaking it a universal model applicable to any number\nof views. At the same time, it couples shareable\ninformation across different views so that the human\nbody pose and shape are optimized using image\nfeatures from all views. Unlike static multi-view\nCNNs which have a fixed number of inputs, they\nmake use of the RNN-like structure in a cyclic form to\naccept any number of views, and prevent the gradient\nvanishing by predicting corrective values instead of\nupdating parameters in each regression block.\n\nExperiments have shown that, after training, this\nmodel can form a single view image, provide equally\ngood pose estimation as the state of the art, and\nprovide considerably improved pose estimation when\nusing multi-view inputs, leading to better shape\nestimation across all datasets. An example is\ndemonstrated in Fig. 2. Moreover, a physically-based\nsynthetic data generation pipeline is introduced to\nenrich the training data, which is very helpful for\n\nshape estimation and regularization in cases that\ntraditional datasets do not capture. While synthetic\ndata improves the diversity of human bodies with\nground-truth parameters, a larger garment dataset\nand a more convenient registration process are needed\nto minimize the performance gap between real-world\nimages and synthetic data. In addition, other\nvariables such as hair, skin color, and 3D backgrounds\nare subtle elements that can influence the perceived\nrealism of the synthetic data at the higher expense of\na more complex data generation pipeline. With the\nrecent progress in image style transfer using GAN, a\n\nFig. 2 Prediction results using the state of the art [18]. The model\ncaptures the shape of the human body by learning from synthetic\ndata. The recovered legs and chest are close to those of the person\nin the image. Reproduced with permission from Ref. [18], c© The\nAuthor(s) 2019.\n\n (1,1 To2.1 Tesi b b b fi f1 fi St Regression Regression Regression View 1 1,1 Block 02,1 Block 3,1 Block (1,1) (2,1) C (3,1) - - 61,2 62,2 3,2 - b b b - f2 NX f2 f2 - Regression Regression Regression - View 2 01,2 Block 92,2 Block 3,2 Block (1,2) (2,2) C O - (3,2) Image Encoder - - 01,n 63.1 - b 'b b - fn fn fn - Regression Regression Regression View n 31,n Block 92,n Block 3.n Block (1,n) (2,n) C (3,n) - 62,1 63,1 * b b b Stage 1 Stage 2 Stage 3 Multi-View Multi-Stage Regression Network \n\n Adreamstime dreamsbr (a) Input image (b) Recovered body \n\n\n\n162 J. Liang, M. C. Lin\n\npromising direction is to transfer the synthetic result\nto more realistic images to further improve the result.\n\n4 Garment material modeling\n4.1 Introduction\nGarment material plays an important role in digital\ntry-on systems. Physical recreation of the fabric not\nonly gives a compelling visual simulation of the cloth,\nbut also affects how the garment feels and fits on\nthe body. However, fabric modeling is a challenging\ntask: the appearance and physical properties of\nthe garment are determined not only by the type\nof materials the clothes are made of, but also by\nsewing and weave. Thus, researchers often focus on\nthe physical behaviour, rather than the underlying\nsemantic primitives.\n\nHence, we state the garment material modeling\nproblem as follows. Given a sufficient amount of data,\nmodel the material’s physical behavior and physical\nproperties, so that visual effects the same as or similar\nto those of the real material can be reproduced by a\ncomputer. This has two implications: firstly, we need\nto define a physical model of the material, and then\nwe must estimate the parameters in the model.\n\nThere are many ways to model clothes, including\nspring–mass systems and finite elements. The latter is\nthe most popular model since it can produce realistic\nresults. While one can use isotropic properties such\nas Young’s modulus and Poisson ratio, an anisotropic\nmodel is a better choice since it can support different\nbehaviors caused by the weave of the material.\n\n4.2 Learning-based estimation\nWhile traditional optimization methods [19] often\ntake a long time to compute material parameters,\nmachine-learning methods can make predictions in\nreal time by a simple feed-forward operation, which\nis more useful in applications that need fast feedback,\nsuch as garment prototyping. The state-of-the-art\nmodel from Yang et al. [20] � uses CNNs combined\nwith LSTM to recover material parameters from\nvideos. To constrain both the input and solution\nspace, they choose one of the materials as a basis;\nthe material sub-space is constructed by multiplying\nthis material basis with a positive coefficient. To\nconstruct an optimal material parameter sub-space, a\n\n� Yang et al.’s data and code are available at http://gamma.cs.unc.edu/\nVideoCloth\n\nmaterial parameter sensitivity analysis is conducted\nto examine the sensitivity of the material parameters\nκ with respect to the amount of deformation D(κ).\nPhysically based cloth simulations are used to\ngenerate a much larger number of data samples within\nthese sub-spaces, which would otherwise be difficult\nor time-consuming to capture. The cloth meshes are\ngenerated through physically-based simulation, and\nthen rendered as 2D images with a randomly assigned\ntexture. Using the data samples, they combine the\nimage signal feature extraction method, a CNN, with\nthe temporal sequence learning method, LSTM, to\nlearn the mapping from visual appearance to material.\nAs shown in Fig. 3, the CNN layer is used to extract\nboth low- and high-level visual features, while the\nLSTM layer focuses on learning the mapping between\nthe material properties of the cloth and its consequent\nmovement.\n\nThey demonstrated the proposed framework with\nthe application of “material cloning”. With the\ntrained deep neural network model being able to\ncapture the cloth motion (Fig. 4), the material type\ncan be inferred from a video recording of the motion\nof the cloth in a fairly small amount of time. The\nrecovered material type can be “cloned” onto another\npiece of cloth or garment as shown in Fig. 5.\n\nIn this work, the videos contain only a single piece\nof cloth which does not interact with any other object.\nWhile this is not applicable to all real-world scenarios,\nthis method provides new insights into addressing this\nchallenging problem. A natural extension would be to\nlearn from videos of clothing directly interacting with\nthe human body, under varying lighting conditions\nand partial occlusion.\n\n4.3 Optimization using differentiable physics\nAnother approach to modeling the fabric is to measure\ngeometric differences directly during parameter\n\nFig. 3 Network model from Ref. [20]. The material is modeled\nby learning motion patterns of image features given by CNNs.\nReproduced with permission from Ref. [20], c© The Author(s) 2017.\n\n = 1 2 hn-1 2n ( c ) , 0 2 ,.... x 2 ) Tu2 Tun CNN CNN CNN Classifier Material type LSTM cell Concatenate op T1 T2 Lrgb Tn rgb -rgb \n\n\n\nMachine learning for digital try-on: Challenges and progress 163\n\nFig. 4 Learned CNN conv5-layer activation visualization from\nRef. [20]. Experiments show that the trained model is able to capture\nmoving parts of the cloth even in an unseen video. Reproduced with\npermission from Ref. [20], c© The Author(s) 2017.\n\nFig. 5 Yang et al. [20] modeled clothes materials in input videos (left),\nand applied those materials to a simulated skirt (right). Reproduced\nwith permission from Ref. [20], c© The Author(s) 2017.\n\noptimization. Assuming that the environment is\nknown to the system, computation of the estimated\nmotion and its gradient with respect to the material\nparameters can be achieved using differentiable\nsimulation. A typical usage of differentiable\nsimulation is motion control (see Fig. 6), where the\ndifference to the target is measured and the loss\nbackpropagated to the network. Similar processes\ncan be applied to material parameter estimation as\nwell. By measuring the distance to the target as the\nloss and computing corresponding gradients, either in\npixel space or in 3D space, the material parameters\n\ncan be learned or optimized to achieve the desired\ncloth motion or visual effect. Recent differentiable\nphysics work covers rigid bodies [22, 23], cloth [24],\nand particle-grid systems [25, 26]. The state-of-\nthe-art is Ref. [24] �, which proposes a method for\ndifferentiable cloth simulation. It is the first work\nto tackle a high dimensional simulation problem\nand to propose a general differentiable collision\nhandling algorithm. Later, a follow-up work [21]\nextended the algorithm to be applicable to coupled\ndynamics with rigid bodies. Overall, they follow\nthe computational flow of the common approach\nto cloth simulation: discretization using the finite\nelement method, integration using an implicit Euler\nmethod, and collision response on impact zones. They\nuse implicit differentiation in the linear solver and\noptimization in order to compute the gradient with\nrespect to the input parameters. The discontinuity\nintroduced by collision response is negligible because\nthe discontinuous states constitute a zero-measure\nset. During backpropagation in the optimization,\ngradient values can be directly computed after QR\ndecomposition of the constraint matrix. Their\npipeline contains several techniques that can be\nemployed in other differentiable simulations.\n4.3.1 Derivatives of the physical solution\nIn modern simulation algorithms, an implicit Euler\nmethod is often used for stable integration results.\nThus the mass matrix M often includes the Jacobian\nof the forces, and is denoted as M̂ to indicate this\ndifference. A linear solver is needed to compute the\nacceleration since it is time-consuming to compute\nM̂−1. Implicit differentiation is used to compute the\ngradients of the linear solution. Given an equation\nM̂a = f with a solution z and propagated gradient\n∂L/∂a|a=z, where L is the task-specific loss function,\nimplicit differentiation is used to derive the gradients.\nWe refer readers to the original paper [24] for more\ndetails.\n4.3.2 Derivatives of the collision response\nA general approach using LCP to integrate collision\nconstraints into physics simulations has been\nproposed, but constructing a static LCP is often\nimpractical in cloth simulation due to the high\ndimensionality. Collisions and contacts which happen\nat each step are very sparse compared to the complete\n\n� Liang et al.’s data and code are available at https://gamma.umd.edu/\nresearchdirections/virtualtryon/differentiablecloth\n\n F-1 F-25 F-38 F-105 F-1 F-25 F-38 F-105 F-1 F-2 F-3 F-4 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 F-1 F-2 F-3 F-4 0.1 O \n\n \n\n\n\n164 J. Liang, M. C. Lin\n\nFig. 6 Differentiable simulation embedding example from Ref. [21]. The loss can be backpropagated through the physics simulator to the\nneural network, enabling learning tasks such as material modeling and motion control.\n\ndata. Therefore, a dynamic approach is used that\nincorporates collision detection and response.\n\nCollision handling in their implementation is based\non impact zone optimization. It finds all colliding\ninstances using continuous collision detection and\nsets up the constraints for all collisions. In order\nto introduce minimum change to the original mesh\nstate, a QP problem is developed to determine the\nconstraints. Since the signed distance function is\nlinear in x, the optimization takes a quadratic form,\nas shown originally in Ref. [24]:\n\nminimize\nz\n\n1\n2\n\n(z − x)TW (z − x),\n\nsubject to Gz + h � 0\nwhere W is a constant diagonal weight matrix related\nto the mass of each vertex, and G and h are constraint\nparameters. The numbers of variables and constraints\nare n and m, i.e. x ∈ R\n\nn, h ∈ R\nm, and G ∈ R\n\nm×n.\nNote that this optimization problem has inputs x,\nG, and h, and output z. The goal here is to derive\n∂L/∂x, ∂L/∂G, and ∂L/∂h given ∂L/∂z, where L\nis the loss function.\n\nWhen computing the gradient using implicit\ndifferentiation, the dimensionality of the linear system\ncan be very high. Their key observation here is that\nn >> m > rank(G), since one contact often involves 4\nvertices (thus 12 variables) and some contacts may be\nlinearly dependent (e.g., multiple adjacent collision\n\npairs). They minimize the size of the linear equation\nbased on the QR decomposition of G, which is the key\nto accelerating backpropagation of high dimensional\nQP problems.\n\nOne of their experiments shows its ability to\noptimize material parameters from observation. The\nscene features a piece of cloth hanging under gravity\nand subjected to a constant wind force. The material\nmodel consists of three parts: density d, stretching\nstiffness S, and bending stiffness B. The stretching\nstiffness quantifies the reaction force when the cloth\nis stretched; the bending stiffness models how easily\nthe cloth can be bent and folded. Table 1 shows\nresults. They achieve a much smaller error in most\nmeasurements in comparison to the baselines; the\nlinear part of the stiffness matrix is modeled well.\nWith the computed gradient using their model, one\ncan effectively optimize the unknown parameters that\ndominate cloth movement to fit the observed data.\n\nIn follow-up work, Qiao et al. extended the\ndifferentiable simulation pipeline to couple with\nrigid body dynamics, formulated using generalized\ncoordinates:\n\nd\ndt\n\n⎛\n⎝ q\n\nq̇\n\n⎞\n⎠ =\n\n⎛\n⎝ q̇\n\nq̈\n\n⎞\n⎠ =\n\n⎛\n⎝ q̇\n\nM−1f(q, q̇)\n\n⎞\n⎠\n\nand update the optimization formulation for collision\nresponse accordingly (see Ref. [21] for details):\n\nTable 1 Material parameter estimation results from Ref. [24]. Their proposed method runs faster than L-BFGS. Values of material parameters\nare Frobenius norms of the difference normalized by the Frobenius norm of the target. Values of the simulated result are the average pairwise\nvertex distances normalized by the size of the cloth. The gradient-based method yields much smaller errors than the baselines\n\nMethod\nRuntime\n\n(sec/step/iter)\n\nDensity\n\nerror (%)\n\nLinear stretching\n\nstiffness error (%)\n\nBending stiffness\n\nerror (%)\n\nSimulation\n\nerror (%)\n\nBaseline — 68 ± 46 160 ± 119 70 ± 42 12 ± 3.0\n\nL-BFGS 2.89 ± 0.02 4.2 ± 5.6 72 ± 90 70 ± 43 4.9 ± 3.3\n\nLiang et al. [24] 2.03 ± 0.06 1.8 ± 2.0 45 ± 41 77 ± 36 1.6 ± 1.4\n\n New Observation Control Simulation Observations Signals Time Collision Results 00000000 0000 000000000 000000000 000000000 Integration Response Loss State: velocity, position, material, force ... Trainable Network Layers Differentiable Simulation Layer \n\n\n\nMachine learning for digital try-on: Challenges and progress 165\n\nminimize\nq′\n\n1\n2\n\n(q − q′)TM̂(q − q′)\n\nsubject to Gf(q′) + h � 0\n\nDue to the inclusion of rigid bodies, the constraints\nused in the optimization are no longer linear. When\ncomputing gradients, they linearize the constraints\naround a neighborhood as an approximation to enable\nQR decomposition for acceleration as previously\nmentioned.\n\n5 Garment modeling and design\nRealistic apparel model generation has become\nincreasingly popular, due to the rapid changes in\nfashion trends and the growing need for garment\nmodels in different applications such as virtual try-\non. It is already used even for state-of-the-art\ninteractive apparel design systems [27]. Application\nrequirements mean that it is important to have a\ngeneral cloth model that can represent a diverse set\nof garments. However, there are many challenges\nin automatic garment model generation. Firstly,\ngarments usually have different types of topology,\nespecially for fashion apparel, that makes it difficult\nto design a universal pipeline. Moreover, it is often\nnot straightforward for general garments design to\nbe retargeted onto another body shape, making\ncustomization difficult.\n\nPrevious work has addressed this problem to some\nextent. Huang et al. [28] proposed a realistic 3D\ngarment generation algorithm based on front and\nback image sketches, but it cannot readily retarget\ngenerated garments to other body shapes. Wang et\nal. [29] proposed an algorithm which can conveniently\nperform retargeting, but permits limited topology\nlike T-shirts or skirts. There is no recent work that\naddresses these two problems at the same time.\n\nWe introduce a learning-based parametric\ngenerative model to overcome the above difficulties,\ngiven garment sewing patterns and human body\nshapes as input. One possible approach would be to\ncompute a displacement image on the U–V space\nof the human body as a unified representation of\nthe garment mesh. Different topology and sizes\nof the garment are represented by different values\nin the image. The 2D displacement image, as the\nrepresentation of the 3D garment mesh data, can\n\nthen be fed into a conditional generative adversarial\nnetwork (cGAN) for latent space learning. The 2D\nrepresentation for the garment mesh can transfer\nthe irregular 3D mesh data to regular image data\nwhere a traditional CNN can easily learn. It can also\nextract relative geometric information with respect\nto the human body, enabling garment retargeting to\na different person.\n\n6 Conclusions\nAlthough virtual reality and digital try-on have\nexcellent potential and are rapidly developing, there\nremain open problems before online try-on systems\ncan be widely adopted. We have listed three major\nchallenges, all of which can be addressed or further\nimproved using machine learning algorithms. For\ngarment material prediction, state-of-the-art methods\nare still limited in that the training data is highly\nconstrained: the scenario contains only a piece\nof cloth floating in the wind. To improve its\napplicability to daily tasks, it is necessary to focus\non solving the problem on a more diverse set of\ninputs. Predicting the material from a garment\non a fixed human body could be a good start,\nbefore generalizing to arbitrary human motions and\npredicting multiple garments on the same body. In\nthe area of human shape estimation, it would be\ninteresting to learn how external constraints could\nimprove estimation accuracy. For example, the shape\nand size of the garment are hard constraints to\nwhich the predicted body should conform. While\noptimization-based methods can integrate these\nconstraints fairly easily, doing so remains elusive\nfor learning-based approaches. One possibility is\nto jointly estimate body and garment together and\nintroduce an intersection loss. This approach would\nrequire a new solution to the open problem of unified\ndeep garment representation, if we do not want to\ntrain one model for every garment type, which could\nbe even more challenging. We believe that substantial\nbreakthroughs in digital try-on are achievable with\nmore investigation in these directions.\n\nAcknowledgements\nThis research was supported in part by the Iribe\nProfessorship and the National Science Foundation.\n\n\n\n166 J. Liang, M. C. Lin\n\nReferences\n\n[1] Zheng, Z. H.; Zhang, H. T.; Zhang, F. L.; Mu, T. J.\nImage-based clothes changing system. Computational\nVisual Media Vol. 3, No. 4, 337–347, 2017.\n\n[2] Dibra, E.; Jain, H.; Öztireli, C.; Ziegler, R.; Gross,\nM. HS-Nets: Estimating human body shape from\nsilhouettes with convolutional neural networks. In:\nProceedings of the 4th International Conference on\n3D Vision, 108–117, 2016.\n\n[3] Bălan, A. O.; Black, M. J. The naked truth: Estimating\nbody shape under clothing. In: Computer Vision –\nECCV 2008. Lecture Notes in Computer Science, Vol.\n5303. Forsyth, D.; Torr, P.; Zisserman, A. Eds. Springer\nBerlin, 15–29, 2008.\n\n[4] Lassner, C.; Romero, J.; Kiefel, M.; Bogo, F.; Black,\nM. J.; Gehler, P. V. Unite the people: Closing the\nloop between 3D and 2D human representations. In:\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 4704–4713, 2017.\n\n[5] Loper, M.; Mahmood, N.; Romero, J.; Pons-Moll, G.;\nBlack, M. J. SMPL: A skinned multi-person linear\nmodel. ACM Transactions on Graphics Vol. 34, No. 6,\nArticle No. 248, 2015.\n\n[6] Wei, S.-E.; Ramakrishna, V.; Kanade, T.; Sheikh, Y.\nConvolutional pose machines. In: Proceedings of the\nIEEE conference on Computer Vision and Pattern\nRecognition, 4724–4732, 2016.\n\n[7] Cao, Z.; Simon, T.; Wei, S.; Sheikh, Y. Realtime multi-\nperson 2D pose estimation using part affinity fields.\nIn: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 1302–1310, 2017.\n\n[8] Mehta, D.; Sridhar, S.; Sotnychenko, O.; Rhodin, H.;\nShafiei, M.; Seidel, H.-P.; Xu, W.; Casas, D.; Theobalt,\nC. VNect: Realtime 3D human pose estimation with\na single RGB camera. ACM Transactions on Graphics\nVol. 36, No. 4, Article No. 44, 2017.\n\n[9] Alldieck, T.; Magnor, M.; Xu, W.; Theobalt, C.; Pons-\nMoll, G. Video based reconstruction of 3D people\nmodels. In: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 8387–8397,\n2018.\n\n[10] Kanazawa, A.; Black, M. J.; Jacobs, D. W.; Malik, J.\nEnd-to-end recovery of human shape and pose. In:\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 7122–7131, 2018.\n\n[11] Varol, G.; Ceylan, D.; Russell, B.; Yang, J.; Yumer,\nE.; Laptev, I. Bodynet: Volumetric inference of 3D\nhuman body shapes. In: Proceedings of the European\nConference on Computer Vision, 20–36, 2018.\n\n[12] Zheng, Z.; Yu, T.; Wei, Y.; Dai, Q.; Liu, Y. Deephuman:\n3D human reconstruction from a single image. In:\n\nProceedings of the IEEE International Conference on\nComputer Vision, 7739–7749, 2019.\n\n[13] Saito, S.; Huang, Z.; Natsume, R.; Morishima, S.;\nKanazawa, A.; Li, H. PIFu: Pixel-aligned implicit\nfunction for high-resolution clothed human digitization.\nIn: Proceedings of the IEEE International Conference\non Computer Vision, 2304–2314, 2019.\n\n[14] Xu, Y.; Zhu, S.-C.; Tung, T. Denserac: Joint 3D pose\nand shape estimation by dense render-and-compare. In:\nProceedings of the IEEE International Conference on\nComputer Vision, 7760–7770, 2019.\n\n[15] Smith, D.; Loper, M.; Hu, X.; Mavroidis, P.; Romero,\nJ. FACSIMILE: Fast and accurate scans from an image\nin less than a second. In: Proceedings of the IEEE\nInternational Conference on Computer Vision, 5329–\n5338, 2019.\n\n[16] Alldieck, T.; Magnor, M.; Bhatnagar, B. L.; Theobalt,\nC.; Pons-Moll, G. Learning to reconstruct people in\nclothing from a single RGB camera. In: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern\nRecognition, 1175–1186, 2019.\n\n[17] Kolotouros, N.; Pavlakos, G.; Black, M. J.; Daniilidis,\nK. Learning to reconstruct 3D human pose and shape\nvia modelfitting in the loop. In: Proceedings of the\nIEEE International Conference on Computer Vision,\n2252–2261, 2019.\n\n[18] Liang, J.; Lin, M. C. Shape-aware human pose and\nshape reconstruction using multi-view images. In:\nProceedings of the IEEE International Conference on\nComputer Vision, 4352–4362, 2019.\n\n[19] Yang, S.; Pan, Z. R.; Amert, T.; Wang, K.; Yu, L.\nC.; Berg, T.; Lin, M. C. Physics-inspired garment\nrecovery from a single-view image. ACM Transactions\non Graphics Vol. 37, No. 5, Article No. 170, 2018.\n\n[20] Yang, S.; Liang, J.; Lin, M. C.; Learning-based cloth\nmaterial recovery from video. In: Proceedings of the\nIEEE International Conference on Computer Vision,\n4383–4393, 2017.\n\n[21] Qiao, Y. L.; Liang, J. B.; Koltun, V.; Lin, M. C.\nScalable differentiable physics for learning and control.\narXiv preprint arXiv:2007.02168, 2020.\n\n[22] De Avila Belbute-Peres, F.; Smith, K. A.; Allen, K.;\nTenenbaum, J.; Kolter, J. Z. End-to-end differentiable\nphysics for learning and control. In: Proceedings of the\nAdvances in Neural Information Processing Systems,\n2018.\n\n[23] Degrave, J.; Hermans, M.; Dambre, J.; Wyffels, F.\nA differentiable physics engine for deep learning in\nrobotics. Frontiers in Neurorobotics Vol. 13, 6, 2019.\n\n[24] Liang, J.; Lin, M.; Koltun, V. Differentiable cloth\nsimulation for inverse problems. In: Proceedings of\nthe 33rd Conference on Neural Information Processing\nSystems, 2019.\n\n\n\nMachine learning for digital try-on: Challenges and progress 167\n\n[25] Hu, Y.; Liu, J.; Spielberg, A.; Tenenbaum, J.\nB.; Freeman, W. T.; Wu, J.; Rus, D.; Matusik,\nW. ChainQueen: A real-time differentiable physical\nsimulator for soft robotics. In: Proceedings of the\nInternational Conference on Robotics and Automation,\n6265–6271, 2019.\n\n[26] Hu, Y. M.; Anderson, L.; Li, T. M.; Sun, Q.; Carr, N.;\nRagan-Kelley, J.; Durand, F. DiffTaichi: Differentiable\nprogramming for physical simulation. arXiv preprint\narXiv:1910.00935, 2019.\n\n[27] Liu, K. X.; Zeng, X. Y.; Bruniaux, P.; Tao, X. Y.; Yao,\nX. F.; Li, V.; Wang, J. 3D interactive garment pattern-\nmaking technology. Computer-Aided Design Vol. 104,\n113–124, 2018.\n\n[28] Huang, P.; Yao, J.; Zhao, H. Automatic realistic\n3D garment generation based on two images. In:\nProceedings of the International Conference on Virtual\nReality and Visualization, 250–257, 2016.\n\n[29] Wang, T. Y.; Ceylan, D.; Popović, J.; Mitra, N. J.\nLearning a shared shape space for multimodal garment\ndesign. ACM Transactions on Graphics Vol. 37, No. 6", "text": [ "", "TSINGHUA UNIVERSITY PRESS Springer", "TSINGHUA UNIVERSITY PRESS Springer", "(1,1 To2.1 Tesi b b b fi f1 fi St Regression Regression Regression View 1 1,1 Block 02,1 Block 3,1 Block (1,1) (2,1) C (3,1) - - 61,2 62,2 3,2 - b b b - f2 NX f2 f2 - Regression Regression Regression - View 2 01,2 Block 92,2 Block 3,2 Block (1,2) (2,2) C O - (3,2) Image Encoder - - 01,n 63.1 - b 'b b - fn fn fn - Regression Regression Regression View n 31,n Block 92,n Block 3.n Block (1,n) (2,n) C (3,n) - 62,1 63,1 * b b b Stage 1 Stage 2 Stage 3 Multi-View Multi-Stage Regression Network", "Adreamstime dreamsbr (a) Input image (b) Recovered body", "= 1 2 hn-1 2n ( c ) , 0 2 ,.... x 2 ) Tu2 Tun CNN CNN CNN Classifier Material type LSTM cell Concatenate op T1 T2 Lrgb Tn rgb -rgb", "F-1 F-25 F-38 F-105 F-1 F-25 F-38 F-105 F-1 F-2 F-3 F-4 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 F-1 F-2 F-3 F-4 0.1 O", "", "New Observation Control Simulation Observations Signals Time Collision Results 00000000 0000 000000000 000000000 000000000 Integration Response Loss State: velocity, position, material, force ... Trainable Network Layers Differentiable Simulation Layer", "", "" ], "layoutText": [ "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}", "{\"language\":\"en\",\"text\":\"TSINGHUA UNIVERSITY PRESS Springer\",\"lines\":[{\"boundingBox\":[{\"x\":1281,\"y\":5},{\"x\":1526,\"y\":6},{\"x\":1526,\"y\":41},{\"x\":1281,\"y\":41}],\"text\":\"TSINGHUA\"},{\"boundingBox\":[{\"x\":1288,\"y\":40},{\"x\":1522,\"y\":40},{\"x\":1522,\"y\":62},{\"x\":1288,\"y\":61}],\"text\":\"UNIVERSITY PRESS\"},{\"boundingBox\":[{\"x\":1603,\"y\":14},{\"x\":1771,\"y\":15},{\"x\":1770,\"y\":58},{\"x\":1603,\"y\":56}],\"text\":\"Springer\"}],\"words\":[{\"boundingBox\":[{\"x\":1284,\"y\":6},{\"x\":1525,\"y\":6},{\"x\":1525,\"y\":41},{\"x\":1285,\"y\":41}],\"text\":\"TSINGHUA\"},{\"boundingBox\":[{\"x\":1288,\"y\":40},{\"x\":1436,\"y\":40},{\"x\":1436,\"y\":62},{\"x\":1288,\"y\":62}],\"text\":\"UNIVERSITY\"},{\"boundingBox\":[{\"x\":1440,\"y\":40},{\"x\":1522,\"y\":41},{\"x\":1522,\"y\":62},{\"x\":1440,\"y\":62}],\"text\":\"PRESS\"},{\"boundingBox\":[{\"x\":1605,\"y\":14},{\"x\":1770,\"y\":16},{\"x\":1770,\"y\":59},{\"x\":1604,\"y\":58}],\"text\":\"Springer\"}]}", "{\"language\":\"en\",\"text\":\"TSINGHUA UNIVERSITY PRESS Springer\",\"lines\":[{\"boundingBox\":[{\"x\":73,\"y\":9},{\"x\":308,\"y\":9},{\"x\":308,\"y\":37},{\"x\":73,\"y\":38}],\"text\":\"TSINGHUA\"},{\"boundingBox\":[{\"x\":79,\"y\":40},{\"x\":312,\"y\":40},{\"x\":312,\"y\":60},{\"x\":79,\"y\":60}],\"text\":\"UNIVERSITY PRESS\"},{\"boundingBox\":[{\"x\":388,\"y\":13},{\"x\":567,\"y\":15},{\"x\":566,\"y\":57},{\"x\":387,\"y\":55}],\"text\":\"Springer\"}],\"words\":[{\"boundingBox\":[{\"x\":75,\"y\":10},{\"x\":308,\"y\":10},{\"x\":308,\"y\":38},{\"x\":76,\"y\":38}],\"text\":\"TSINGHUA\"},{\"boundingBox\":[{\"x\":80,\"y\":41},{\"x\":225,\"y\":41},{\"x\":224,\"y\":61},{\"x\":80,\"y\":60}],\"text\":\"UNIVERSITY\"},{\"boundingBox\":[{\"x\":228,\"y\":41},{\"x\":311,\"y\":41},{\"x\":311,\"y\":61},{\"x\":228,\"y\":61}],\"text\":\"PRESS\"},{\"boundingBox\":[{\"x\":388,\"y\":14},{\"x\":566,\"y\":16},{\"x\":567,\"y\":57},{\"x\":388,\"y\":55}],\"text\":\"Springer\"}]}", "{\"language\":\"en\",\"text\":\"(1,1 To2.1 Tesi b b b fi f1 fi St Regression Regression Regression View 1 1,1 Block 02,1 Block 3,1 Block (1,1) (2,1) C (3,1) - - 61,2 62,2 3,2 - b b b - f2 NX f2 f2 - Regression Regression Regression - View 2 01,2 Block 92,2 Block 3,2 Block (1,2) (2,2) C O - (3,2) Image Encoder - - 01,n 63.1 - b 'b b - fn fn fn - Regression Regression Regression View n 31,n Block 92,n Block 3.n Block (1,n) (2,n) C (3,n) - 62,1 63,1 * b b b Stage 1 Stage 2 Stage 3 Multi-View Multi-Stage Regression Network\",\"lines\":[{\"boundingBox\":[{\"x\":920,\"y\":25},{\"x\":971,\"y\":17},{\"x\":972,\"y\":37},{\"x\":921,\"y\":46}],\"text\":\"(1,1\"},{\"boundingBox\":[{\"x\":1226,\"y\":8},{\"x\":1302,\"y\":9},{\"x\":1304,\"y\":42},{\"x\":1228,\"y\":53}],\"text\":\"To2.1\"},{\"boundingBox\":[{\"x\":1551,\"y\":6},{\"x\":1633,\"y\":7},{\"x\":1635,\"y\":44},{\"x\":1555,\"y\":56}],\"text\":\"Tesi\"},{\"boundingBox\":[{\"x\":942,\"y\":43},{\"x\":956,\"y\":44},{\"x\":956,\"y\":59},{\"x\":941,\"y\":58}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1265,\"y\":41},{\"x\":1284,\"y\":40},{\"x\":1285,\"y\":60},{\"x\":1266,\"y\":60}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1594,\"y\":40},{\"x\":1613,\"y\":40},{\"x\":1613,\"y\":61},{\"x\":1594,\"y\":61}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":555,\"y\":69},{\"x\":581,\"y\":69},{\"x\":580,\"y\":97},{\"x\":553,\"y\":97}],\"text\":\"fi\"},{\"boundingBox\":[{\"x\":776,\"y\":61},{\"x\":803,\"y\":63},{\"x\":800,\"y\":93},{\"x\":773,\"y\":91}],\"text\":\"f1\"},{\"boundingBox\":[{\"x\":1100,\"y\":60},{\"x\":1129,\"y\":62},{\"x\":1127,\"y\":95},{\"x\":1098,\"y\":93}],\"text\":\"fi\"},{\"boundingBox\":[{\"x\":1428,\"y\":94},{\"x\":1430,\"y\":62},{\"x\":1450,\"y\":61},{\"x\":1448,\"y\":93}],\"text\":\"St\"},{\"boundingBox\":[{\"x\":837,\"y\":95},{\"x\":992,\"y\":95},{\"x\":992,\"y\":125},{\"x\":837,\"y\":125}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":1164,\"y\":95},{\"x\":1317,\"y\":95},{\"x\":1317,\"y\":125},{\"x\":1164,\"y\":125}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":1489,\"y\":95},{\"x\":1644,\"y\":94},{\"x\":1644,\"y\":125},{\"x\":1490,\"y\":126}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":613,\"y\":123},{\"x\":818,\"y\":130},{\"x\":816,\"y\":164},{\"x\":613,\"y\":158}],\"text\":\"View 1 1,1\"},{\"boundingBox\":[{\"x\":876,\"y\":130},{\"x\":953,\"y\":131},{\"x\":953,\"y\":158},{\"x\":876,\"y\":158}],\"text\":\"Block\"},{\"boundingBox\":[{\"x\":1049,\"y\":142},{\"x\":1102,\"y\":138},{\"x\":1103,\"y\":165},{\"x\":1051,\"y\":174}],\"text\":\"02,1\"},{\"boundingBox\":[{\"x\":1200,\"y\":130},{\"x\":1281,\"y\":131},{\"x\":1281,\"y\":158},{\"x\":1200,\"y\":158}],\"text\":\"Block\"},{\"boundingBox\":[{\"x\":1401,\"y\":139},{\"x\":1428,\"y\":138},{\"x\":1429,\"y\":158},{\"x\":1402,\"y\":158}],\"text\":\"3,1\"},{\"boundingBox\":[{\"x\":1525,\"y\":130},{\"x\":1606,\"y\":131},{\"x\":1607,\"y\":158},{\"x\":1525,\"y\":158}],\"text\":\"Block\"},{\"boundingBox\":[{\"x\":886,\"y\":167},{\"x\":944,\"y\":167},{\"x\":945,\"y\":196},{\"x\":885,\"y\":198}],\"text\":\"(1,1)\"},{\"boundingBox\":[{\"x\":1208,\"y\":167},{\"x\":1268,\"y\":167},{\"x\":1268,\"y\":197},{\"x\":1209,\"y\":197}],\"text\":\"(2,1)\"},{\"boundingBox\":[{\"x\":1393,\"y\":158},{\"x\":1419,\"y\":155},{\"x\":1419,\"y\":176},{\"x\":1394,\"y\":179}],\"text\":\"C\"},{\"boundingBox\":[{\"x\":1536,\"y\":168},{\"x\":1597,\"y\":166},{\"x\":1598,\"y\":197},{\"x\":1536,\"y\":197}],\"text\":\"(3,1)\"},{\"boundingBox\":[{\"x\":609,\"y\":249},{\"x\":609,\"y\":192},{\"x\":630,\"y\":193},{\"x\":629,\"y\":250}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":1775,\"y\":271},{\"x\":1773,\"y\":223},{\"x\":1793,\"y\":222},{\"x\":1794,\"y\":271}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":916,\"y\":265},{\"x\":973,\"y\":260},{\"x\":974,\"y\":285},{\"x\":918,\"y\":293}],\"text\":\"61,2\"},{\"boundingBox\":[{\"x\":1241,\"y\":265},{\"x\":1300,\"y\":260},{\"x\":1303,\"y\":286},{\"x\":1244,\"y\":296}],\"text\":\"62,2\"},{\"boundingBox\":[{\"x\":1597,\"y\":264},{\"x\":1628,\"y\":262},{\"x\":1629,\"y\":282},{\"x\":1599,\"y\":284}],\"text\":\"3,2\"},{\"boundingBox\":[{\"x\":609,\"y\":320},{\"x\":607,\"y\":268},{\"x\":629,\"y\":268},{\"x\":629,\"y\":321}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":940,\"y\":289},{\"x\":958,\"y\":289},{\"x\":957,\"y\":304},{\"x\":939,\"y\":304}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1267,\"y\":287},{\"x\":1288,\"y\":286},{\"x\":1288,\"y\":305},{\"x\":1267,\"y\":306}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1599,\"y\":289},{\"x\":1612,\"y\":289},{\"x\":1612,\"y\":307},{\"x\":1599,\"y\":308}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1774,\"y\":352},{\"x\":1773,\"y\":298},{\"x\":1791,\"y\":297},{\"x\":1794,\"y\":353}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":552,\"y\":322},{\"x\":583,\"y\":322},{\"x\":582,\"y\":356},{\"x\":551,\"y\":356}],\"text\":\"f2\"},{\"boundingBox\":[{\"x\":783,\"y\":366},{\"x\":780,\"y\":340},{\"x\":798,\"y\":336},{\"x\":801,\"y\":362}],\"text\":\"NX\"},{\"boundingBox\":[{\"x\":1101,\"y\":334},{\"x\":1130,\"y\":337},{\"x\":1127,\"y\":367},{\"x\":1098,\"y\":364}],\"text\":\"f2\"},{\"boundingBox\":[{\"x\":1426,\"y\":334},{\"x\":1457,\"y\":335},{\"x\":1456,\"y\":368},{\"x\":1424,\"y\":366}],\"text\":\"f2\"},{\"boundingBox\":[{\"x\":605,\"y\":399},{\"x\":606,\"y\":351},{\"x\":627,\"y\":351},{\"x\":627,\"y\":400}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":836,\"y\":367},{\"x\":991,\"y\":368},{\"x\":991,\"y\":398},{\"x\":836,\"y\":397}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":1163,\"y\":367},{\"x\":1319,\"y\":368},{\"x\":1318,\"y\":397},{\"x\":1163,\"y\":396}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":1488,\"y\":368},{\"x\":1644,\"y\":368},{\"x\":1644,\"y\":398},{\"x\":1488,\"y\":398}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":1772,\"y\":430},{\"x\":1771,\"y\":377},{\"x\":1793,\"y\":377},{\"x\":1795,\"y\":431}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":638,\"y\":402},{\"x\":819,\"y\":410},{\"x\":818,\"y\":440},{\"x\":638,\"y\":430}],\"text\":\"View 2 01,2\"},{\"boundingBox\":[{\"x\":876,\"y\":403},{\"x\":954,\"y\":404},{\"x\":953,\"y\":430},{\"x\":876,\"y\":430}],\"text\":\"Block\"},{\"boundingBox\":[{\"x\":1053,\"y\":419},{\"x\":1104,\"y\":411},{\"x\":1106,\"y\":433},{\"x\":1055,\"y\":443}],\"text\":\"92,2\"},{\"boundingBox\":[{\"x\":1201,\"y\":403},{\"x\":1279,\"y\":403},{\"x\":1279,\"y\":430},{\"x\":1201,\"y\":430}],\"text\":\"Block\"},{\"boundingBox\":[{\"x\":1399,\"y\":412},{\"x\":1428,\"y\":412},{\"x\":1428,\"y\":435},{\"x\":1399,\"y\":435}],\"text\":\"3,2\"},{\"boundingBox\":[{\"x\":1526,\"y\":403},{\"x\":1606,\"y\":404},{\"x\":1606,\"y\":431},{\"x\":1526,\"y\":430}],\"text\":\"Block\"},{\"boundingBox\":[{\"x\":884,\"y\":439},{\"x\":943,\"y\":439},{\"x\":943,\"y\":471},{\"x\":883,\"y\":471}],\"text\":\"(1,2)\"},{\"boundingBox\":[{\"x\":1209,\"y\":439},{\"x\":1269,\"y\":439},{\"x\":1267,\"y\":471},{\"x\":1210,\"y\":471}],\"text\":\"(2,2)\"},{\"boundingBox\":[{\"x\":1401,\"y\":436},{\"x\":1413,\"y\":435},{\"x\":1413,\"y\":450},{\"x\":1401,\"y\":450}],\"text\":\"C\"},{\"boundingBox\":[{\"x\":1798,\"y\":422},{\"x\":1825,\"y\":423},{\"x\":1827,\"y\":448},{\"x\":1800,\"y\":447}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":609,\"y\":478},{\"x\":605,\"y\":433},{\"x\":627,\"y\":430},{\"x\":630,\"y\":480}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":1536,\"y\":439},{\"x\":1594,\"y\":439},{\"x\":1593,\"y\":471},{\"x\":1536,\"y\":471}],\"text\":\"(3,2)\"},{\"boundingBox\":[{\"x\":475,\"y\":359},{\"x\":477,\"y\":598},{\"x\":444,\"y\":598},{\"x\":442,\"y\":359}],\"text\":\"Image Encoder\"},{\"boundingBox\":[{\"x\":1774,\"y\":509},{\"x\":1773,\"y\":461},{\"x\":1792,\"y\":460},{\"x\":1793,\"y\":509}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":610,\"y\":561},{\"x\":608,\"y\":508},{\"x\":629,\"y\":507},{\"x\":631,\"y\":562}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":916,\"y\":540},{\"x\":971,\"y\":536},{\"x\":973,\"y\":561},{\"x\":917,\"y\":568}],\"text\":\"01,n\"},{\"boundingBox\":[{\"x\":1572,\"y\":539},{\"x\":1630,\"y\":534},{\"x\":1632,\"y\":561},{\"x\":1575,\"y\":568}],\"text\":\"63.1\"},{\"boundingBox\":[{\"x\":1774,\"y\":584},{\"x\":1772,\"y\":536},{\"x\":1792,\"y\":536},{\"x\":1793,\"y\":584}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":939,\"y\":562},{\"x\":958,\"y\":561},{\"x\":958,\"y\":578},{\"x\":938,\"y\":579}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1265,\"y\":560},{\"x\":1289,\"y\":559},{\"x\":1290,\"y\":579},{\"x\":1265,\"y\":580}],\"text\":\"'b\"},{\"boundingBox\":[{\"x\":1596,\"y\":562},{\"x\":1615,\"y\":562},{\"x\":1614,\"y\":579},{\"x\":1595,\"y\":579}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":607,\"y\":639},{\"x\":608,\"y\":589},{\"x\":630,\"y\":588},{\"x\":632,\"y\":641}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":777,\"y\":608},{\"x\":803,\"y\":612},{\"x\":799,\"y\":642},{\"x\":773,\"y\":638}],\"text\":\"fn\"},{\"boundingBox\":[{\"x\":1100,\"y\":609},{\"x\":1130,\"y\":610},{\"x\":1127,\"y\":642},{\"x\":1098,\"y\":640}],\"text\":\"fn\"},{\"boundingBox\":[{\"x\":1427,\"y\":609},{\"x\":1455,\"y\":611},{\"x\":1453,\"y\":640},{\"x\":1424,\"y\":639}],\"text\":\"fn\"},{\"boundingBox\":[{\"x\":1775,\"y\":660},{\"x\":1772,\"y\":615},{\"x\":1792,\"y\":614},{\"x\":1794,\"y\":660}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":836,\"y\":641},{\"x\":991,\"y\":642},{\"x\":991,\"y\":673},{\"x\":836,\"y\":672}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":1164,\"y\":642},{\"x\":1318,\"y\":642},{\"x\":1317,\"y\":672},{\"x\":1164,\"y\":671}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":1490,\"y\":642},{\"x\":1644,\"y\":642},{\"x\":1644,\"y\":673},{\"x\":1490,\"y\":673}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":610,\"y\":674},{\"x\":818,\"y\":682},{\"x\":817,\"y\":714},{\"x\":610,\"y\":708}],\"text\":\"View n 31,n\"},{\"boundingBox\":[{\"x\":876,\"y\":677},{\"x\":952,\"y\":677},{\"x\":952,\"y\":704},{\"x\":876,\"y\":704}],\"text\":\"Block\"},{\"boundingBox\":[{\"x\":1050,\"y\":694},{\"x\":1104,\"y\":688},{\"x\":1105,\"y\":707},{\"x\":1052,\"y\":718}],\"text\":\"92,n\"},{\"boundingBox\":[{\"x\":1200,\"y\":677},{\"x\":1280,\"y\":677},{\"x\":1279,\"y\":704},{\"x\":1200,\"y\":704}],\"text\":\"Block\"},{\"boundingBox\":[{\"x\":1399,\"y\":686},{\"x\":1430,\"y\":688},{\"x\":1430,\"y\":706},{\"x\":1400,\"y\":704}],\"text\":\"3.n\"},{\"boundingBox\":[{\"x\":1526,\"y\":677},{\"x\":1605,\"y\":677},{\"x\":1606,\"y\":705},{\"x\":1526,\"y\":704}],\"text\":\"Block\"},{\"boundingBox\":[{\"x\":885,\"y\":713},{\"x\":945,\"y\":714},{\"x\":944,\"y\":744},{\"x\":884,\"y\":745}],\"text\":\"(1,n)\"},{\"boundingBox\":[{\"x\":1208,\"y\":713},{\"x\":1270,\"y\":714},{\"x\":1270,\"y\":745},{\"x\":1208,\"y\":745}],\"text\":\"(2,n)\"},{\"boundingBox\":[{\"x\":1402,\"y\":709},{\"x\":1413,\"y\":709},{\"x\":1412,\"y\":723},{\"x\":1401,\"y\":723}],\"text\":\"C\"},{\"boundingBox\":[{\"x\":1537,\"y\":713},{\"x\":1598,\"y\":713},{\"x\":1596,\"y\":745},{\"x\":1536,\"y\":743}],\"text\":\"(3,n)\"},{\"boundingBox\":[{\"x\":613,\"y\":800},{\"x\":610,\"y\":746},{\"x\":631,\"y\":747},{\"x\":632,\"y\":798}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":918,\"y\":796},{\"x\":968,\"y\":790},{\"x\":971,\"y\":813},{\"x\":920,\"y\":822}],\"text\":\"62,1\"},{\"boundingBox\":[{\"x\":1240,\"y\":793},{\"x\":1297,\"y\":788},{\"x\":1300,\"y\":816},{\"x\":1245,\"y\":823}],\"text\":\"63,1\"},{\"boundingBox\":[{\"x\":1770,\"y\":786},{\"x\":1844,\"y\":782},{\"x\":1843,\"y\":807},{\"x\":1768,\"y\":817}],\"text\":\"*\"},{\"boundingBox\":[{\"x\":1822,\"y\":800},{\"x\":1841,\"y\":801},{\"x\":1839,\"y\":821},{\"x\":1820,\"y\":820}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":940,\"y\":817},{\"x\":958,\"y\":818},{\"x\":957,\"y\":834},{\"x\":939,\"y\":833}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1266,\"y\":815},{\"x\":1287,\"y\":815},{\"x\":1287,\"y\":835},{\"x\":1266,\"y\":835}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":855,\"y\":857},{\"x\":971,\"y\":858},{\"x\":971,\"y\":889},{\"x\":854,\"y\":887}],\"text\":\"Stage 1\"},{\"boundingBox\":[{\"x\":1179,\"y\":858},{\"x\":1301,\"y\":858},{\"x\":1301,\"y\":888},{\"x\":1179,\"y\":887}],\"text\":\"Stage 2\"},{\"boundingBox\":[{\"x\":1507,\"y\":857},{\"x\":1626,\"y\":857},{\"x\":1626,\"y\":889},{\"x\":1507,\"y\":888}],\"text\":\"Stage 3\"},{\"boundingBox\":[{\"x\":905,\"y\":892},{\"x\":1573,\"y\":892},{\"x\":1573,\"y\":926},{\"x\":905,\"y\":926}],\"text\":\"Multi-View Multi-Stage Regression Network\"}],\"words\":[{\"boundingBox\":[{\"x\":920,\"y\":24},{\"x\":970,\"y\":17},{\"x\":973,\"y\":36},{\"x\":921,\"y\":45}],\"text\":\"(1,1\"},{\"boundingBox\":[{\"x\":1226,\"y\":8},{\"x\":1300,\"y\":8},{\"x\":1303,\"y\":48},{\"x\":1228,\"y\":53}],\"text\":\"To2.1\"},{\"boundingBox\":[{\"x\":1554,\"y\":6},{\"x\":1630,\"y\":6},{\"x\":1634,\"y\":50},{\"x\":1557,\"y\":56}],\"text\":\"Tesi\"},{\"boundingBox\":[{\"x\":948,\"y\":43},{\"x\":957,\"y\":44},{\"x\":956,\"y\":59},{\"x\":947,\"y\":58}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1274,\"y\":40},{\"x\":1284,\"y\":40},{\"x\":1285,\"y\":59},{\"x\":1274,\"y\":60}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1601,\"y\":40},{\"x\":1612,\"y\":40},{\"x\":1612,\"y\":61},{\"x\":1601,\"y\":61}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":557,\"y\":69},{\"x\":581,\"y\":69},{\"x\":581,\"y\":97},{\"x\":557,\"y\":97}],\"text\":\"fi\"},{\"boundingBox\":[{\"x\":777,\"y\":61},{\"x\":803,\"y\":63},{\"x\":801,\"y\":93},{\"x\":775,\"y\":91}],\"text\":\"f1\"},{\"boundingBox\":[{\"x\":1100,\"y\":60},{\"x\":1129,\"y\":62},{\"x\":1126,\"y\":95},{\"x\":1098,\"y\":93}],\"text\":\"fi\"},{\"boundingBox\":[{\"x\":1429,\"y\":84},{\"x\":1430,\"y\":61},{\"x\":1449,\"y\":62},{\"x\":1448,\"y\":85}],\"text\":\"St\"},{\"boundingBox\":[{\"x\":837,\"y\":96},{\"x\":992,\"y\":96},{\"x\":992,\"y\":125},{\"x\":838,\"y\":126}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":1165,\"y\":96},{\"x\":1318,\"y\":96},{\"x\":1317,\"y\":125},{\"x\":1165,\"y\":125}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":1490,\"y\":96},{\"x\":1645,\"y\":95},{\"x\":1644,\"y\":125},{\"x\":1490,\"y\":126}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":644,\"y\":123},{\"x\":711,\"y\":126},{\"x\":712,\"y\":162},{\"x\":644,\"y\":160}],\"text\":\"View\"},{\"boundingBox\":[{\"x\":718,\"y\":126},{\"x\":753,\"y\":129},{\"x\":754,\"y\":162},{\"x\":719,\"y\":162}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":780,\"y\":132},{\"x\":815,\"y\":136},{\"x\":817,\"y\":164},{\"x\":782,\"y\":163}],\"text\":\"1,1\"},{\"boundingBox\":[{\"x\":876,\"y\":130},{\"x\":953,\"y\":130},{\"x\":952,\"y\":158},{\"x\":876,\"y\":157}],\"text\":\"Block\"},{\"boundingBox\":[{\"x\":1055,\"y\":141},{\"x\":1100,\"y\":138},{\"x\":1104,\"y\":167},{\"x\":1059,\"y\":173}],\"text\":\"02,1\"},{\"boundingBox\":[{\"x\":1201,\"y\":131},{\"x\":1281,\"y\":131},{\"x\":1280,\"y\":159},{\"x\":1200,\"y\":158}],\"text\":\"Block\"},{\"boundingBox\":[{\"x\":1401,\"y\":138},{\"x\":1429,\"y\":138},{\"x\":1429,\"y\":157},{\"x\":1401,\"y\":158}],\"text\":\"3,1\"},{\"boundingBox\":[{\"x\":1525,\"y\":130},{\"x\":1605,\"y\":130},{\"x\":1605,\"y\":158},{\"x\":1525,\"y\":157}],\"text\":\"Block\"},{\"boundingBox\":[{\"x\":885,\"y\":167},{\"x\":944,\"y\":167},{\"x\":945,\"y\":197},{\"x\":885,\"y\":198}],\"text\":\"(1,1)\"},{\"boundingBox\":[{\"x\":1208,\"y\":167},{\"x\":1268,\"y\":167},{\"x\":1268,\"y\":197},{\"x\":1208,\"y\":197}],\"text\":\"(2,1)\"},{\"boundingBox\":[{\"x\":1401,\"y\":157},{\"x\":1418,\"y\":155},{\"x\":1420,\"y\":176},{\"x\":1404,\"y\":178}],\"text\":\"C\"},{\"boundingBox\":[{\"x\":1536,\"y\":167},{\"x\":1597,\"y\":166},{\"x\":1598,\"y\":197},{\"x\":1536,\"y\":198}],\"text\":\"(3,1)\"},{\"boundingBox\":[{\"x\":609,\"y\":242},{\"x\":609,\"y\":221},{\"x\":630,\"y\":221},{\"x\":630,\"y\":242}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":1774,\"y\":270},{\"x\":1773,\"y\":250},{\"x\":1793,\"y\":250},{\"x\":1794,\"y\":270}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":926,\"y\":264},{\"x\":972,\"y\":260},{\"x\":975,\"y\":286},{\"x\":929,\"y\":292}],\"text\":\"61,2\"},{\"boundingBox\":[{\"x\":1254,\"y\":263},{\"x\":1299,\"y\":260},{\"x\":1303,\"y\":288},{\"x\":1258,\"y\":294}],\"text\":\"62,2\"},{\"boundingBox\":[{\"x\":1597,\"y\":264},{\"x\":1627,\"y\":262},{\"x\":1629,\"y\":282},{\"x\":1598,\"y\":284}],\"text\":\"3,2\"},{\"boundingBox\":[{\"x\":608,\"y\":321},{\"x\":608,\"y\":299},{\"x\":630,\"y\":299},{\"x\":630,\"y\":321}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":947,\"y\":289},{\"x\":958,\"y\":289},{\"x\":958,\"y\":304},{\"x\":947,\"y\":304}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1274,\"y\":287},{\"x\":1286,\"y\":286},{\"x\":1287,\"y\":305},{\"x\":1275,\"y\":306}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1602,\"y\":289},{\"x\":1611,\"y\":289},{\"x\":1612,\"y\":308},{\"x\":1603,\"y\":308}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1774,\"y\":350},{\"x\":1773,\"y\":330},{\"x\":1793,\"y\":329},{\"x\":1794,\"y\":349}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":553,\"y\":322},{\"x\":583,\"y\":322},{\"x\":583,\"y\":356},{\"x\":553,\"y\":356}],\"text\":\"f2\"},{\"boundingBox\":[{\"x\":782,\"y\":359},{\"x\":780,\"y\":339},{\"x\":798,\"y\":337},{\"x\":800,\"y\":357}],\"text\":\"NX\"},{\"boundingBox\":[{\"x\":1103,\"y\":334},{\"x\":1129,\"y\":337},{\"x\":1126,\"y\":367},{\"x\":1100,\"y\":364}],\"text\":\"f2\"},{\"boundingBox\":[{\"x\":1425,\"y\":334},{\"x\":1456,\"y\":335},{\"x\":1454,\"y\":368},{\"x\":1424,\"y\":366}],\"text\":\"f2\"},{\"boundingBox\":[{\"x\":605,\"y\":400},{\"x\":605,\"y\":379},{\"x\":627,\"y\":380},{\"x\":626,\"y\":400}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":837,\"y\":368},{\"x\":991,\"y\":370},{\"x\":991,\"y\":398},{\"x\":836,\"y\":398}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":1164,\"y\":368},{\"x\":1319,\"y\":370},{\"x\":1318,\"y\":397},{\"x\":1163,\"y\":397}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":1488,\"y\":368},{\"x\":1644,\"y\":369},{\"x\":1644,\"y\":398},{\"x\":1489,\"y\":398}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":1772,\"y\":430},{\"x\":1771,\"y\":407},{\"x\":1794,\"y\":407},{\"x\":1795,\"y\":430}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":638,\"y\":403},{\"x\":710,\"y\":404},{\"x\":711,\"y\":435},{\"x\":639,\"y\":431}],\"text\":\"View\"},{\"boundingBox\":[{\"x\":717,\"y\":404},{\"x\":745,\"y\":406},{\"x\":745,\"y\":437},{\"x\":718,\"y\":435}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":761,\"y\":407},{\"x\":816,\"y\":411},{\"x\":817,\"y\":441},{\"x\":762,\"y\":438}],\"text\":\"01,2\"},{\"boundingBox\":[{\"x\":876,\"y\":403},{\"x\":953,\"y\":403},{\"x\":953,\"y\":430},{\"x\":876,\"y\":429}],\"text\":\"Block\"},{\"boundingBox\":[{\"x\":1058,\"y\":417},{\"x\":1102,\"y\":411},{\"x\":1107,\"y\":433},{\"x\":1062,\"y\":441}],\"text\":\"92,2\"},{\"boundingBox\":[{\"x\":1201,\"y\":403},{\"x\":1278,\"y\":403},{\"x\":1278,\"y\":430},{\"x\":1201,\"y\":430}],\"text\":\"Block\"},{\"boundingBox\":[{\"x\":1399,\"y\":412},{\"x\":1427,\"y\":412},{\"x\":1427,\"y\":435},{\"x\":1399,\"y\":435}],\"text\":\"3,2\"},{\"boundingBox\":[{\"x\":1526,\"y\":403},{\"x\":1605,\"y\":404},{\"x\":1605,\"y\":431},{\"x\":1526,\"y\":430}],\"text\":\"Block\"},{\"boundingBox\":[{\"x\":883,\"y\":439},{\"x\":943,\"y\":439},{\"x\":943,\"y\":471},{\"x\":883,\"y\":471}],\"text\":\"(1,2)\"},{\"boundingBox\":[{\"x\":1209,\"y\":439},{\"x\":1269,\"y\":439},{\"x\":1269,\"y\":471},{\"x\":1209,\"y\":471}],\"text\":\"(2,2)\"},{\"boundingBox\":[{\"x\":1404,\"y\":435},{\"x\":1412,\"y\":435},{\"x\":1413,\"y\":449},{\"x\":1405,\"y\":450}],\"text\":\"C\"},{\"boundingBox\":[{\"x\":1800,\"y\":422},{\"x\":1825,\"y\":423},{\"x\":1824,\"y\":448},{\"x\":1799,\"y\":447}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":608,\"y\":481},{\"x\":606,\"y\":460},{\"x\":628,\"y\":458},{\"x\":630,\"y\":480}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":1536,\"y\":439},{\"x\":1594,\"y\":439},{\"x\":1594,\"y\":471},{\"x\":1536,\"y\":471}],\"text\":\"(3,2)\"},{\"boundingBox\":[{\"x\":476,\"y\":361},{\"x\":477,\"y\":456},{\"x\":444,\"y\":456},{\"x\":443,\"y\":360}],\"text\":\"Image\"},{\"boundingBox\":[{\"x\":477,\"y\":463},{\"x\":478,\"y\":595},{\"x\":445,\"y\":597},{\"x\":444,\"y\":463}],\"text\":\"Encoder\"},{\"boundingBox\":[{\"x\":1774,\"y\":508},{\"x\":1774,\"y\":489},{\"x\":1793,\"y\":489},{\"x\":1793,\"y\":508}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":610,\"y\":560},{\"x\":609,\"y\":539},{\"x\":630,\"y\":538},{\"x\":631,\"y\":559}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":918,\"y\":540},{\"x\":970,\"y\":536},{\"x\":972,\"y\":562},{\"x\":920,\"y\":567}],\"text\":\"01,n\"},{\"boundingBox\":[{\"x\":1583,\"y\":538},{\"x\":1630,\"y\":534},{\"x\":1633,\"y\":562},{\"x\":1586,\"y\":567}],\"text\":\"63.1\"},{\"boundingBox\":[{\"x\":1773,\"y\":585},{\"x\":1772,\"y\":567},{\"x\":1792,\"y\":567},{\"x\":1793,\"y\":584}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":946,\"y\":562},{\"x\":956,\"y\":561},{\"x\":957,\"y\":578},{\"x\":947,\"y\":579}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1265,\"y\":559},{\"x\":1288,\"y\":559},{\"x\":1289,\"y\":579},{\"x\":1265,\"y\":580}],\"text\":\"'b\"},{\"boundingBox\":[{\"x\":1603,\"y\":562},{\"x\":1614,\"y\":562},{\"x\":1614,\"y\":579},{\"x\":1603,\"y\":579}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":607,\"y\":640},{\"x\":607,\"y\":615},{\"x\":632,\"y\":614},{\"x\":632,\"y\":639}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":779,\"y\":608},{\"x\":803,\"y\":612},{\"x\":798,\"y\":641},{\"x\":774,\"y\":638}],\"text\":\"fn\"},{\"boundingBox\":[{\"x\":1101,\"y\":609},{\"x\":1129,\"y\":610},{\"x\":1127,\"y\":642},{\"x\":1100,\"y\":640}],\"text\":\"fn\"},{\"boundingBox\":[{\"x\":1427,\"y\":609},{\"x\":1455,\"y\":610},{\"x\":1453,\"y\":640},{\"x\":1425,\"y\":638}],\"text\":\"fn\"},{\"boundingBox\":[{\"x\":1774,\"y\":661},{\"x\":1773,\"y\":645},{\"x\":1793,\"y\":644},{\"x\":1794,\"y\":660}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":837,\"y\":642},{\"x\":990,\"y\":644},{\"x\":990,\"y\":672},{\"x\":836,\"y\":673}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":1165,\"y\":642},{\"x\":1317,\"y\":645},{\"x\":1316,\"y\":671},{\"x\":1165,\"y\":672}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":1490,\"y\":642},{\"x\":1645,\"y\":643},{\"x\":1644,\"y\":672},{\"x\":1490,\"y\":673}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":636,\"y\":675},{\"x\":707,\"y\":678},{\"x\":708,\"y\":711},{\"x\":637,\"y\":710}],\"text\":\"View\"},{\"boundingBox\":[{\"x\":714,\"y\":678},{\"x\":749,\"y\":680},{\"x\":750,\"y\":712},{\"x\":715,\"y\":711}],\"text\":\"n\"},{\"boundingBox\":[{\"x\":760,\"y\":681},{\"x\":815,\"y\":686},{\"x\":817,\"y\":714},{\"x\":761,\"y\":712}],\"text\":\"31,n\"},{\"boundingBox\":[{\"x\":876,\"y\":677},{\"x\":952,\"y\":677},{\"x\":952,\"y\":704},{\"x\":876,\"y\":704}],\"text\":\"Block\"},{\"boundingBox\":[{\"x\":1059,\"y\":691},{\"x\":1102,\"y\":688},{\"x\":1106,\"y\":709},{\"x\":1063,\"y\":716}],\"text\":\"92,n\"},{\"boundingBox\":[{\"x\":1200,\"y\":677},{\"x\":1279,\"y\":677},{\"x\":1279,\"y\":704},{\"x\":1200,\"y\":704}],\"text\":\"Block\"},{\"boundingBox\":[{\"x\":1404,\"y\":686},{\"x\":1430,\"y\":688},{\"x\":1429,\"y\":706},{\"x\":1403,\"y\":704}],\"text\":\"3.n\"},{\"boundingBox\":[{\"x\":1526,\"y\":677},{\"x\":1604,\"y\":677},{\"x\":1604,\"y\":705},{\"x\":1526,\"y\":704}],\"text\":\"Block\"},{\"boundingBox\":[{\"x\":884,\"y\":713},{\"x\":944,\"y\":713},{\"x\":944,\"y\":745},{\"x\":884,\"y\":745}],\"text\":\"(1,n)\"},{\"boundingBox\":[{\"x\":1208,\"y\":713},{\"x\":1270,\"y\":713},{\"x\":1270,\"y\":745},{\"x\":1208,\"y\":744}],\"text\":\"(2,n)\"},{\"boundingBox\":[{\"x\":1405,\"y\":709},{\"x\":1412,\"y\":709},{\"x\":1412,\"y\":723},{\"x\":1405,\"y\":723}],\"text\":\"C\"},{\"boundingBox\":[{\"x\":1536,\"y\":713},{\"x\":1596,\"y\":713},{\"x\":1595,\"y\":745},{\"x\":1536,\"y\":744}],\"text\":\"(3,n)\"},{\"boundingBox\":[{\"x\":612,\"y\":798},{\"x\":611,\"y\":777},{\"x\":632,\"y\":776},{\"x\":633,\"y\":797}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":926,\"y\":795},{\"x\":966,\"y\":790},{\"x\":970,\"y\":815},{\"x\":930,\"y\":820}],\"text\":\"62,1\"},{\"boundingBox\":[{\"x\":1254,\"y\":792},{\"x\":1295,\"y\":788},{\"x\":1299,\"y\":817},{\"x\":1257,\"y\":821}],\"text\":\"63,1\"},{\"boundingBox\":[{\"x\":1829,\"y\":782},{\"x\":1843,\"y\":782},{\"x\":1845,\"y\":809},{\"x\":1832,\"y\":810}],\"text\":\"*\"},{\"boundingBox\":[{\"x\":1830,\"y\":800},{\"x\":1841,\"y\":801},{\"x\":1840,\"y\":821},{\"x\":1829,\"y\":820}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":948,\"y\":817},{\"x\":957,\"y\":818},{\"x\":956,\"y\":834},{\"x\":947,\"y\":833}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1273,\"y\":815},{\"x\":1286,\"y\":815},{\"x\":1286,\"y\":835},{\"x\":1273,\"y\":835}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":855,\"y\":858},{\"x\":949,\"y\":859},{\"x\":949,\"y\":889},{\"x\":855,\"y\":886}],\"text\":\"Stage\"},{\"boundingBox\":[{\"x\":955,\"y\":859},{\"x\":972,\"y\":859},{\"x\":972,\"y\":888},{\"x\":955,\"y\":889}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":1181,\"y\":858},{\"x\":1273,\"y\":858},{\"x\":1273,\"y\":888},{\"x\":1180,\"y\":887}],\"text\":\"Stage\"},{\"boundingBox\":[{\"x\":1279,\"y\":858},{\"x\":1301,\"y\":859},{\"x\":1301,\"y\":888},{\"x\":1278,\"y\":888}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":1508,\"y\":858},{\"x\":1597,\"y\":858},{\"x\":1597,\"y\":889},{\"x\":1508,\"y\":888}],\"text\":\"Stage\"},{\"boundingBox\":[{\"x\":1603,\"y\":858},{\"x\":1625,\"y\":859},{\"x\":1625,\"y\":889},{\"x\":1603,\"y\":889}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":906,\"y\":894},{\"x\":1068,\"y\":894},{\"x\":1068,\"y\":926},{\"x\":906,\"y\":923}],\"text\":\"Multi-View\"},{\"boundingBox\":[{\"x\":1074,\"y\":894},{\"x\":1253,\"y\":893},{\"x\":1253,\"y\":927},{\"x\":1074,\"y\":926}],\"text\":\"Multi-Stage\"},{\"boundingBox\":[{\"x\":1259,\"y\":893},{\"x\":1434,\"y\":893},{\"x\":1434,\"y\":926},{\"x\":1259,\"y\":927}],\"text\":\"Regression\"},{\"boundingBox\":[{\"x\":1440,\"y\":893},{\"x\":1573,\"y\":893},{\"x\":1573,\"y\":924},{\"x\":1440,\"y\":926}],\"text\":\"Network\"}]}", "{\"language\":\"en\",\"text\":\"Adreamstime dreamsbr (a) Input image (b) Recovered body\",\"lines\":[{\"boundingBox\":[{\"x\":108,\"y\":207},{\"x\":177,\"y\":138},{\"x\":187,\"y\":148},{\"x\":117,\"y\":217}],\"text\":\"Adreamstime\"},{\"boundingBox\":[{\"x\":113,\"y\":521},{\"x\":166,\"y\":467},{\"x\":176,\"y\":476},{\"x\":123,\"y\":531}],\"text\":\"dreamsbr\"},{\"boundingBox\":[{\"x\":9,\"y\":594},{\"x\":242,\"y\":594},{\"x\":242,\"y\":626},{\"x\":9,\"y\":626}],\"text\":\"(a) Input image\"},{\"boundingBox\":[{\"x\":383,\"y\":594},{\"x\":668,\"y\":594},{\"x\":668,\"y\":626},{\"x\":383,\"y\":626}],\"text\":\"(b) Recovered body\"}],\"words\":[{\"boundingBox\":[{\"x\":108,\"y\":206},{\"x\":178,\"y\":139},{\"x\":187,\"y\":148},{\"x\":118,\"y\":218}],\"text\":\"Adreamstime\"},{\"boundingBox\":[{\"x\":114,\"y\":522},{\"x\":166,\"y\":467},{\"x\":176,\"y\":476},{\"x\":123,\"y\":531}],\"text\":\"dreamsbr\"},{\"boundingBox\":[{\"x\":11,\"y\":594},{\"x\":52,\"y\":594},{\"x\":51,\"y\":627},{\"x\":10,\"y\":627}],\"text\":\"(a)\"},{\"boundingBox\":[{\"x\":58,\"y\":594},{\"x\":140,\"y\":595},{\"x\":140,\"y\":626},{\"x\":58,\"y\":626}],\"text\":\"Input\"},{\"boundingBox\":[{\"x\":147,\"y\":595},{\"x\":242,\"y\":596},{\"x\":242,\"y\":625},{\"x\":147,\"y\":626}],\"text\":\"image\"},{\"boundingBox\":[{\"x\":384,\"y\":595},{\"x\":423,\"y\":595},{\"x\":422,\"y\":627},{\"x\":384,\"y\":627}],\"text\":\"(b)\"},{\"boundingBox\":[{\"x\":429,\"y\":595},{\"x\":588,\"y\":595},{\"x\":587,\"y\":627},{\"x\":429,\"y\":627}],\"text\":\"Recovered\"},{\"boundingBox\":[{\"x\":594,\"y\":595},{\"x\":667,\"y\":595},{\"x\":667,\"y\":627},{\"x\":594,\"y\":627}],\"text\":\"body\"}]}", "{\"language\":\"en\",\"text\":\"= 1 2 hn-1 2n ( c ) , 0 2 ,.... x 2 ) Tu2 Tun CNN CNN CNN Classifier Material type LSTM cell Concatenate op T1 T2 Lrgb Tn rgb -rgb\",\"lines\":[{\"boundingBox\":[{\"x\":556,\"y\":34},{\"x\":557,\"y\":8},{\"x\":572,\"y\":9},{\"x\":571,\"y\":35}],\"text\":\"=\"},{\"boundingBox\":[{\"x\":123,\"y\":81},{\"x\":139,\"y\":81},{\"x\":139,\"y\":98},{\"x\":123,\"y\":98}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":340,\"y\":81},{\"x\":358,\"y\":81},{\"x\":359,\"y\":101},{\"x\":341,\"y\":102}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":470,\"y\":108},{\"x\":608,\"y\":83},{\"x\":613,\"y\":106},{\"x\":472,\"y\":133}],\"text\":\"hn-1 2n\"},{\"boundingBox\":[{\"x\":651,\"y\":87},{\"x\":862,\"y\":86},{\"x\":862,\"y\":117},{\"x\":651,\"y\":119}],\"text\":\"( c ) , 0 2 ,.... x 2 )\"},{\"boundingBox\":[{\"x\":295,\"y\":187},{\"x\":340,\"y\":182},{\"x\":341,\"y\":214},{\"x\":298,\"y\":218}],\"text\":\"Tu2\"},{\"boundingBox\":[{\"x\":552,\"y\":195},{\"x\":608,\"y\":181},{\"x\":611,\"y\":199},{\"x\":556,\"y\":219}],\"text\":\"Tun\"},{\"boundingBox\":[{\"x\":59,\"y\":233},{\"x\":117,\"y\":233},{\"x\":116,\"y\":255},{\"x\":59,\"y\":255}],\"text\":\"CNN\"},{\"boundingBox\":[{\"x\":277,\"y\":234},{\"x\":333,\"y\":234},{\"x\":333,\"y\":255},{\"x\":277,\"y\":256}],\"text\":\"CNN\"},{\"boundingBox\":[{\"x\":536,\"y\":233},{\"x\":592,\"y\":234},{\"x\":592,\"y\":255},{\"x\":536,\"y\":255}],\"text\":\"CNN\"},{\"boundingBox\":[{\"x\":758,\"y\":169},{\"x\":760,\"y\":305},{\"x\":731,\"y\":305},{\"x\":729,\"y\":169}],\"text\":\"Classifier\"},{\"boundingBox\":[{\"x\":843,\"y\":222},{\"x\":969,\"y\":223},{\"x\":968,\"y\":244},{\"x\":843,\"y\":243}],\"text\":\"Material type\"},{\"boundingBox\":[{\"x\":794,\"y\":321},{\"x\":916,\"y\":321},{\"x\":916,\"y\":347},{\"x\":794,\"y\":347}],\"text\":\"LSTM cell\"},{\"boundingBox\":[{\"x\":794,\"y\":386},{\"x\":985,\"y\":388},{\"x\":985,\"y\":413},{\"x\":794,\"y\":411}],\"text\":\"Concatenate op\"},{\"boundingBox\":[{\"x\":137,\"y\":417},{\"x\":159,\"y\":414},{\"x\":161,\"y\":431},{\"x\":138,\"y\":436}],\"text\":\"T1\"},{\"boundingBox\":[{\"x\":354,\"y\":418},{\"x\":377,\"y\":414},{\"x\":379,\"y\":431},{\"x\":356,\"y\":436}],\"text\":\"T2\"},{\"boundingBox\":[{\"x\":142,\"y\":430},{\"x\":177,\"y\":429},{\"x\":178,\"y\":445},{\"x\":143,\"y\":445}],\"text\":\"Lrgb\"},{\"boundingBox\":[{\"x\":615,\"y\":417},{\"x\":639,\"y\":415},{\"x\":640,\"y\":429},{\"x\":615,\"y\":433}],\"text\":\"Tn\"},{\"boundingBox\":[{\"x\":362,\"y\":431},{\"x\":395,\"y\":429},{\"x\":396,\"y\":445},{\"x\":363,\"y\":446}],\"text\":\"rgb\"},{\"boundingBox\":[{\"x\":622,\"y\":428},{\"x\":655,\"y\":428},{\"x\":656,\"y\":443},{\"x\":622,\"y\":443}],\"text\":\"-rgb\"}],\"words\":[{\"boundingBox\":[{\"x\":556,\"y\":35},{\"x\":556,\"y\":24},{\"x\":571,\"y\":25},{\"x\":571,\"y\":36}],\"text\":\"=\"},{\"boundingBox\":[{\"x\":125,\"y\":81},{\"x\":139,\"y\":81},{\"x\":139,\"y\":98},{\"x\":125,\"y\":98}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":342,\"y\":81},{\"x\":358,\"y\":81},{\"x\":358,\"y\":101},{\"x\":343,\"y\":102}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":476,\"y\":107},{\"x\":546,\"y\":97},{\"x\":549,\"y\":119},{\"x\":477,\"y\":132}],\"text\":\"hn-1\"},{\"boundingBox\":[{\"x\":576,\"y\":91},{\"x\":610,\"y\":83},{\"x\":613,\"y\":105},{\"x\":579,\"y\":113}],\"text\":\"2n\"},{\"boundingBox\":[{\"x\":652,\"y\":88},{\"x\":663,\"y\":88},{\"x\":667,\"y\":118},{\"x\":655,\"y\":117}],\"text\":\"(\"},{\"boundingBox\":[{\"x\":669,\"y\":88},{\"x\":675,\"y\":88},{\"x\":678,\"y\":118},{\"x\":672,\"y\":118}],\"text\":\"c\"},{\"boundingBox\":[{\"x\":681,\"y\":88},{\"x\":686,\"y\":89},{\"x\":690,\"y\":118},{\"x\":684,\"y\":118}],\"text\":\")\"},{\"boundingBox\":[{\"x\":692,\"y\":89},{\"x\":708,\"y\":89},{\"x\":711,\"y\":119},{\"x\":695,\"y\":119}],\"text\":\",\"},{\"boundingBox\":[{\"x\":714,\"y\":89},{\"x\":719,\"y\":89},{\"x\":722,\"y\":119},{\"x\":716,\"y\":119}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":725,\"y\":89},{\"x\":731,\"y\":89},{\"x\":734,\"y\":119},{\"x\":728,\"y\":119}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":737,\"y\":89},{\"x\":810,\"y\":88},{\"x\":812,\"y\":117},{\"x\":739,\"y\":119}],\"text\":\",....\"},{\"boundingBox\":[{\"x\":816,\"y\":87},{\"x\":826,\"y\":87},{\"x\":827,\"y\":117},{\"x\":818,\"y\":117}],\"text\":\"x\"},{\"boundingBox\":[{\"x\":831,\"y\":87},{\"x\":837,\"y\":87},{\"x\":839,\"y\":116},{\"x\":833,\"y\":116}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":843,\"y\":87},{\"x\":860,\"y\":86},{\"x\":862,\"y\":115},{\"x\":845,\"y\":116}],\"text\":\")\"},{\"boundingBox\":[{\"x\":295,\"y\":186},{\"x\":339,\"y\":182},{\"x\":342,\"y\":213},{\"x\":298,\"y\":218}],\"text\":\"Tu2\"},{\"boundingBox\":[{\"x\":556,\"y\":193},{\"x\":607,\"y\":181},{\"x\":612,\"y\":201},{\"x\":563,\"y\":216}],\"text\":\"Tun\"},{\"boundingBox\":[{\"x\":59,\"y\":233},{\"x\":116,\"y\":233},{\"x\":116,\"y\":255},{\"x\":59,\"y\":255}],\"text\":\"CNN\"},{\"boundingBox\":[{\"x\":277,\"y\":234},{\"x\":333,\"y\":234},{\"x\":333,\"y\":255},{\"x\":277,\"y\":256}],\"text\":\"CNN\"},{\"boundingBox\":[{\"x\":536,\"y\":233},{\"x\":592,\"y\":233},{\"x\":592,\"y\":255},{\"x\":536,\"y\":254}],\"text\":\"CNN\"},{\"boundingBox\":[{\"x\":759,\"y\":170},{\"x\":760,\"y\":305},{\"x\":732,\"y\":306},{\"x\":729,\"y\":170}],\"text\":\"Classifier\"},{\"boundingBox\":[{\"x\":843,\"y\":223},{\"x\":917,\"y\":223},{\"x\":917,\"y\":245},{\"x\":843,\"y\":244}],\"text\":\"Material\"},{\"boundingBox\":[{\"x\":921,\"y\":223},{\"x\":969,\"y\":224},{\"x\":968,\"y\":245},{\"x\":921,\"y\":245}],\"text\":\"type\"},{\"boundingBox\":[{\"x\":795,\"y\":322},{\"x\":865,\"y\":322},{\"x\":865,\"y\":348},{\"x\":795,\"y\":348}],\"text\":\"LSTM\"},{\"boundingBox\":[{\"x\":870,\"y\":322},{\"x\":917,\"y\":322},{\"x\":916,\"y\":348},{\"x\":870,\"y\":348}],\"text\":\"cell\"},{\"boundingBox\":[{\"x\":795,\"y\":387},{\"x\":949,\"y\":388},{\"x\":949,\"y\":414},{\"x\":796,\"y\":412}],\"text\":\"Concatenate\"},{\"boundingBox\":[{\"x\":954,\"y\":388},{\"x\":985,\"y\":389},{\"x\":985,\"y\":414},{\"x\":954,\"y\":414}],\"text\":\"op\"},{\"boundingBox\":[{\"x\":138,\"y\":417},{\"x\":158,\"y\":414},{\"x\":161,\"y\":431},{\"x\":141,\"y\":435}],\"text\":\"T1\"},{\"boundingBox\":[{\"x\":356,\"y\":418},{\"x\":376,\"y\":414},{\"x\":379,\"y\":431},{\"x\":359,\"y\":435}],\"text\":\"T2\"},{\"boundingBox\":[{\"x\":142,\"y\":429},{\"x\":177,\"y\":429},{\"x\":177,\"y\":444},{\"x\":142,\"y\":445}],\"text\":\"Lrgb\"},{\"boundingBox\":[{\"x\":619,\"y\":416},{\"x\":639,\"y\":415},{\"x\":641,\"y\":429},{\"x\":621,\"y\":431}],\"text\":\"Tn\"},{\"boundingBox\":[{\"x\":368,\"y\":430},{\"x\":394,\"y\":429},{\"x\":395,\"y\":444},{\"x\":368,\"y\":446}],\"text\":\"rgb\"},{\"boundingBox\":[{\"x\":622,\"y\":428},{\"x\":656,\"y\":428},{\"x\":656,\"y\":443},{\"x\":622,\"y\":443}],\"text\":\"-rgb\"}]}", "{\"language\":\"en\",\"text\":\"F-1 F-25 F-38 F-105 F-1 F-25 F-38 F-105 F-1 F-2 F-3 F-4 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 F-1 F-2 F-3 F-4 0.1 O\",\"lines\":[{\"boundingBox\":[{\"x\":5,\"y\":201},{\"x\":28,\"y\":200},{\"x\":28,\"y\":217},{\"x\":5,\"y\":218}],\"text\":\"F-1\"},{\"boundingBox\":[{\"x\":236,\"y\":200},{\"x\":272,\"y\":200},{\"x\":272,\"y\":217},{\"x\":236,\"y\":216}],\"text\":\"F-25\"},{\"boundingBox\":[{\"x\":472,\"y\":199},{\"x\":510,\"y\":199},{\"x\":511,\"y\":215},{\"x\":472,\"y\":216}],\"text\":\"F-38\"},{\"boundingBox\":[{\"x\":699,\"y\":200},{\"x\":748,\"y\":200},{\"x\":748,\"y\":217},{\"x\":699,\"y\":216}],\"text\":\"F-105\"},{\"boundingBox\":[{\"x\":6,\"y\":433},{\"x\":29,\"y\":432},{\"x\":29,\"y\":449},{\"x\":6,\"y\":450}],\"text\":\"F-1\"},{\"boundingBox\":[{\"x\":236,\"y\":433},{\"x\":271,\"y\":432},{\"x\":271,\"y\":448},{\"x\":236,\"y\":448}],\"text\":\"F-25\"},{\"boundingBox\":[{\"x\":469,\"y\":432},{\"x\":512,\"y\":431},{\"x\":512,\"y\":447},{\"x\":469,\"y\":448}],\"text\":\"F-38\"},{\"boundingBox\":[{\"x\":699,\"y\":432},{\"x\":748,\"y\":432},{\"x\":748,\"y\":448},{\"x\":700,\"y\":448}],\"text\":\"F-105\"},{\"boundingBox\":[{\"x\":5,\"y\":665},{\"x\":29,\"y\":664},{\"x\":28,\"y\":681},{\"x\":5,\"y\":682}],\"text\":\"F-1\"},{\"boundingBox\":[{\"x\":238,\"y\":664},{\"x\":263,\"y\":664},{\"x\":263,\"y\":680},{\"x\":237,\"y\":680}],\"text\":\"F-2\"},{\"boundingBox\":[{\"x\":472,\"y\":664},{\"x\":493,\"y\":664},{\"x\":493,\"y\":681},{\"x\":471,\"y\":681}],\"text\":\"F-3\"},{\"boundingBox\":[{\"x\":700,\"y\":664},{\"x\":723,\"y\":664},{\"x\":723,\"y\":680},{\"x\":701,\"y\":680}],\"text\":\"F-4\"},{\"boundingBox\":[{\"x\":956,\"y\":654},{\"x\":981,\"y\":654},{\"x\":981,\"y\":669},{\"x\":956,\"y\":669}],\"text\":\"0.9\"},{\"boundingBox\":[{\"x\":955,\"y\":683},{\"x\":984,\"y\":681},{\"x\":984,\"y\":696},{\"x\":956,\"y\":697}],\"text\":\"0.8\"},{\"boundingBox\":[{\"x\":955,\"y\":711},{\"x\":984,\"y\":709},{\"x\":984,\"y\":723},{\"x\":957,\"y\":725}],\"text\":\"0.7\"},{\"boundingBox\":[{\"x\":959,\"y\":738},{\"x\":984,\"y\":739},{\"x\":984,\"y\":753},{\"x\":958,\"y\":752}],\"text\":\"0.6\"},{\"boundingBox\":[{\"x\":961,\"y\":767},{\"x\":983,\"y\":766},{\"x\":983,\"y\":781},{\"x\":961,\"y\":781}],\"text\":\"0.5\"},{\"boundingBox\":[{\"x\":960,\"y\":795},{\"x\":984,\"y\":795},{\"x\":984,\"y\":809},{\"x\":960,\"y\":810}],\"text\":\"0.4\"},{\"boundingBox\":[{\"x\":959,\"y\":824},{\"x\":982,\"y\":823},{\"x\":983,\"y\":839},{\"x\":960,\"y\":839}],\"text\":\"0.3\"},{\"boundingBox\":[{\"x\":956,\"y\":852},{\"x\":984,\"y\":850},{\"x\":984,\"y\":865},{\"x\":957,\"y\":867}],\"text\":\"0.2\"},{\"boundingBox\":[{\"x\":7,\"y\":895},{\"x\":29,\"y\":894},{\"x\":28,\"y\":910},{\"x\":7,\"y\":911}],\"text\":\"F-1\"},{\"boundingBox\":[{\"x\":236,\"y\":894},{\"x\":262,\"y\":895},{\"x\":262,\"y\":910},{\"x\":236,\"y\":910}],\"text\":\"F-2\"},{\"boundingBox\":[{\"x\":472,\"y\":894},{\"x\":497,\"y\":893},{\"x\":497,\"y\":909},{\"x\":472,\"y\":910}],\"text\":\"F-3\"},{\"boundingBox\":[{\"x\":704,\"y\":894},{\"x\":727,\"y\":894},{\"x\":728,\"y\":910},{\"x\":704,\"y\":910}],\"text\":\"F-4\"},{\"boundingBox\":[{\"x\":956,\"y\":882},{\"x\":982,\"y\":879},{\"x\":982,\"y\":893},{\"x\":958,\"y\":895}],\"text\":\"0.1\"},{\"boundingBox\":[{\"x\":957,\"y\":922},{\"x\":958,\"y\":905},{\"x\":969,\"y\":905},{\"x\":969,\"y\":922}],\"text\":\"O\"}],\"words\":[{\"boundingBox\":[{\"x\":5,\"y\":201},{\"x\":27,\"y\":200},{\"x\":28,\"y\":217},{\"x\":5,\"y\":218}],\"text\":\"F-1\"},{\"boundingBox\":[{\"x\":236,\"y\":200},{\"x\":271,\"y\":200},{\"x\":271,\"y\":217},{\"x\":236,\"y\":216}],\"text\":\"F-25\"},{\"boundingBox\":[{\"x\":472,\"y\":199},{\"x\":510,\"y\":199},{\"x\":511,\"y\":216},{\"x\":472,\"y\":216}],\"text\":\"F-38\"},{\"boundingBox\":[{\"x\":699,\"y\":201},{\"x\":747,\"y\":201},{\"x\":747,\"y\":217},{\"x\":699,\"y\":217}],\"text\":\"F-105\"},{\"boundingBox\":[{\"x\":6,\"y\":433},{\"x\":28,\"y\":432},{\"x\":29,\"y\":449},{\"x\":6,\"y\":450}],\"text\":\"F-1\"},{\"boundingBox\":[{\"x\":236,\"y\":432},{\"x\":270,\"y\":432},{\"x\":270,\"y\":448},{\"x\":236,\"y\":448}],\"text\":\"F-25\"},{\"boundingBox\":[{\"x\":470,\"y\":432},{\"x\":511,\"y\":431},{\"x\":512,\"y\":447},{\"x\":470,\"y\":448}],\"text\":\"F-38\"},{\"boundingBox\":[{\"x\":700,\"y\":433},{\"x\":748,\"y\":432},{\"x\":748,\"y\":449},{\"x\":700,\"y\":448}],\"text\":\"F-105\"},{\"boundingBox\":[{\"x\":5,\"y\":665},{\"x\":28,\"y\":664},{\"x\":29,\"y\":681},{\"x\":5,\"y\":682}],\"text\":\"F-1\"},{\"boundingBox\":[{\"x\":237,\"y\":664},{\"x\":263,\"y\":664},{\"x\":263,\"y\":680},{\"x\":237,\"y\":680}],\"text\":\"F-2\"},{\"boundingBox\":[{\"x\":471,\"y\":664},{\"x\":493,\"y\":664},{\"x\":493,\"y\":681},{\"x\":471,\"y\":681}],\"text\":\"F-3\"},{\"boundingBox\":[{\"x\":700,\"y\":664},{\"x\":722,\"y\":664},{\"x\":722,\"y\":680},{\"x\":700,\"y\":680}],\"text\":\"F-4\"},{\"boundingBox\":[{\"x\":957,\"y\":654},{\"x\":981,\"y\":654},{\"x\":981,\"y\":669},{\"x\":957,\"y\":669}],\"text\":\"0.9\"},{\"boundingBox\":[{\"x\":957,\"y\":682},{\"x\":983,\"y\":681},{\"x\":984,\"y\":695},{\"x\":958,\"y\":697}],\"text\":\"0.8\"},{\"boundingBox\":[{\"x\":957,\"y\":711},{\"x\":984,\"y\":709},{\"x\":985,\"y\":723},{\"x\":958,\"y\":725}],\"text\":\"0.7\"},{\"boundingBox\":[{\"x\":958,\"y\":738},{\"x\":983,\"y\":739},{\"x\":983,\"y\":753},{\"x\":958,\"y\":752}],\"text\":\"0.6\"},{\"boundingBox\":[{\"x\":961,\"y\":766},{\"x\":983,\"y\":766},{\"x\":983,\"y\":780},{\"x\":961,\"y\":781}],\"text\":\"0.5\"},{\"boundingBox\":[{\"x\":960,\"y\":795},{\"x\":984,\"y\":795},{\"x\":984,\"y\":809},{\"x\":960,\"y\":810}],\"text\":\"0.4\"},{\"boundingBox\":[{\"x\":959,\"y\":823},{\"x\":982,\"y\":823},{\"x\":982,\"y\":838},{\"x\":959,\"y\":839}],\"text\":\"0.3\"},{\"boundingBox\":[{\"x\":957,\"y\":852},{\"x\":984,\"y\":850},{\"x\":985,\"y\":865},{\"x\":958,\"y\":867}],\"text\":\"0.2\"},{\"boundingBox\":[{\"x\":7,\"y\":895},{\"x\":27,\"y\":894},{\"x\":28,\"y\":910},{\"x\":7,\"y\":911}],\"text\":\"F-1\"},{\"boundingBox\":[{\"x\":236,\"y\":894},{\"x\":262,\"y\":894},{\"x\":261,\"y\":910},{\"x\":236,\"y\":909}],\"text\":\"F-2\"},{\"boundingBox\":[{\"x\":472,\"y\":894},{\"x\":496,\"y\":893},{\"x\":496,\"y\":909},{\"x\":472,\"y\":910}],\"text\":\"F-3\"},{\"boundingBox\":[{\"x\":704,\"y\":894},{\"x\":727,\"y\":894},{\"x\":727,\"y\":910},{\"x\":704,\"y\":910}],\"text\":\"F-4\"},{\"boundingBox\":[{\"x\":956,\"y\":881},{\"x\":981,\"y\":879},{\"x\":982,\"y\":892},{\"x\":958,\"y\":895}],\"text\":\"0.1\"},{\"boundingBox\":[{\"x\":957,\"y\":922},{\"x\":957,\"y\":911},{\"x\":969,\"y\":911},{\"x\":968,\"y\":922}],\"text\":\"O\"}]}", "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}", "{\"language\":\"en\",\"text\":\"New Observation Control Simulation Observations Signals Time Collision Results 00000000 0000 000000000 000000000 000000000 Integration Response Loss State: velocity, position, material, force ... Trainable Network Layers Differentiable Simulation Layer\",\"lines\":[{\"boundingBox\":[{\"x\":214,\"y\":21},{\"x\":261,\"y\":21},{\"x\":260,\"y\":41},{\"x\":213,\"y\":41}],\"text\":\"New\"},{\"boundingBox\":[{\"x\":174,\"y\":49},{\"x\":298,\"y\":49},{\"x\":298,\"y\":73},{\"x\":174,\"y\":73}],\"text\":\"Observation\"},{\"boundingBox\":[{\"x\":743,\"y\":165},{\"x\":820,\"y\":164},{\"x\":820,\"y\":188},{\"x\":743,\"y\":189}],\"text\":\"Control\"},{\"boundingBox\":[{\"x\":1429,\"y\":163},{\"x\":1538,\"y\":163},{\"x\":1538,\"y\":188},{\"x\":1429,\"y\":188}],\"text\":\"Simulation\"},{\"boundingBox\":[{\"x\":226,\"y\":191},{\"x\":387,\"y\":189},{\"x\":387,\"y\":215},{\"x\":226,\"y\":217}],\"text\":\"Observations\"},{\"boundingBox\":[{\"x\":745,\"y\":194},{\"x\":820,\"y\":194},{\"x\":820,\"y\":219},{\"x\":745,\"y\":219}],\"text\":\"Signals\"},{\"boundingBox\":[{\"x\":978,\"y\":192},{\"x\":1048,\"y\":194},{\"x\":1047,\"y\":220},{\"x\":979,\"y\":220}],\"text\":\"Time\"},{\"boundingBox\":[{\"x\":1174,\"y\":193},{\"x\":1294,\"y\":193},{\"x\":1294,\"y\":222},{\"x\":1174,\"y\":221}],\"text\":\"Collision\"},{\"boundingBox\":[{\"x\":1445,\"y\":195},{\"x\":1521,\"y\":195},{\"x\":1521,\"y\":217},{\"x\":1445,\"y\":217}],\"text\":\"Results\"},{\"boundingBox\":[{\"x\":356,\"y\":381},{\"x\":357,\"y\":117},{\"x\":394,\"y\":117},{\"x\":393,\"y\":381}],\"text\":\"00000000\"},{\"boundingBox\":[{\"x\":697,\"y\":313},{\"x\":698,\"y\":185},{\"x\":728,\"y\":185},{\"x\":726,\"y\":314}],\"text\":\"0000\"},{\"boundingBox\":[{\"x\":524,\"y\":401},{\"x\":525,\"y\":101},{\"x\":564,\"y\":101},{\"x\":563,\"y\":401}],\"text\":\"000000000\"},{\"boundingBox\":[{\"x\":440,\"y\":401},{\"x\":440,\"y\":102},{\"x\":479,\"y\":102},{\"x\":478,\"y\":401}],\"text\":\"000000000\"},{\"boundingBox\":[{\"x\":609,\"y\":404},{\"x\":610,\"y\":100},{\"x\":647,\"y\":101},{\"x\":646,\"y\":404}],\"text\":\"000000000\"},{\"boundingBox\":[{\"x\":941,\"y\":230},{\"x\":1084,\"y\":231},{\"x\":1084,\"y\":263},{\"x\":941,\"y\":262}],\"text\":\"Integration\"},{\"boundingBox\":[{\"x\":1174,\"y\":232},{\"x\":1295,\"y\":233},{\"x\":1295,\"y\":262},{\"x\":1174,\"y\":261}],\"text\":\"Response\"},{\"boundingBox\":[{\"x\":1920,\"y\":237},{\"x\":1979,\"y\":237},{\"x\":1978,\"y\":262},{\"x\":1920,\"y\":262}],\"text\":\"Loss\"},{\"boundingBox\":[{\"x\":864,\"y\":320},{\"x\":1388,\"y\":320},{\"x\":1388,\"y\":351},{\"x\":864,\"y\":352}],\"text\":\"State: velocity, position, material, force ...\"},{\"boundingBox\":[{\"x\":356,\"y\":421},{\"x\":693,\"y\":423},{\"x\":693,\"y\":452},{\"x\":356,\"y\":450}],\"text\":\"Trainable Network Layers\"},{\"boundingBox\":[{\"x\":923,\"y\":421},{\"x\":1333,\"y\":423},{\"x\":1333,\"y\":451},{\"x\":923,\"y\":449}],\"text\":\"Differentiable Simulation Layer\"}],\"words\":[{\"boundingBox\":[{\"x\":213,\"y\":21},{\"x\":261,\"y\":21},{\"x\":261,\"y\":41},{\"x\":213,\"y\":41}],\"text\":\"New\"},{\"boundingBox\":[{\"x\":174,\"y\":50},{\"x\":298,\"y\":50},{\"x\":298,\"y\":73},{\"x\":175,\"y\":74}],\"text\":\"Observation\"},{\"boundingBox\":[{\"x\":744,\"y\":166},{\"x\":820,\"y\":164},{\"x\":820,\"y\":189},{\"x\":744,\"y\":190}],\"text\":\"Control\"},{\"boundingBox\":[{\"x\":1430,\"y\":164},{\"x\":1538,\"y\":165},{\"x\":1538,\"y\":189},{\"x\":1430,\"y\":189}],\"text\":\"Simulation\"},{\"boundingBox\":[{\"x\":227,\"y\":193},{\"x\":366,\"y\":191},{\"x\":363,\"y\":217},{\"x\":227,\"y\":216}],\"text\":\"Observations\"},{\"boundingBox\":[{\"x\":745,\"y\":194},{\"x\":819,\"y\":195},{\"x\":819,\"y\":219},{\"x\":745,\"y\":219}],\"text\":\"Signals\"},{\"boundingBox\":[{\"x\":978,\"y\":192},{\"x\":1047,\"y\":193},{\"x\":1047,\"y\":221},{\"x\":978,\"y\":220}],\"text\":\"Time\"},{\"boundingBox\":[{\"x\":1174,\"y\":193},{\"x\":1294,\"y\":194},{\"x\":1293,\"y\":222},{\"x\":1175,\"y\":222}],\"text\":\"Collision\"},{\"boundingBox\":[{\"x\":1446,\"y\":195},{\"x\":1521,\"y\":195},{\"x\":1520,\"y\":218},{\"x\":1445,\"y\":217}],\"text\":\"Results\"},{\"boundingBox\":[{\"x\":356,\"y\":381},{\"x\":359,\"y\":118},{\"x\":394,\"y\":118},{\"x\":394,\"y\":382}],\"text\":\"00000000\"},{\"boundingBox\":[{\"x\":698,\"y\":314},{\"x\":700,\"y\":186},{\"x\":729,\"y\":187},{\"x\":727,\"y\":314}],\"text\":\"0000\"},{\"boundingBox\":[{\"x\":525,\"y\":401},{\"x\":527,\"y\":102},{\"x\":564,\"y\":102},{\"x\":564,\"y\":401}],\"text\":\"000000000\"},{\"boundingBox\":[{\"x\":441,\"y\":402},{\"x\":441,\"y\":104},{\"x\":479,\"y\":104},{\"x\":479,\"y\":401}],\"text\":\"000000000\"},{\"boundingBox\":[{\"x\":610,\"y\":402},{\"x\":612,\"y\":101},{\"x\":647,\"y\":102},{\"x\":647,\"y\":402}],\"text\":\"000000000\"},{\"boundingBox\":[{\"x\":941,\"y\":231},{\"x\":1083,\"y\":233},{\"x\":1084,\"y\":262},{\"x\":941,\"y\":263}],\"text\":\"Integration\"},{\"boundingBox\":[{\"x\":1175,\"y\":232},{\"x\":1295,\"y\":235},{\"x\":1294,\"y\":262},{\"x\":1175,\"y\":262}],\"text\":\"Response\"},{\"boundingBox\":[{\"x\":1920,\"y\":237},{\"x\":1978,\"y\":237},{\"x\":1978,\"y\":262},{\"x\":1920,\"y\":262}],\"text\":\"Loss\"},{\"boundingBox\":[{\"x\":865,\"y\":322},{\"x\":938,\"y\":321},{\"x\":937,\"y\":352},{\"x\":864,\"y\":351}],\"text\":\"State:\"},{\"boundingBox\":[{\"x\":944,\"y\":321},{\"x\":1052,\"y\":320},{\"x\":1052,\"y\":352},{\"x\":943,\"y\":352}],\"text\":\"velocity,\"},{\"boundingBox\":[{\"x\":1058,\"y\":320},{\"x\":1171,\"y\":320},{\"x\":1170,\"y\":352},{\"x\":1058,\"y\":352}],\"text\":\"position,\"},{\"boundingBox\":[{\"x\":1176,\"y\":320},{\"x\":1289,\"y\":320},{\"x\":1289,\"y\":351},{\"x\":1176,\"y\":352}],\"text\":\"material,\"},{\"boundingBox\":[{\"x\":1295,\"y\":320},{\"x\":1354,\"y\":321},{\"x\":1354,\"y\":350},{\"x\":1295,\"y\":351}],\"text\":\"force\"},{\"boundingBox\":[{\"x\":1359,\"y\":321},{\"x\":1388,\"y\":321},{\"x\":1388,\"y\":349},{\"x\":1360,\"y\":349}],\"text\":\"...\"},{\"boundingBox\":[{\"x\":358,\"y\":421},{\"x\":474,\"y\":422},{\"x\":474,\"y\":452},{\"x\":357,\"y\":451}],\"text\":\"Trainable\"},{\"boundingBox\":[{\"x\":480,\"y\":422},{\"x\":593,\"y\":424},{\"x\":593,\"y\":452},{\"x\":480,\"y\":452}],\"text\":\"Network\"},{\"boundingBox\":[{\"x\":599,\"y\":424},{\"x\":692,\"y\":425},{\"x\":692,\"y\":451},{\"x\":599,\"y\":452}],\"text\":\"Layers\"},{\"boundingBox\":[{\"x\":923,\"y\":422},{\"x\":1102,\"y\":422},{\"x\":1103,\"y\":450},{\"x\":924,\"y\":450}],\"text\":\"Differentiable\"},{\"boundingBox\":[{\"x\":1108,\"y\":422},{\"x\":1246,\"y\":423},{\"x\":1246,\"y\":450},{\"x\":1108,\"y\":450}],\"text\":\"Simulation\"},{\"boundingBox\":[{\"x\":1251,\"y\":423},{\"x\":1332,\"y\":424},{\"x\":1333,\"y\":451},{\"x\":1252,\"y\":450}],\"text\":\"Layer\"}]}", "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}", "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}" ] }, { "@search.score": 1.0993793, "content": "\nJ Braz Comput Soc (2013) 19:573–587\nDOI 10.1007/s13173-013-0117-7\n\nSURVEY PAPER\n\nA systematic review on keystroke dynamics\n\nPaulo Henrique Pisani · Ana Carolina Lorena\n\nReceived: 18 March 2013 / Accepted: 24 June 2013 / Published online: 10 July 2013\n© The Brazilian Computer Society 2013\n\nAbstract Computing and communication systems have\nimproved our way of life, but have also contributed to an\nincreased data exposure and, consequently, to identity theft.\nA possible way to overcome this issue is by the use of biomet-\nric technologies for user authentication. Among the possible\ntechnologies to be analysed, this work focuses on keystroke\ndynamics, which attempts to recognize users by their typ-\ning rhythm. In order to guide future researches in this area,\na systematic review on keystroke dynamics was conducted\nand presented here. The systematic review method adopts\na rigorous procedure with the definition of a formal review\nprotocol. Systematic reviews are not commonly used in arti-\nficial intelligence, and this work contributes to its use in the\narea. This paper discusses the process involved in the review\nalong with the results obtained in order to identify the state\nof the art of keystroke dynamics. We summarized main clas-\nsifiers, performance measures, extracted features and bench-\nmark datasets used in the area.\n\nKeywords Behavioral intrusion detection · Biometrics ·\nKeystroke dynamics · Systematic review\n\nP. H. Pisani (B)\nInstituto de Ciências Matemáticas e de Computação (ICMC),\nUniversidade de São Paulo (USP), São Carlos, SP, Brazil\ne-mail: phpisani@icmc.usp.br\n\nA. C. Lorena\nInstituto de Ciência e Tecnologia (ICT),\nUniversidade Federal de São Paulo (UNIFESP),\nSão José dos Campos, SP, Brazil\ne-mail: aclorena@unifesp.br\n\n1 Introduction\n\nThe wider dissemination of digital identities has contributed\nto greater worries regarding information exposure [47].\nRecently, in view of the increased dissemination of the inter-\nnet in several activities (e.g. online banking, e-commerce,\ne-mail), security problems became more evident [24]. As a\nresult, identity theft has gained new momentum. The term\nidentity theft is commonly used to refer to the crime of using\npersonal information of someone else to illegally pretend to\nbe a certain person [38].\n\nIn view of this scenario, more sophisticated methods for\nuser authentication have been developed. Authentication is\nthe process used to confirm the identity of a user. In the case of\nworkstations, for example, the authentication usually occurs\nin the system initialization, known as initial authentication.\nNevertheless, even more secure authentication methods do\nnot provide an entirely effective security mechanism, as the\ncomputer may be vulnerable to intruders when the user leaves\nthe workstation and does not end the session. Consequently,\nan intruder could use the computer masquerading as the legit-\nimate user, resulting in identity theft [38]. One of the ways to\nmitigate this problem is by using intrusion detection systems\nthat act on the workstation (host-based).\n\nMore recently, the concept of detecting intrusions by the\nbehavioral analysis of the user of the computer [39] has\nemerged, also known as Behavioral Intrusion Detection [49];\nseveral aspects of this method have yet to be explored. This\nconcept is grounded on the fact that, by observing the behav-\nior of a user, it is possible to define models that represent\nthe regular behavior (profile) of this user, thus allowing the\nidentification of deviations that are potential intrusions. The\nprocess of defining these models is known as user profil-\ning [46]. There is a great variety of features that can be\nused to define the model of a user. This work focuses on\n\n123\n\n\n\n574 J Braz Comput Soc (2013) 19:573–587\n\nkeystroke dynamics, classified as a behavioral biometric\ntechnology.\n\nThis paper adopts a rigorous method to perform a review\non intrusion detection with keystroke dynamics, known as\nsystematic review. As the name suggests, a systematic review\nadopts a formal and systematic procedure for the conduction\nof the bibliographic review, with the definition of explicit\nprotocols for obtaining information. Consequently, by using\nthese protocols, the results attained by the systematic review\ncan be reproduced by other researchers as a way of validation,\ndecreasing the incidence of bias in the review, a problem\nboosted in non-systematic bibliographic reviews [33].\n\nSystematic reviews are commonly applied in other areas,\nmainly in medicine, and have a number of reported benefits\n[33]. In the area of computing, this review method is more\ndisseminated in software engineering [7]. This paper con-\ntributes to the use of systematic review in computing, partic-\nularly in artificial intelligence. Here, we discuss how the sys-\ntematic review was applied and the achieved results, which\nare valuable information for the area of intrusion detection\nwith keystroke dynamics.\n\nThis paper presents a systematic review carried out with\nthe aim of identifying the state of the art in keystroke dynam-\nics applied to intrusion detection. Preliminary results of this\nreview are shown in [42] and [41]. The remaining sections are\norganized as follows: in Sect. 2, basic concepts of keystroke\ndynamics are introduced; in Sect. 3, the process of system-\natic review is presented; Sect. 4 discusses how the systematic\nreview was applied in this work, specifying the review proto-\ncol and the steps adopted; in Sect. 5, the results obtained\nby the systematic review are summarized; and, finally,\nSect. 6 presents our conclusions.\n\n2 Background\n\nIn information security, intrusion detection is the process of\nmonitoring events in a computer or network and analyse them\nto detect signals of possible incidents, which are violations\nor threats of violations of security policies, acceptable use or\nsecurity practices [45]. An intrusion detection system (IDS)\nautomatizes this process.\n\nAs previously discussed, more recently, a new concept\nof detecting intrusions by the analysis of the user behaviour\nin the computer has emerged [39], which is performed by\nthe behavioural IDS [49]. This type of system is grounded\non a concept known as user profiling, which consists of\nobserving the behaviour of a user in order to generate mod-\nels that represent its normal behaviour. Observed events\nare then compared to these models and possible devia-\ntions are classified as potential intrusions [46]. An IDS\nthat applies user profiling is a system based on anomaly\ndetection, as it generates alarms for events that deviates\n\nKeystroke dynamics,\nApplication usage, etc.\n\nUser\n\nTraining\n\nRecognition\n\nGet profile\n\nYes/No\n\nTraining\nphase?\n\nS\n\nN\n\nUser profile\n\nStore profile\n\nFig. 1 Behavioural intrusion detection (adapted from [42])\n\nfrom a behaviour pattern. Figure 1 represents the basic\nflow of a behavioural IDS, which involves two major steps\n[16,21]:\n\n– Training: obtaining features for the definition of the user\nbehavior pattern;\n\n– Recognition: matching observed features against user\nbehavior pattern.\n\nA key issue in the application of user profiling is how to\ndefine the profile, that is, which aspects will be observed.\nThe process of choosing these aspects is one of the major\nquestions when applying user profiling. Ideally, the chosen\naspects should allow the identification of a user within a\ngroup of users and, at the same time, maintain similar values\nthrough the time for the same user [21]. There is a number of\naspects that can be used for the definition of the user profile,\nsuch as keystroke dynamics, system audit logs, e-mail and\ncommand line use [46].\n\nThis work studies keystroke dynamics as an aspect to\nbe analysed by the behavioural intrusion detection sys-\ntem. Keystroke dynamics analyzes how users type from\nthe monitoring of the keyboard input. As a result, mod-\nels that represent the regular typing rhythm of the user are\ndefined. Afterwards, these models are used for the recogni-\n\n123\n\n\n\n\n\n\n\n\n\n\n\n\n\nJ Braz Comput Soc (2013) 19:573–587 575\n\ntion [28], in such a way that typing rhythms deviating from\nthis model are classified as being from intruders. Here, we\nhave chosen keystroke dynamics instead of other aspects\nbecause it may be used either in the initial authentication\nof a system or as continuous authentication after the ini-\ntial authentication. It makes this technology more flexible\nthan an analysis of systems audit logs or e-mail behav-\niour.\n\nKeystroke dynamics can be applied in two ways: static\ntext or dynamic text. Static text only performs an analysis\nof fixed expressions as, for example, a password. While, in\ndynamic text, the analysis occurs for any text that is typed by\nthe user. Keystroke dynamics in static text requires less effort\nto be implemented and it also reached lower error rates in\nliterature [11].\n\nTwo distinctive processes are involved in keystroke dynam-\nics: feature extraction and classification of the extracted\nfeatures. In the first process, a number of features are\nextracted for the recognition of a user. These features\nshould represent how the user behaves in terms of keystroke\ndynamics.\n\nIn the second process, which corresponds to the feature\nclassification, several algorithms can be used. For instance,\nmachine learning algorithms, like neural networks [48] and\nsupport vector machines [19], were applied in this classifica-\ntion, which consists of verifying whether the typing features\nbelong or not to a specific user.\n\n3 Systematic review\n\nSystematic literature review (called just systematic review\nin this paper) is a method for conducting bibliographic\nreviews in a formal way, following well defined steps, which\nallows the results to be reproducible. In addition, the pro-\ntocol adopted for the conduction of the review must assure\nits completion. This review method is commonly used in\nother areas, mainly in Medicine [7] and has several reported\nbenefits, like less susceptibility to bias [33]. In the area of\nComputing, this method of review is more disseminated in\nSoftware Engineering.\n\nThe application of the systematic review involves three\nmajor phases: planning, conduction and presentation of\nresults. In the first phase, a review protocol is defined, in\nwhich research questions are specified along with search\nstrategies. After that, in the second phase, the review pro-\ntocol is applied and the information is extracted from the\nreturned references. References used for the extraction of\ninformation are called primary studies, while the review\nis a secondary study. Finally, the third phase defines the\nway to present the results and the final report is done.\nThe items comprehended in each of the three phases are\n[33]:\n\n3.1 Planning\n\n– Identification of the review need: a systematic review has\nthe goal of summarizing all information regarding a spe-\ncific topic. However, before starting a systematic review,\nthe need of this review has to be checked. This check-\ning, for instance, should verify the existence of previ-\nously published systematic reviews that deal with the\ntopic under investigation and whether the protocol of\nthese reviews meet the requirements of the research.\n\n– Commissioning (optional): in some cases, due to the lack\nof time or specific knowledge, one may need to request\nthat other researchers conduct the systematic review.\n\n– Specification of the research questions: this is considered\nto be the most important part of the systematic review,\nas these questions will guide all the following steps, as\nthe search for primary studies, extraction and analysis of\ninformation.\n\n– Development of the review protocol: this step defines\nstrategies to be used for the search, selection and eval-\nuation of the references. In addition, the information to\nbe extracted from each of the selected references is also\ndefined.\n\n– Protocol evaluation (optional): as the review protocol is\nan essential part of the systematic review, it is recom-\nmended to be reviewed by other researches.\n\n3.2 Conduction\n\n– Reference search: search for the greatest possible number\nof references which can answer the research question in\norder to avoid bias. In the systematic review, the search is\nperformed with increased rigour, with the pre-definition\nof search expressions and databases, making it different\nfrom traditional reviews.\n\n– Selection of primary studies: after reference search, the\nstudies that are in fact relevant for the research must be\nselected, by the use of inclusion/exclusion criteria.\n\n– Quality evaluation: each of the selected references\nundergo a quality evaluation. This evaluation may be\nused with diverse aims, like contributing for the inclu-\nsion/exclusion criteria or supporting the summary results,\nby measuring the importance of each study.\n\n– Information extraction: the information extraction from\nthe references must be done with the support of forms\ndefined during the planning phase of the systematic\nreview.\n\n– Data synthesis: this step corresponds to summarizing the\nresults attained during the review. This summary may\ninvolve qualitative and quantitative aspects. For quanti-\ntative aspects, a meta-analysis may also be applied.\n\n123\n\n\n\n576 J Braz Comput Soc (2013) 19:573–587\n\n3.3 Reporting the review\n\n– Specification of the dissemination mechanisms and for-\nmulation of the report: dissemination of the results\nattained by the systematic review. This can be done by\npublishing in academic journals and conferences or even\nin web sites.\n\n– Report evaluation (optional): this evaluation can be\nrequested to experts in the area of the research. If the\nreview is submitted to a journal or conference, the review\nprocess of the publication can be considered an evalua-\ntion of the report.\n\nThe explicit definition of the review protocol allows the\nresults to be reproduced. The review presented in this paper\nwas performed by two researchers in the planning phase,\nbut by just one in the conduction phase. Due to that, this\nreview can be called a quasi-systematic review, as it follows\nthe principles of a systematic review, but was not conducted\nby two researchers in all phases. This term, quasi-systematic\nreview, was also used in previous work [35]. More details on\nhow to carry out each of the phases are discussed in the next\nsections, in which the systematic review process is applied to\nthe topic of keystroke dynamics for intrusion detection.\n\n4 How the systematic review was applied\n\nIn this work, the application of the systematic review has the\ngoal of studying the state of the art in keystroke dynamics in\norder to identify:\n\n1. Advantages and disadvantages of using keystroke dynam-\nics in intrusion detection;\n\n2. Extracted features;\n3. Classification algorithms applied;\n4. Performance measures commonly adopted;\n5. Benchmarking datasets, which are useful for conducting\n\ncomparative experiments in the area.\n\nBefore presenting details of how the systematic review\nwas applied in this work, it is important to highlight that we\nonly considered references indexed by reference databases\navailable on the Internet and written in English.\n\n4.1 Planning\n\nAccording to a research carried by the authors, there are\nno published systematic reviews that meet the goals of this\nwork. Besides, the newer review article on keystroke dynam-\nics known by the authors was submitted for publication in\n2009 [28]. Moreover, part of our aims was not met in that\n\npublication, as the identification of benchmarking datasets.\nHence, the conduction of the review in this work is justified.\n\n4.1.1 Research questions\n\nIn view of the need of the systematic review, we defined a\nresearch question and some respective sub-questions to meet\nthe established goals:\n\nHow keystroke dynamics is used for intrusion\ndetection?\n\n– What are the advantages and disadvantages of using\nkeystroke dynamics for intrusion detection?\n\n– What features are extracted from the typing data?\n– What classification algorithms are applied? What algo-\n\nrithms are used in the performance comparisons?\n– What measures were used to evaluate the performance?\n\nWhat was the performance achieved?\n– What datasets are used to measure the performance of\n\nthe classifier? How many users took part in the tests\nperformed?\n\n4.1.2 References search\n\nAfter defining the research question, we enumerated a list\nof terms related to papers that could answer it: keystroke\ndynamics, typing dynamics, keystroke biometric(s), key-\nstroke authentication, keystroke pattern(s), typing pattern(s),\nbehaviour intrusion detection, behavior intrusion detection,\nbehavioral IDS, biometric intrusion detection, user profil-\ning, behavioural biometrics, behavioral biometrics, contin-\nuous authentication, typing biometric(s), keypress biomet-\nric(s), keystroke analysis. The use of various terms for the\nsame topic, sometimes even synonyms, contributes to the\ncompleteness of the search [1]. From this list of terms, we\nbuilt search expressions for each database of references. The\nbasic search expression is the conjunction of each term in the\nlist using the logical connective O R.\n\nNevertheless, after some tests with this search expres-\nsion, we observed that many of the returned references dealt\nwith topics not related to the research question, as personal-\nization systems and recommender systems. For this reason,\nsome terms that could exclude these unrelated topics were\nidentified: web search, personalized information, personal-\nized content, content delivery, recommendation system, rec-\nommendations system, information retrieval, personalizing,\npersonalization, recommender. The basic search expression\nwas then modified to consider the exclusion terms with the\nuse of the logic connective AN D and N OT together, as\nfollows:\n\n(‘‘behavioural intrusion detection’’\nOR ‘‘behavioral intrusion detection’’\n\n123\n\n\n\nJ Braz Comput Soc (2013) 19:573–587 577\n\nOR ‘‘behavioral IDS’’\nOR ‘‘behavioural IDS’’\nOR ‘‘biometric intrusion detection’’\nOR ‘‘user profiling’’\nOR ‘‘keystroke dynamics’’\nOR ‘‘typing dynamics’’\nOR ‘‘keystroke biometrics’’\nOR ‘‘keystroke biometric’’\nOR ‘‘continuous authentication’’\nOR ‘‘keystroke authentication’’\nOR ‘‘behavioural biometrics’’\nOR ‘‘behavioral biometrics’’\nOR ‘‘keystroke pattern’’\nOR ‘‘keystroke patterns’’\nOR ‘‘typing pattern’’\nOR ‘‘typing patterns’’\nOR ‘‘typing biometric’’\nOR ‘‘typing biometrics’’\nOR ‘‘keypress biometric’’\nOR ‘‘keypress biometrics’’\nOR ‘‘keystroke analysis’’)\n\nAND NOT\n\n(‘‘web search’’\nOR ‘‘personalized information’’\nOR ‘‘personalized content’’\nOR ‘‘content delivery’’\nOR ‘‘recommendation system’’\nOR ‘‘recommendations system’’\nOR ‘‘information retrieval’’\nOR ‘‘personalizing’’\nOR ‘‘personalization’’\nOR ‘‘recommender’’)\n\nThis search expression was applied in several data-bases\nthat included references in the computing area. As each data-\nbase has differences in its syntax for search expression, the\nbasic search expression presented here was adapted to each\ndatabase, as specified in Appendix A. The following data-\nbases were considered in this work:\n\n– ACM Digital Library\n(http://dl.acm.org/)\n\n– IEEE Xplore\n(http://ieeexplore.ieee.org/)\n\n– Science Direct\n(http://www.sciencedirect.com/)\n\n– Web of Science\n(http://isiknowledge.com/)\n\n– Scopus\n(http://www.scopus.com/)\n\n4.1.3 Selection criteria\n\nThe last part of the planning phase is the definition of\nthe selection criteria (inclusion and exclusion) that will be\napplied to the returned references. In this systematic review,\nall the returned references are included for analysis in the\nnext steps, except the ones that meet the following exclusion\ncriteria:\n\n1. Publications that do not deal with keystroke dynamics\nfor intrusion detection: the aim of this review is to work\nwith intrusion detection, which comprehends authentica-\ntion systems. Therefore, references that do not meet this\nrequirement were not included.\n\n2. Publications with one page, posters, presentations, abstra-\ncts and editorials, texts in magazines/newspaper and\nduplicate publications in terms of results, except the most\ncomplete version: references without enough informa-\ntion to answer the research question. This criterion also\navoids unnecessary work for the cases in which the same\nstudy is published in different versions.\n\n3. Publication hosted in services with restricted access and\nnot accessible or publications not written in English.\n\nIn this phase, we also created a quality score to be applied\nto the returned references. This score was determined to high-\nlight references that better answer our research question. The\nvalue of the quality score is the sum of the score reached in\neach of the assessed items. For each of these items, the ref-\nerence scores 1 if fully meets it, 0.5 if partially meets it and\n0 if does not meet the assessed item. As there are nine items,\nthe possible scores ranges between 0 and 9, in such a way\nthat higher values indicate better publications according to\nthe established research criteria. The items are:\n\n1. Were the goals clearly presented in the beginning of the\nwork?\n\n2. Were the advantages/disadvantages of keystroke dynam-\nics discussed?\n\n3. Is the dataset available to be reused?\n4. Was it detailed how the feature vector is generated?\n5. Were the values of the algorithm parameters presented?\n6. Were the applied approaches detailed so as to allow them\n\nto be replicated?\n7. Were experimental tests conducted?\n8. Were the results compared to previous researches in the\n\narea?\n9. Were the limitations of the study presented?\n\nThe quality criteria were defined considering that researc-\nhes may present problems in the following steps: design,\nconduction, analysis and conclusion [33]. The items 1 and\n\n123\n\nhttp://dl.acm.org/\nhttp://ieeexplore.ieee.org/\nhttp://www.sciencedirect.com/\nhttp://isiknowledge.com/\nhttp://www.scopus.com/\n\n\n578 J Braz Comput Soc (2013) 19:573–587\n\n2 refer to the design step, the items 3–6 to the conduction\nstep, the items 7–8 to the analysis step and the item 9 to the\nconclusion step. Part of the items used to assess the quality\nwas based on the list in [33], which presents several items to\nbe evaluated in references.\n\n4.1.4 Information extraction\n\nStill in the planning phase of the systematic review, we\ndefined a set of information to be extracted from each selected\nreference (after the application of the exclusion criteria), as\nfollows:\n\n– Basic information about the publication (title, authors,\nname and year of publication)\n\n– Were performance tests conducted?\n– Type of device (e.g. PC, mobile)\n– Best performance achieved: algorithm, measure and\n\nperformance\n– Number of users in the tests\n– Algorithms used in the tests\n– Extracted features\n– Is the test dataset available to be reused? Where?\n– Type of verification: static text or dynamic text?\n– Observations\n\nThese items were defined in line with the research question,\nin order to answer it and guide the information extraction in\nthe conduction phase of this review.\n\n4.2 Conduction\n\nFrom the review protocol defined in the planning phase, the\nconduction of the systematic review was started.\n\n4.2.1 Application of the search expressions\n\nThe first step was to apply the search expressions in each\ndatabase of references and save the returned results. Apart\nfrom the returned references, we also included a reference\npreviously known by the authors, but not indexed by the data-\nbases used in this review: [15]. This reference is mentioned\nin several papers as being one of the first publications about\nkeystroke dynamics. Table 1 shows the number of references\nreturned by each database on 18/February/2013.\n\nThese results were centralized in order to continue the\nreview, using a tool called Mendeley (available in: http://\nwww.mendeley.com/). We used this tool to import the results\nexported from the databases. Mendeley has a series of use-\nful features that can be used for systematic reviews, such as\nsearch for duplicates, organization of references by category\nand associations of the entries with PDF files stored in the\ncomputer.\n\nTable 1 Number of returned references\n\nDatabase Number of references\n\nACM Digital Library 71\n\nIEEE Xplore 308\n\nScience Direct 104\n\nWeb of Science 596\n\nScopus 943\n\nGaines et al. [15] 1\n\nTotal 2, 023\n\n4.2.2 Selection of references\n\nAfter the centralization of the information returned from the\nsearch databases, duplicate references were removed. Dupli-\ncate references may appear since databases can have some\nintersection in the indexed data, as in the case of Scopus and\nWeb of Science.\n\nOnce the removal of duplicates was finished, a fast read-\ning of the text of the remaining references was performed.\nBefore starting this step, we needed to download the com-\nplete text of each publication. However, it was not possible\nto download 27 of them, which were hosted in services not\navailable from our university (exclusion criterion 3). Conse-\nquently, the number of eligible references was again reduced.\nIn the end, another fast reading of the eligible references was\nperformed to revalidate the exclusion criteria 1 and 2. A great\nnumber of references that do not deal with keystroke dynam-\nics for intrusion detection has been eliminated just by the title\nand abstract, nevertheless, some references were eliminated\nonly after reading their full text. Once the exclusion crite-\nria 1 to 3 were applied, secondary studies were removed,\nwhich were only three: [11,28,40]. Secondary studies are\nthose commonly known as reviews or surveys. Table 2 shows\nthe number of references returned after the application of\neach step.\n\nWith the application of all exclusion criteria, 200 refer-\nences (Table 2) were left for the next steps: information\nextraction and quality assessment. Aiming at accelerating\nthese tasks, we created a spreadsheet with all the items for\ninformation extraction and quality assessment discussed in\n\nTable 2 Number of references after each step\n\nStep Number\n\nTotal of references 2,023\n\nAfter elimination of duplicates and exclusion\ncriteria 1 and 2\n\n230\n\nAfter exclusion criterion 3 203\n\nAfter exclusion of secondary studies 200\n\n123\n\nhttp://www.mendeley.com/\nhttp://www.mendeley.com/\n\n\nJ Braz Comput Soc (2013) 19:573–587 579\n\nthe planning phase (Sect. 4.1). This spreadsheet was then\nfilled with the information from the references.\n\nThis was the part of the systematic review that consumed\nmore time due to the need to read in detail several texts. In\naddition, sometimes the information to be extracted were not\npresent in a direct way in the text. For example, in some pub-\nlications, there were tables summarizing tested algorithms\nand their performance [19] or it was even possible to extract\nalmost all information from the abstract [22]. However, this\nwas not the case of some publications, which needed to be\nread more deeply to find the desired information. Actually,\nthis observation may be related to the one mentioned in [7],\nwhich highlights the fact that abstracts in Computing are usu-\nally not well structured, making it difficult to get informa-\ntion about the publication only by the abstract. According to\n[7], the scenario is different in medicine, area in which the\nabstracts are, in general, better structured and usually contain\nmore information about the publication.\n\n4.2.3 Quality assessment\n\nDue to the high number of selected references, they were\nsorted in descending order of quality score and only the ones\nwith the highest scores are discussed in details here. For the\npurpose of this review, only those papers with quality score\nequals or higher than 7.5 were considered, resulting in 16\npublications. The focus on references with higher scores has\nthe goal of spending greater efforts on references more rel-\nevant to the research question, as the quality scores were\nspecially designed with this purpose.\n\nThe graph in Fig. 2 shows the number of publications for\neach quality score. The average score among those different\nfrom zero was 5.54 and, as shown in Fig. 2, the scores follow\nan approximate normal distribution. The maximum reached\nscore was 8.5.\n\nAnother aspect analysed was the number of selected publi-\ncations by year, as shown in the graph in Fig. 3. In this graph,\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\n30\n\n35\n\n0 1 2 3 4 5 6 7 8 9\n\nN\nu\n\nm\nb\n\ner\n o\n\nf \nP\n\nu\nb\n\nlic\nat\n\nio\nn\n\ns\n\nQuality Score\n\nFig. 2 Publications by quality score\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\n30\n\n1998 2000 2002 2004 2006 2008 2010 2012\n\nN\nu\n\nm\nb\n\ner\n o\n\nf \nP\n\nu\nb\n\nlic\nat\n\nio\nn\n\ns\n\nPublication Year\n\nFig. 3 Publications by year in keystroke dynamics. The growth trend\nillustrates that the field is gaining new momentum, justifying additional\nresearch efforts\n\nit is important to highlight the growth trend in the number of\npublications by year in the area of keystroke dynamics. This\ntrend was higher between 2002 and 2006. Such a growth\ntrend indicates that the area has been receiving more atten-\ntion from the scientific community. This may justify addi-\ntional research efforts in keystroke dynamics.\n\nBoth graphs consider only the references with available\ntexts.\n\n5 Results\n\nIn this section, we focus on the 16 publications with highest\nquality score and on some papers referenced by them. The\nfollowing subsections are organized in such a way to answer\neach of the research sub-questions: advantages and disadvan-\ntages of keystroke dynamics, feature extraction, classifica-\ntion algorithms, performance evaluation and benchmarking\ndatasets.\n\n5.1 Advantages and disadvantages\n\nAuthentication of users is done by the use of credentials, also\nknown as authentication factors, which can be [47]:\n\n1. what the user knows (e.g. password);\n2. what the user has (e.g. access card, token);\n3. what the user is/does (e.g. biometrics: recognition by fin-\n\ngerprint, iris, keystroke dynamics, voice recognition);\n4. some combination of the above items.\n\nThe primary method of authentication, be it for\ne-commerce or for military purposes, is a simple login and\npassword [12]. The use of this method is based on the fact that\nthe secrecy of the password will be held [40]. However, this\nis not always the case, implying in a number of weaknesses\n[10]:\n\n123\n\n\n\n580 J Braz Comput Soc (2013) 19:573–587\n\n– Passwords may be shared by several users, resulting in\nunauthorized access;\n\n– Passwords may be copied without authorization;\n– Passwords may be guessed, particularly for easy pass-\n\nwords, as when someone uses his/her birthday as a pass-\nword [43].\n\nMoreover, even in scenarios in which the user authenti-\ncation is performed by the use of access cards, the security\nis compromised. This is because the card ownership can be\nshared with an unauthorized user and it may also be stolen\n[26].\n\nThese problems, along with widespread use of the Web,\ncontributed to expansion of identity theft, which occurs when\na person uses personal information of someone else to ille-\ngally pretend to be this person [38]. In recent years, identity\ntheft has become a crime with the rate of greatest growth in\nthe USA [6]. Furthermore, the sum of losses in the world due\nto identity theft have been estimated to be around US$ 221\nbillion in 2003 [25]. According to research, [29], weaknesses\nof passwords was the most exploited factor by insiders (users\nfrom the same institution which is the victim of the attack).\n\nOne way to mitigate this problem is the use of biometric\ntechnologies to enhance the security provided by passwords.\nIn the security context, biometrics is a science which studies\nmethods for the determination of user identity based on phys-\niological and behavioral features [26]. Keystroke dynamics,\nwhich is considered a biometric technology, can be used with-\nout any additional cost with hardware, in contrast to other\nbiometric technologies (e.g., iris, fingerprint), which need\nspecific devices for the capture of biometric data [24,37].\nIn addition, the level of transparency in the use of keystroke\ndynamics is high [40]. This means that there is no need to\nperform specific operations for the authentication by key-\nstroke dynamics [3]. This factor contributes for an increased\nacceptance of keystroke dynamics among users.\n\nRecognition precision by keystroke dynamics may be\naffected in the presence of keyboards with different charac-\nteristics in the same environment. Nevertheless, it is expected\nthat such differences does not significantly impair the recog-\nnition performance and, consequently, still enable proper\nuser identification [24]. This can be compared to the sig-\nnature recognition biometrics in which, regardless of the pen\nused, the system is still able to differentiate between legiti-\nmate and illegitimate users [24].\n\nFurthermore, false alarm rates (when a legitimate user\nis classified as an intruder) in keystroke dynamics are usu-\nally high and do not meet standards in some access con-\ntrol systems, such as the European. Additionally, differences\namong systems, like p", "metadata_storage_path": "aHR0cHM6Ly9zdG9yYWdlYWNjb3VudGxpZ2lyay5ibG9iLmNvcmUud2luZG93cy5uZXQvcGFwZXIvczEzMTczLTAxMy0wMTE3LTcucGRm0", "metadata_author": null, "metadata_title": null, "keyphrases": [ "São José dos Campos", "J Braz Comput Soc", "Ciências Matemáticas", "The Brazilian Computer Society", "São Paulo", "São Carlos", "Ciência e", "Paulo Henrique Pisani", "Ana Carolina Lorena", "typ- ing rhythm", "P. H. Pisani", "A. C. Lorena", "behavioral biometric technology", "effective security mechanism", "Behavioral intrusion detection", "formal review protocol", "intrusion detection systems", "user profil- ing", "secure authentication methods", "systematic review method", "Computação", "behavioral analysis", "communication systems", "security problems", "sophisticated methods", "The process", "Systematic reviews", "keystroke dynamics", "Abstract Computing", "data exposure", "ric technologies", "possible technologies", "future researches", "rigorous procedure", "ficial intelligence", "performance measures", "mark datasets", "digital identities", "greater worries", "information exposure", "several activities", "online banking", "new momentum", "personal information", "system initialization", "several aspects", "regular behavior", "great variety", "rigorous method", "initial authentication", "possible way", "Instituto de", "Universidade Federal", "wider dissemination", "identity theft", "imate user", "potential intrusions", "SURVEY PAPER", "user authentication", "DOI", "life", "issue", "work", "users", "order", "area", "definition", "results", "state", "art", "sifiers", "features", "Keywords", "Biometrics", "ICMC", "USP", "mail", "phpisani", "Tecnologia", "ICT", "UNIFESP", "aclorena", "1 Introduction", "increased", "net", "commerce", "term", "crime", "someone", "scenario", "case", "example", "intruders", "session", "ways", "concept", "fact", "models", "profile", "identification", "deviations", "18", "24", "10", "S N User profile Store profile", "regular typing rhythm", "possible devia- tions", "keystroke dynam- ics", "system audit logs", "command line use", "user behavior pattern", "systematic bibliographic reviews", "Behavioural intrusion detection", "two major steps", "intrusion detection system", "possible incidents", "behaviour pattern", "anomaly detection", "user profiling", "same user", "systematic procedure", "behavioural IDS", "other researchers", "other areas", "software engineering", "artificial intelligence", "remaining sections", "basic concepts", "security policies", "acceptable use", "security practices", "mod- els", "basic flow", "key issue", "similar values", "keyboard input", "user behaviour", "User Training", "review method", "normal behaviour", "valuable information", "information security", "Training phase", "explicit protocols", "new concept", "Application usage", "same time", "Preliminary results", "An IDS", "name", "formal", "conduction", "way", "validation", "incidence", "bias", "problem", "medicine", "number", "benefits", "computing", "paper", "tributes", "aim", "Sect.", "process", "conclusions", "2 Background", "events", "computer", "signals", "violations", "threats", "analysis", "type", "alarms", "Recognition", "Fig.", "Figure", "matching", "aspects", "questions", "group", "monitoring", "systems audit logs", "lower error rates", "support vector machines", "Two distinctive processes", "machine learning algorithms", "Systematic literature review", "two ways", "several algorithms", "other aspects", "continuous authentication", "mail behav", "less effort", "first process", "second process", "neural networks", "less susceptibility", "Software Engineering", "major phases", "first phase", "second phase", "secondary study", "third phase", "final report", "check- ing", "specific knowledge", "important part", "primary studies", "eval- uation", "essential part", "3 Systematic review", "Protocol evaluation", "systematic reviews", "dynamic text", "Static text", "bibliographic reviews", "review protocol", "feature classification", "classifica- tion", "pro- tocol", "three phases", "cific topic", "following steps", "research questions", "feature extraction", "formal way", "review need", "typing features", "specific user", "search strategies", "rhythms", "model", "technology", "iour", "expressions", "password", "recognition", "terms", "instance", "addition", "completion", "Medicine", "Computing", "application", "planning", "presentation", "information", "references", "items", "Identification", "goal", "existence", "investigation", "requirements", "Commissioning", "cases", "lack", "time", "Specification", "Development", "selection", "576 J Braz Comput Soc", "greatest possible number", "newer review article", "systematic review process", "other researches", "traditional reviews", "inclusion/exclusion criteria", "Data synthesis", "quantitative aspects", "academic journals", "web sites", "two researchers", "next sections", "Classification algorithms", "Performance measures", "Benchmarking datasets", "comparative experiments", "respective sub-questions", "typing data", "Reference search", "search expressions", "planning phase", "Quality evaluation", "research question", "diverse aims", "Information extraction", "dissemination mechanisms", "explicit definition", "More details", "intrusion detection", "Extracted features", "reference databases", "conduction phase", "previous work", "Report evaluation", "summary results", "rigour", "Selection", "use", "importance", "study", "support", "forms", "step", "qualitative", "meta-analysis", "mulation", "publishing", "conferences", "experts", "publication", "principles", "phases", "topic", "Advantages", "Internet", "English", "authors", "part", "need", "3.2", "1.1", "logical connective O R.", "personal- ization systems", "authentica- tion systems", "behaviour intrusion detection", "behavior intrusion detection", "ACM Digital Library", "biometric intrusion detection", "behavioural intrusion detection", "behavioral intrusion detection", "basic search expression", "following exclusion criteria", "keystroke biometric(s", "recommender systems", "typing biometric", "keypress biometric", "1.3 Selection criteria", "behavioral IDS", "behavioural biometrics", "keystroke pattern", "keystroke authentication", "behavioral biometrics", "many users", "typing pattern", "uous authentication", "same topic", "personalized information", "ized content", "content delivery", "recommendation system", "ommendations system", "information retrieval", "AN D", "N OT", "several data-bases", "computing area", "data- base", "Appendix A", "next steps", "one page", "keystroke biometrics", "typing dynamics", "web search", "keystroke analysis", "unrelated topics", "IEEE Xplore", "Science Direct", "last part", "systematic review", "various terms", "exclusion terms", "performance comparisons", "1.2 References search", "OR ‘‘personalization", "rithms", "measures", "datasets", "classifier", "tests", "list", "papers", "completeness", "database", "conjunction", "reason", "differences", "syntax", "org", "ieeexplore", "sciencedirect", "isiknowledge", "Scopus", "inclusion", "Publications", "requirement", "posters", "presentations", "cts", "editorials", "texts", "magazines", "newspaper", "578 J Braz Comput Soc", "enough informa- tion", "use- ful features", "high- light references", "complete version", "unnecessary work", "different versions", "restricted access", "The value", "possible scores", "research criteria", "feature vector", "previous researches", "researc- hes", "exclusion criteria", "static text", "first step", "data- bases", "several papers", "quality criteria", "Basic information", "Best performance", "duplicate publications", "same study", "higher values", "algorithm parameters", "experimental tests", "design step", "analysis step", "conclusion step", "test dataset", "first publications", "conduction step", "quality score", "nine items", "several items", "performance tests", "criterion", "services", "accessible", "sum", "goals", "beginning", "advantages", "approaches", "limitations", "problems", "acm", "scopus", "Part", "title", "year", "Type", "device", "PC", "measure", "Number", "Algorithms", "verification", "Observations", "line", "Table", "18/February", "tool", "mendeley", "series", "duplicates", "organization", "category", "4", "2.1", "references Database Number", "step Step Number", "PDF files", "fast reading", "secondary studies", "200 refer- ences", "quality assessment", "several texts", "direct way", "pub- lications", "descending order", "highest scores", "higher scores", "greater efforts", "average score", "high number", "exclusion criterion", "search databases", "plete text", "full text", "duplicate references", "remaining references", "eligible references", "information extraction", "Table 2 Number", "Table 1", "associations", "entries", "Web", "Gaines", "centralization", "intersection", "removal", "university", "Conse", "reviews", "surveys", "tasks", "spreadsheet", "Total", "elimination", "detail", "tables", "algorithms", "performance", "abstract", "observation", "one", "ally", "purpose", "equals", "focus", "rel", "graph", "different", "4.2.2", "2.3", "580 J Braz Comput Soc", "approximate normal distribution", "tional research efforts", "highest quality score", "publi- cations", "scientific community", "research sub-questions", "performance evaluation", "benchmarking datasets", "access card", "military purposes", "simple login", "unauthorized access", "card ownership", "recent years", "greatest growth", "same institution", "biometric technologies", "behavioral features", "biometric technology", "specific devices", "biometric data", "growth trend", "voice recognition", "primary method", "One way", "additional cost", "authentication factors", "several users", "unauthorized user", "security context", "Publication Year", "widespread use", "user identity", "zero", "scores", "maximum", "aspect", "field", "atten", "Both", "5 Results", "section", "disadvan", "credentials", "token", "biometrics", "gerprint", "iris", "combination", "secrecy", "weaknesses", "authorization", "words", "birthday", "scenarios", "expansion", "gally", "rate", "USA", "losses", "world", "insiders", "victim", "attack", "science", "methods", "determination", "iological", "hardware", "contrast", "other", "capture", "level", "transparency", "false alarm rates", "nature recognition biometrics", "Recognition precision", "specific operations", "stroke dynamics", "same environment", "nition performance", "user identification", "legiti- mate", "legitimate user", "trol systems", "authentication", "factor", "acceptance", "presence", "keyboards", "teristics", "pen", "intruder", "standards", "access", "European" ], "merged_content": "\nJ Braz Comput Soc (2013) 19:573–587\nDOI 10.1007/s13173-013-0117-7\n\nSURVEY PAPER\n\nA systematic review on keystroke dynamics\n\nPaulo Henrique Pisani · Ana Carolina Lorena\n\nReceived: 18 March 2013 / Accepted: 24 June 2013 / Published online: 10 July 2013\n© The Brazilian Computer Society 2013\n\nAbstract Computing and communication systems have\nimproved our way of life, but have also contributed to an\nincreased data exposure and, consequently, to identity theft.\nA possible way to overcome this issue is by the use of biomet-\nric technologies for user authentication. Among the possible\ntechnologies to be analysed, this work focuses on keystroke\ndynamics, which attempts to recognize users by their typ-\ning rhythm. In order to guide future researches in this area,\na systematic review on keystroke dynamics was conducted\nand presented here. The systematic review method adopts\na rigorous procedure with the definition of a formal review\nprotocol. Systematic reviews are not commonly used in arti-\nficial intelligence, and this work contributes to its use in the\narea. This paper discusses the process involved in the review\nalong with the results obtained in order to identify the state\nof the art of keystroke dynamics. We summarized main clas-\nsifiers, performance measures, extracted features and bench-\nmark datasets used in the area.\n\nKeywords Behavioral intrusion detection · Biometrics ·\nKeystroke dynamics · Systematic review\n\nP. H. Pisani (B)\nInstituto de Ciências Matemáticas e de Computação (ICMC),\nUniversidade de São Paulo (USP), São Carlos, SP, Brazil\ne-mail: phpisani@icmc.usp.br\n\nA. C. Lorena\nInstituto de Ciência e Tecnologia (ICT),\nUniversidade Federal de São Paulo (UNIFESP),\nSão José dos Campos, SP, Brazil\ne-mail: aclorena@unifesp.br\n\n1 Introduction\n\nThe wider dissemination of digital identities has contributed\nto greater worries regarding information exposure [47].\nRecently, in view of the increased dissemination of the inter-\nnet in several activities (e.g. online banking, e-commerce,\ne-mail), security problems became more evident [24]. As a\nresult, identity theft has gained new momentum. The term\nidentity theft is commonly used to refer to the crime of using\npersonal information of someone else to illegally pretend to\nbe a certain person [38].\n\nIn view of this scenario, more sophisticated methods for\nuser authentication have been developed. Authentication is\nthe process used to confirm the identity of a user. In the case of\nworkstations, for example, the authentication usually occurs\nin the system initialization, known as initial authentication.\nNevertheless, even more secure authentication methods do\nnot provide an entirely effective security mechanism, as the\ncomputer may be vulnerable to intruders when the user leaves\nthe workstation and does not end the session. Consequently,\nan intruder could use the computer masquerading as the legit-\nimate user, resulting in identity theft [38]. One of the ways to\nmitigate this problem is by using intrusion detection systems\nthat act on the workstation (host-based).\n\nMore recently, the concept of detecting intrusions by the\nbehavioral analysis of the user of the computer [39] has\nemerged, also known as Behavioral Intrusion Detection [49];\nseveral aspects of this method have yet to be explored. This\nconcept is grounded on the fact that, by observing the behav-\nior of a user, it is possible to define models that represent\nthe regular behavior (profile) of this user, thus allowing the\nidentification of deviations that are potential intrusions. The\nprocess of defining these models is known as user profil-\ning [46]. There is a great variety of features that can be\nused to define the model of a user. This work focuses on\n\n123\n\n\n\n574 J Braz Comput Soc (2013) 19:573–587\n\nkeystroke dynamics, classified as a behavioral biometric\ntechnology.\n\nThis paper adopts a rigorous method to perform a review\non intrusion detection with keystroke dynamics, known as\nsystematic review. As the name suggests, a systematic review\nadopts a formal and systematic procedure for the conduction\nof the bibliographic review, with the definition of explicit\nprotocols for obtaining information. Consequently, by using\nthese protocols, the results attained by the systematic review\ncan be reproduced by other researchers as a way of validation,\ndecreasing the incidence of bias in the review, a problem\nboosted in non-systematic bibliographic reviews [33].\n\nSystematic reviews are commonly applied in other areas,\nmainly in medicine, and have a number of reported benefits\n[33]. In the area of computing, this review method is more\ndisseminated in software engineering [7]. This paper con-\ntributes to the use of systematic review in computing, partic-\nularly in artificial intelligence. Here, we discuss how the sys-\ntematic review was applied and the achieved results, which\nare valuable information for the area of intrusion detection\nwith keystroke dynamics.\n\nThis paper presents a systematic review carried out with\nthe aim of identifying the state of the art in keystroke dynam-\nics applied to intrusion detection. Preliminary results of this\nreview are shown in [42] and [41]. The remaining sections are\norganized as follows: in Sect. 2, basic concepts of keystroke\ndynamics are introduced; in Sect. 3, the process of system-\natic review is presented; Sect. 4 discusses how the systematic\nreview was applied in this work, specifying the review proto-\ncol and the steps adopted; in Sect. 5, the results obtained\nby the systematic review are summarized; and, finally,\nSect. 6 presents our conclusions.\n\n2 Background\n\nIn information security, intrusion detection is the process of\nmonitoring events in a computer or network and analyse them\nto detect signals of possible incidents, which are violations\nor threats of violations of security policies, acceptable use or\nsecurity practices [45]. An intrusion detection system (IDS)\nautomatizes this process.\n\nAs previously discussed, more recently, a new concept\nof detecting intrusions by the analysis of the user behaviour\nin the computer has emerged [39], which is performed by\nthe behavioural IDS [49]. This type of system is grounded\non a concept known as user profiling, which consists of\nobserving the behaviour of a user in order to generate mod-\nels that represent its normal behaviour. Observed events\nare then compared to these models and possible devia-\ntions are classified as potential intrusions [46]. An IDS\nthat applies user profiling is a system based on anomaly\ndetection, as it generates alarms for events that deviates\n\nKeystroke dynamics,\nApplication usage, etc.\n\nUser\n\nTraining\n\nRecognition\n\nGet profile\n\nYes/No\n\nTraining\nphase?\n\nS\n\nN\n\nUser profile\n\nStore profile\n\nFig. 1 Behavioural intrusion detection (adapted from [42])\n\nfrom a behaviour pattern. Figure 1 represents the basic\nflow of a behavioural IDS, which involves two major steps\n[16,21]:\n\n– Training: obtaining features for the definition of the user\nbehavior pattern;\n\n– Recognition: matching observed features against user\nbehavior pattern.\n\nA key issue in the application of user profiling is how to\ndefine the profile, that is, which aspects will be observed.\nThe process of choosing these aspects is one of the major\nquestions when applying user profiling. Ideally, the chosen\naspects should allow the identification of a user within a\ngroup of users and, at the same time, maintain similar values\nthrough the time for the same user [21]. There is a number of\naspects that can be used for the definition of the user profile,\nsuch as keystroke dynamics, system audit logs, e-mail and\ncommand line use [46].\n\nThis work studies keystroke dynamics as an aspect to\nbe analysed by the behavioural intrusion detection sys-\ntem. Keystroke dynamics analyzes how users type from\nthe monitoring of the keyboard input. As a result, mod-\nels that represent the regular typing rhythm of the user are\ndefined. Afterwards, these models are used for the recogni-\n\n123\n\n \n\n \n\n \n\n \n\n \n\n\n\nJ Braz Comput Soc (2013) 19:573–587 575\n\ntion [28], in such a way that typing rhythms deviating from\nthis model are classified as being from intruders. Here, we\nhave chosen keystroke dynamics instead of other aspects\nbecause it may be used either in the initial authentication\nof a system or as continuous authentication after the ini-\ntial authentication. It makes this technology more flexible\nthan an analysis of systems audit logs or e-mail behav-\niour.\n\nKeystroke dynamics can be applied in two ways: static\ntext or dynamic text. Static text only performs an analysis\nof fixed expressions as, for example, a password. While, in\ndynamic text, the analysis occurs for any text that is typed by\nthe user. Keystroke dynamics in static text requires less effort\nto be implemented and it also reached lower error rates in\nliterature [11].\n\nTwo distinctive processes are involved in keystroke dynam-\nics: feature extraction and classification of the extracted\nfeatures. In the first process, a number of features are\nextracted for the recognition of a user. These features\nshould represent how the user behaves in terms of keystroke\ndynamics.\n\nIn the second process, which corresponds to the feature\nclassification, several algorithms can be used. For instance,\nmachine learning algorithms, like neural networks [48] and\nsupport vector machines [19], were applied in this classifica-\ntion, which consists of verifying whether the typing features\nbelong or not to a specific user.\n\n3 Systematic review\n\nSystematic literature review (called just systematic review\nin this paper) is a method for conducting bibliographic\nreviews in a formal way, following well defined steps, which\nallows the results to be reproducible. In addition, the pro-\ntocol adopted for the conduction of the review must assure\nits completion. This review method is commonly used in\nother areas, mainly in Medicine [7] and has several reported\nbenefits, like less susceptibility to bias [33]. In the area of\nComputing, this method of review is more disseminated in\nSoftware Engineering.\n\nThe application of the systematic review involves three\nmajor phases: planning, conduction and presentation of\nresults. In the first phase, a review protocol is defined, in\nwhich research questions are specified along with search\nstrategies. After that, in the second phase, the review pro-\ntocol is applied and the information is extracted from the\nreturned references. References used for the extraction of\ninformation are called primary studies, while the review\nis a secondary study. Finally, the third phase defines the\nway to present the results and the final report is done.\nThe items comprehended in each of the three phases are\n[33]:\n\n3.1 Planning\n\n– Identification of the review need: a systematic review has\nthe goal of summarizing all information regarding a spe-\ncific topic. However, before starting a systematic review,\nthe need of this review has to be checked. This check-\ning, for instance, should verify the existence of previ-\nously published systematic reviews that deal with the\ntopic under investigation and whether the protocol of\nthese reviews meet the requirements of the research.\n\n– Commissioning (optional): in some cases, due to the lack\nof time or specific knowledge, one may need to request\nthat other researchers conduct the systematic review.\n\n– Specification of the research questions: this is considered\nto be the most important part of the systematic review,\nas these questions will guide all the following steps, as\nthe search for primary studies, extraction and analysis of\ninformation.\n\n– Development of the review protocol: this step defines\nstrategies to be used for the search, selection and eval-\nuation of the references. In addition, the information to\nbe extracted from each of the selected references is also\ndefined.\n\n– Protocol evaluation (optional): as the review protocol is\nan essential part of the systematic review, it is recom-\nmended to be reviewed by other researches.\n\n3.2 Conduction\n\n– Reference search: search for the greatest possible number\nof references which can answer the research question in\norder to avoid bias. In the systematic review, the search is\nperformed with increased rigour, with the pre-definition\nof search expressions and databases, making it different\nfrom traditional reviews.\n\n– Selection of primary studies: after reference search, the\nstudies that are in fact relevant for the research must be\nselected, by the use of inclusion/exclusion criteria.\n\n– Quality evaluation: each of the selected references\nundergo a quality evaluation. This evaluation may be\nused with diverse aims, like contributing for the inclu-\nsion/exclusion criteria or supporting the summary results,\nby measuring the importance of each study.\n\n– Information extraction: the information extraction from\nthe references must be done with the support of forms\ndefined during the planning phase of the systematic\nreview.\n\n– Data synthesis: this step corresponds to summarizing the\nresults attained during the review. This summary may\ninvolve qualitative and quantitative aspects. For quanti-\ntative aspects, a meta-analysis may also be applied.\n\n123\n\n\n\n576 J Braz Comput Soc (2013) 19:573–587\n\n3.3 Reporting the review\n\n– Specification of the dissemination mechanisms and for-\nmulation of the report: dissemination of the results\nattained by the systematic review. This can be done by\npublishing in academic journals and conferences or even\nin web sites.\n\n– Report evaluation (optional): this evaluation can be\nrequested to experts in the area of the research. If the\nreview is submitted to a journal or conference, the review\nprocess of the publication can be considered an evalua-\ntion of the report.\n\nThe explicit definition of the review protocol allows the\nresults to be reproduced. The review presented in this paper\nwas performed by two researchers in the planning phase,\nbut by just one in the conduction phase. Due to that, this\nreview can be called a quasi-systematic review, as it follows\nthe principles of a systematic review, but was not conducted\nby two researchers in all phases. This term, quasi-systematic\nreview, was also used in previous work [35]. More details on\nhow to carry out each of the phases are discussed in the next\nsections, in which the systematic review process is applied to\nthe topic of keystroke dynamics for intrusion detection.\n\n4 How the systematic review was applied\n\nIn this work, the application of the systematic review has the\ngoal of studying the state of the art in keystroke dynamics in\norder to identify:\n\n1. Advantages and disadvantages of using keystroke dynam-\nics in intrusion detection;\n\n2. Extracted features;\n3. Classification algorithms applied;\n4. Performance measures commonly adopted;\n5. Benchmarking datasets, which are useful for conducting\n\ncomparative experiments in the area.\n\nBefore presenting details of how the systematic review\nwas applied in this work, it is important to highlight that we\nonly considered references indexed by reference databases\navailable on the Internet and written in English.\n\n4.1 Planning\n\nAccording to a research carried by the authors, there are\nno published systematic reviews that meet the goals of this\nwork. Besides, the newer review article on keystroke dynam-\nics known by the authors was submitted for publication in\n2009 [28]. Moreover, part of our aims was not met in that\n\npublication, as the identification of benchmarking datasets.\nHence, the conduction of the review in this work is justified.\n\n4.1.1 Research questions\n\nIn view of the need of the systematic review, we defined a\nresearch question and some respective sub-questions to meet\nthe established goals:\n\nHow keystroke dynamics is used for intrusion\ndetection?\n\n– What are the advantages and disadvantages of using\nkeystroke dynamics for intrusion detection?\n\n– What features are extracted from the typing data?\n– What classification algorithms are applied? What algo-\n\nrithms are used in the performance comparisons?\n– What measures were used to evaluate the performance?\n\nWhat was the performance achieved?\n– What datasets are used to measure the performance of\n\nthe classifier? How many users took part in the tests\nperformed?\n\n4.1.2 References search\n\nAfter defining the research question, we enumerated a list\nof terms related to papers that could answer it: keystroke\ndynamics, typing dynamics, keystroke biometric(s), key-\nstroke authentication, keystroke pattern(s), typing pattern(s),\nbehaviour intrusion detection, behavior intrusion detection,\nbehavioral IDS, biometric intrusion detection, user profil-\ning, behavioural biometrics, behavioral biometrics, contin-\nuous authentication, typing biometric(s), keypress biomet-\nric(s), keystroke analysis. The use of various terms for the\nsame topic, sometimes even synonyms, contributes to the\ncompleteness of the search [1]. From this list of terms, we\nbuilt search expressions for each database of references. The\nbasic search expression is the conjunction of each term in the\nlist using the logical connective O R.\n\nNevertheless, after some tests with this search expres-\nsion, we observed that many of the returned references dealt\nwith topics not related to the research question, as personal-\nization systems and recommender systems. For this reason,\nsome terms that could exclude these unrelated topics were\nidentified: web search, personalized information, personal-\nized content, content delivery, recommendation system, rec-\nommendations system, information retrieval, personalizing,\npersonalization, recommender. The basic search expression\nwas then modified to consider the exclusion terms with the\nuse of the logic connective AN D and N OT together, as\nfollows:\n\n(‘‘behavioural intrusion detection’’\nOR ‘‘behavioral intrusion detection’’\n\n123\n\n\n\nJ Braz Comput Soc (2013) 19:573–587 577\n\nOR ‘‘behavioral IDS’’\nOR ‘‘behavioural IDS’’\nOR ‘‘biometric intrusion detection’’\nOR ‘‘user profiling’’\nOR ‘‘keystroke dynamics’’\nOR ‘‘typing dynamics’’\nOR ‘‘keystroke biometrics’’\nOR ‘‘keystroke biometric’’\nOR ‘‘continuous authentication’’\nOR ‘‘keystroke authentication’’\nOR ‘‘behavioural biometrics’’\nOR ‘‘behavioral biometrics’’\nOR ‘‘keystroke pattern’’\nOR ‘‘keystroke patterns’’\nOR ‘‘typing pattern’’\nOR ‘‘typing patterns’’\nOR ‘‘typing biometric’’\nOR ‘‘typing biometrics’’\nOR ‘‘keypress biometric’’\nOR ‘‘keypress biometrics’’\nOR ‘‘keystroke analysis’’)\n\nAND NOT\n\n(‘‘web search’’\nOR ‘‘personalized information’’\nOR ‘‘personalized content’’\nOR ‘‘content delivery’’\nOR ‘‘recommendation system’’\nOR ‘‘recommendations system’’\nOR ‘‘information retrieval’’\nOR ‘‘personalizing’’\nOR ‘‘personalization’’\nOR ‘‘recommender’’)\n\nThis search expression was applied in several data-bases\nthat included references in the computing area. As each data-\nbase has differences in its syntax for search expression, the\nbasic search expression presented here was adapted to each\ndatabase, as specified in Appendix A. The following data-\nbases were considered in this work:\n\n– ACM Digital Library\n(http://dl.acm.org/)\n\n– IEEE Xplore\n(http://ieeexplore.ieee.org/)\n\n– Science Direct\n(http://www.sciencedirect.com/)\n\n– Web of Science\n(http://isiknowledge.com/)\n\n– Scopus\n(http://www.scopus.com/)\n\n4.1.3 Selection criteria\n\nThe last part of the planning phase is the definition of\nthe selection criteria (inclusion and exclusion) that will be\napplied to the returned references. In this systematic review,\nall the returned references are included for analysis in the\nnext steps, except the ones that meet the following exclusion\ncriteria:\n\n1. Publications that do not deal with keystroke dynamics\nfor intrusion detection: the aim of this review is to work\nwith intrusion detection, which comprehends authentica-\ntion systems. Therefore, references that do not meet this\nrequirement were not included.\n\n2. Publications with one page, posters, presentations, abstra-\ncts and editorials, texts in magazines/newspaper and\nduplicate publications in terms of results, except the most\ncomplete version: references without enough informa-\ntion to answer the research question. This criterion also\navoids unnecessary work for the cases in which the same\nstudy is published in different versions.\n\n3. Publication hosted in services with restricted access and\nnot accessible or publications not written in English.\n\nIn this phase, we also created a quality score to be applied\nto the returned references. This score was determined to high-\nlight references that better answer our research question. The\nvalue of the quality score is the sum of the score reached in\neach of the assessed items. For each of these items, the ref-\nerence scores 1 if fully meets it, 0.5 if partially meets it and\n0 if does not meet the assessed item. As there are nine items,\nthe possible scores ranges between 0 and 9, in such a way\nthat higher values indicate better publications according to\nthe established research criteria. The items are:\n\n1. Were the goals clearly presented in the beginning of the\nwork?\n\n2. Were the advantages/disadvantages of keystroke dynam-\nics discussed?\n\n3. Is the dataset available to be reused?\n4. Was it detailed how the feature vector is generated?\n5. Were the values of the algorithm parameters presented?\n6. Were the applied approaches detailed so as to allow them\n\nto be replicated?\n7. Were experimental tests conducted?\n8. Were the results compared to previous researches in the\n\narea?\n9. Were the limitations of the study presented?\n\nThe quality criteria were defined considering that researc-\nhes may present problems in the following steps: design,\nconduction, analysis and conclusion [33]. The items 1 and\n\n123\n\nhttp://dl.acm.org/\nhttp://ieeexplore.ieee.org/\nhttp://www.sciencedirect.com/\nhttp://isiknowledge.com/\nhttp://www.scopus.com/\n\n\n578 J Braz Comput Soc (2013) 19:573–587\n\n2 refer to the design step, the items 3–6 to the conduction\nstep, the items 7–8 to the analysis step and the item 9 to the\nconclusion step. Part of the items used to assess the quality\nwas based on the list in [33], which presents several items to\nbe evaluated in references.\n\n4.1.4 Information extraction\n\nStill in the planning phase of the systematic review, we\ndefined a set of information to be extracted from each selected\nreference (after the application of the exclusion criteria), as\nfollows:\n\n– Basic information about the publication (title, authors,\nname and year of publication)\n\n– Were performance tests conducted?\n– Type of device (e.g. PC, mobile)\n– Best performance achieved: algorithm, measure and\n\nperformance\n– Number of users in the tests\n– Algorithms used in the tests\n– Extracted features\n– Is the test dataset available to be reused? Where?\n– Type of verification: static text or dynamic text?\n– Observations\n\nThese items were defined in line with the research question,\nin order to answer it and guide the information extraction in\nthe conduction phase of this review.\n\n4.2 Conduction\n\nFrom the review protocol defined in the planning phase, the\nconduction of the systematic review was started.\n\n4.2.1 Application of the search expressions\n\nThe first step was to apply the search expressions in each\ndatabase of references and save the returned results. Apart\nfrom the returned references, we also included a reference\npreviously known by the authors, but not indexed by the data-\nbases used in this review: [15]. This reference is mentioned\nin several papers as being one of the first publications about\nkeystroke dynamics. Table 1 shows the number of references\nreturned by each database on 18/February/2013.\n\nThese results were centralized in order to continue the\nreview, using a tool called Mendeley (available in: http://\nwww.mendeley.com/). We used this tool to import the results\nexported from the databases. Mendeley has a series of use-\nful features that can be used for systematic reviews, such as\nsearch for duplicates, organization of references by category\nand associations of the entries with PDF files stored in the\ncomputer.\n\nTable 1 Number of returned references\n\nDatabase Number of references\n\nACM Digital Library 71\n\nIEEE Xplore 308\n\nScience Direct 104\n\nWeb of Science 596\n\nScopus 943\n\nGaines et al. [15] 1\n\nTotal 2, 023\n\n4.2.2 Selection of references\n\nAfter the centralization of the information returned from the\nsearch databases, duplicate references were removed. Dupli-\ncate references may appear since databases can have some\nintersection in the indexed data, as in the case of Scopus and\nWeb of Science.\n\nOnce the removal of duplicates was finished, a fast read-\ning of the text of the remaining references was performed.\nBefore starting this step, we needed to download the com-\nplete text of each publication. However, it was not possible\nto download 27 of them, which were hosted in services not\navailable from our university (exclusion criterion 3). Conse-\nquently, the number of eligible references was again reduced.\nIn the end, another fast reading of the eligible references was\nperformed to revalidate the exclusion criteria 1 and 2. A great\nnumber of references that do not deal with keystroke dynam-\nics for intrusion detection has been eliminated just by the title\nand abstract, nevertheless, some references were eliminated\nonly after reading their full text. Once the exclusion crite-\nria 1 to 3 were applied, secondary studies were removed,\nwhich were only three: [11,28,40]. Secondary studies are\nthose commonly known as reviews or surveys. Table 2 shows\nthe number of references returned after the application of\neach step.\n\nWith the application of all exclusion criteria, 200 refer-\nences (Table 2) were left for the next steps: information\nextraction and quality assessment. Aiming at accelerating\nthese tasks, we created a spreadsheet with all the items for\ninformation extraction and quality assessment discussed in\n\nTable 2 Number of references after each step\n\nStep Number\n\nTotal of references 2,023\n\nAfter elimination of duplicates and exclusion\ncriteria 1 and 2\n\n230\n\nAfter exclusion criterion 3 203\n\nAfter exclusion of secondary studies 200\n\n123\n\nhttp://www.mendeley.com/\nhttp://www.mendeley.com/\n\n\nJ Braz Comput Soc (2013) 19:573–587 579\n\nthe planning phase (Sect. 4.1). This spreadsheet was then\nfilled with the information from the references.\n\nThis was the part of the systematic review that consumed\nmore time due to the need to read in detail several texts. In\naddition, sometimes the information to be extracted were not\npresent in a direct way in the text. For example, in some pub-\nlications, there were tables summarizing tested algorithms\nand their performance [19] or it was even possible to extract\nalmost all information from the abstract [22]. However, this\nwas not the case of some publications, which needed to be\nread more deeply to find the desired information. Actually,\nthis observation may be related to the one mentioned in [7],\nwhich highlights the fact that abstracts in Computing are usu-\nally not well structured, making it difficult to get informa-\ntion about the publication only by the abstract. According to\n[7], the scenario is different in medicine, area in which the\nabstracts are, in general, better structured and usually contain\nmore information about the publication.\n\n4.2.3 Quality assessment\n\nDue to the high number of selected references, they were\nsorted in descending order of quality score and only the ones\nwith the highest scores are discussed in details here. For the\npurpose of this review, only those papers with quality score\nequals or higher than 7.5 were considered, resulting in 16\npublications. The focus on references with higher scores has\nthe goal of spending greater efforts on references more rel-\nevant to the research question, as the quality scores were\nspecially designed with this purpose.\n\nThe graph in Fig. 2 shows the number of publications for\neach quality score. The average score among those different\nfrom zero was 5.54 and, as shown in Fig. 2, the scores follow\nan approximate normal distribution. The maximum reached\nscore was 8.5.\n\nAnother aspect analysed was the number of selected publi-\ncations by year, as shown in the graph in Fig. 3. In this graph,\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\n30\n\n35\n\n0 1 2 3 4 5 6 7 8 9\n\nN\nu\n\nm\nb\n\ner\n o\n\nf \nP\n\nu\nb\n\nlic\nat\n\nio\nn\n\ns\n\nQuality Score\n\nFig. 2 Publications by quality score\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\n30\n\n1998 2000 2002 2004 2006 2008 2010 2012\n\nN\nu\n\nm\nb\n\ner\n o\n\nf \nP\n\nu\nb\n\nlic\nat\n\nio\nn\n\ns\n\nPublication Year\n\nFig. 3 Publications by year in keystroke dynamics. The growth trend\nillustrates that the field is gaining new momentum, justifying additional\nresearch efforts\n\nit is important to highlight the growth trend in the number of\npublications by year in the area of keystroke dynamics. This\ntrend was higher between 2002 and 2006. Such a growth\ntrend indicates that the area has been receiving more atten-\ntion from the scientific community. This may justify addi-\ntional research efforts in keystroke dynamics.\n\nBoth graphs consider only the references with available\ntexts.\n\n5 Results\n\nIn this section, we focus on the 16 publications with highest\nquality score and on some papers referenced by them. The\nfollowing subsections are organized in such a way to answer\neach of the research sub-questions: advantages and disadvan-\ntages of keystroke dynamics, feature extraction, classifica-\ntion algorithms, performance evaluation and benchmarking\ndatasets.\n\n5.1 Advantages and disadvantages\n\nAuthentication of users is done by the use of credentials, also\nknown as authentication factors, which can be [47]:\n\n1. what the user knows (e.g. password);\n2. what the user has (e.g. access card, token);\n3. what the user is/does (e.g. biometrics: recognition by fin-\n\ngerprint, iris, keystroke dynamics, voice recognition);\n4. some combination of the above items.\n\nThe primary method of authentication, be it for\ne-commerce or for military purposes, is a simple login and\npassword [12]. The use of this method is based on the fact that\nthe secrecy of the password will be held [40]. However, this\nis not always the case, implying in a number of weaknesses\n[10]:\n\n123\n\n\n\n580 J Braz Comput Soc (2013) 19:573–587\n\n– Passwords may be shared by several users, resulting in\nunauthorized access;\n\n– Passwords may be copied without authorization;\n– Passwords may be guessed, particularly for easy pass-\n\nwords, as when someone uses his/her birthday as a pass-\nword [43].\n\nMoreover, even in scenarios in which the user authenti-\ncation is performed by the use of access cards, the security\nis compromised. This is because the card ownership can be\nshared with an unauthorized user and it may also be stolen\n[26].\n\nThese problems, along with widespread use of the Web,\ncontributed to expansion of identity theft, which occurs when\na person uses personal information of someone else to ille-\ngally pretend to be this person [38]. In recent years, identity\ntheft has become a crime with the rate of greatest growth in\nthe USA [6]. Furthermore, the sum of losses in the world due\nto identity theft have been estimated to be around US$ 221\nbillion in 2003 [25]. According to research, [29], weaknesses\nof passwords was the most exploited factor by insiders (users\nfrom the same institution which is the victim of the attack).\n\nOne way to mitigate this problem is the use of biometric\ntechnologies to enhance the security provided by passwords.\nIn the security context, biometrics is a science which studies\nmethods for the determination of user identity based on phys-\niological and behavioral features [26]. Keystroke dynamics,\nwhich is considered a biometric technology, can be used with-\nout any additional cost with hardware, in contrast to other\nbiometric technologies (e.g., iris, fingerprint), which need\nspecific devices for the capture of biometric data [24,37].\nIn addition, the level of transparency in the use of keystroke\ndynamics is high [40]. This means that there is no need to\nperform specific operations for the authentication by key-\nstroke dynamics [3]. This factor contributes for an increased\nacceptance of keystroke dynamics among users.\n\nRecognition precision by keystroke dynamics may be\naffected in the presence of keyboards with different charac-\nteristics in the same environment. Nevertheless, it is expected\nthat such differences does not significantly impair the recog-\nnition performance and, consequently, still enable proper\nuser identification [24]. This can be compared to the sig-\nnature recognition biometrics in which, regardless of the pen\nused, the system is still able to differentiate between legiti-\nmate and illegitimate users [24].\n\nFurthermore, false alarm rates (when a legitimate user\nis classified as an intruder) in keystroke dynamics are usu-\nally high and do not meet standards in some access con-\ntrol systems, such as the European. Additionally, differences\namong systems, like p", "text": [ "", "", "", "", "" ], "layoutText": [ "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}", "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}", "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}", "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}", "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}" ] } ] }